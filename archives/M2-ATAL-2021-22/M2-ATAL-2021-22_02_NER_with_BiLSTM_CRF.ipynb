{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2-ATAL-2021-22_02_NER_with_BiLSTM_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNr2Oc6qJHX"
      },
      "source": [
        "---\n",
        "#¬†Recent Advances in Sequence Labeling from Deep Learning Models\n",
        "\n",
        "Les approches pour l'√©tiquetage de s√©quence fond√©es sur les r√©seaux de neurones profonds compte trois √©tapes :\n",
        "1. The embedding module is the first stage that maps words into their distributed representations (pretrained word embeddings, character-\n",
        "level representations, hand-crafted features and sentence-level\n",
        "representations). \n",
        "2. The context encoder module extracts contextual features (e.g. RNN/Bi-LSTM, CNN)\n",
        "3. and the inference module predict labels and generate optimal label sequence as output of the model (e.g. SoftMax, CRF, RNN). \n",
        "\n",
        "[Zhiyong He, Zanbo Wang, Sheng Jiang. A Survey on Recent Advances in Sequence Labeling from Deep Learning Models. Published 13 November 2020. Computer Science. ArXiv](https://arxiv.org/pdf/2011.06727.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0boga_k7vMTc"
      },
      "source": [
        "---\n",
        "# Bref historique des syst√®mes de NER neuronaux\n",
        "\n",
        "On ne vous demande pas de lire les articles suivants mais √† minima de lire ce bref historique et de jeter un oeil aux sections 2.2 √† 2.5 de (Huang et al., 2015) pour comprendre le mod√®le Bi-LSTM_CRF.\n",
        "\n",
        "* L'architecture \"SENNA\", novatrice dans l'id√©e de la r√©solution des t√¢ches du TAL avec un mod√®le de langue neuronal (incluant notamment une m√©thode de construction de \"pretrained word embeddings\") : R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch, Journal of Machine Learning Research (JMLR), 2011. ; [[article]](http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf) ; [[impl√©mentation]](https://ronan.collobert.com/senna/)\n",
        "* Premier article √† appliquer les BiLSTM-CRF au NER : Zhiheng Huang, Wei Xu, Kai Yu, Bidirectional LSTM-CRF Models for Sequence Tagging, Arxiv, Computation and Language, Submitted on 9 Aug 2015 ; [[article]](https://arxiv.org/pdf/1508.01991.pdf) ; [[impl√©mentation1]](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html) (tutoriel avanc√© de pytorch) ; [[impl√©mentation2]](https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch) (inclut aussi un mod√®le Bi-LSTM-CNN-CRF) ; [[impl√©mentation3]](https://github.com/jidasheng/bi-lstm-crf)  ; [[impl√©mentation4]](http://www.gabormelli.com/RKB/index.php?title=Bidirectional_LSTM/CRF_(BiLTSM-CRF)_Training_System) ; [[impl√©mentation5]](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html) (avec tensorflow)\n",
        "* BiLSTM-CNN-CRF Implementation for Sequence Tagging (extension with the ELMo representations) : Reimers, Nils, and Gurevych, Iryna, Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), September 2017, Copenhagen, Denmark, 338-348 ; [[article]](http://aclweb.org/anthology/D17-1035) ; [[impl√©mentation]](https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf)\n",
        "* Le 3e mod√®le le plus performant en 2020 sur la t√¢che NER sans ressources externes : Ying Luo, Fengshun Xiao, and Hai Zhao. Hierarchical contextualized representation for named entity recognition. In AAAI, pages 8441‚Äì8448, 2020 ; [[impl√©mentation]](https://github.com/cslydia/Hire-NER) ; Utilise [NCRF++: An Open-source Neural Sequence Labeling Toolkit](https://github.com/jiesutd/NCRFpp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inik1ZFZ2tOa"
      },
      "source": [
        "---\n",
        "#¬†Bidirectional LSTM-CRF Impl√©mentation de (Huang et al., 2015)\n",
        "\n",
        "Le code dans les cellules suivantes provient de l'[impl√©mentation 3](https://github.com/jidasheng/bi-lstm-crf/) de (Huang et al., 2015). Celle-ci s'appuie sur la biblioth√®que pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLo59Npr9S5-"
      },
      "source": [
        "\n",
        "### VOTRE TRAVAIL \n",
        "* Ex√©cutez les cellules sans passer trop de temps √† comprendre les d√©tails de l'impl√©mentation. R√©pondez aux questions quand vous y √™tes invit√©.\n",
        "* Passez en type d'ex√©cution \"gpu\". Plus tard vous ferez un test en type \"None\" c'est-√†-dire \"cpu\" afin d'avoir une id√©e des temps d'entra√Ænement de l'architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZoON5ru4-Ed"
      },
      "source": [
        "##¬†Installation des d√©pendances \n",
        "\n",
        "üí° La cellule suivante requiert 2 ex√©cutions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr7DMncsp_uN",
        "outputId": "00bf6ad6-cc02-49cc-ec35-eb540511484b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "#¬†!pip install torch #¬†1.10.0\n",
        "# RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor\n",
        "# https://stackoverflow.com/questions/54358280/packed-padded-sequence-gives-error-when-used-with-gpu\n",
        "!pip install torch==1.6.0 # torchvision==0.7.0\n",
        "#!pip install torchtext\n",
        "# The torchtext package consists of data processing utilities and popular datasets for natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK2lrEy5PSH"
      },
      "source": [
        "V√©rifie que le hardware de votre machine dispose d'un gpu et que la version de torch install√©e est bien celle attendue.\n",
        "\n",
        "‚ö†Ô∏è Attention, si la version n'est pas celle attendue alors red√©marrer l'environnement d'ex√©cution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOVf1iODp5Gj",
        "outputId": "d276984b-1518-4fe4-a374-c066239131bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 16 21:42:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "cuda:0\n",
            "1.6.0\n"
          ]
        }
      ],
      "source": [
        "# info sur le gpu \n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "import torch\n",
        "# Get cpu or gpu device for training (un peu redondant avec le code pr√©c√©dent... mais montre une variante via torch)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)\n",
        "\n",
        "#¬†version de torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWm5eZxlLKrh"
      },
      "source": [
        "En GCollab Pro\n",
        "\n",
        "```\n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   37C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                                  |\n",
        "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "|        ID   ID                                                   Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVPsXbxLHWi"
      },
      "source": [
        "Impl√©mentations de m√©thodes utiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jcxrgoT_LHgo"
      },
      "outputs": [],
      "source": [
        "# utilities \n",
        "def flatten(t):\n",
        "  # applatie une liste de listes en une unique liste... \n",
        "  #¬†[[a, b], [c], [d, e, f]] -> [a, b, c, d, e, f]\n",
        "  return [item for sublist in t for item in sublist]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzPQYDlMtsu"
      },
      "source": [
        "## Impl. couche _CRF_\n",
        "D√©finition de la couche CRF qui retourne la s√©quence d'√©tiquettes la plus probable correspondant √† une s√©quence de mots donn√©e.\n",
        "\n",
        "* Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/crf.py\n",
        "* A l'aide d'un treillis mots x etiquettes, l'_algorithme Viterbi_ retourne la s√©quence d'√©tiquettes la plus probables pour une s√©quence de mots (d'une phrase) donn√©e.\n",
        "* Les _probabilit√©s de transition_ sont des probabilit√©s conditionnelles. Il s'agit de la probabilit√© d'avoir une √©tiquette sachant 1 historique d'√©tiquettes `P(t_i|t_i-1)` (ici dans un mod√®le bigramme). Les _probabilit√©s d'√©mission_ sont les probabilit√©s des mots `P(w_i | t_i)` √† √™tre g√©n√©r√©s par leur propre √©tiquette. \n",
        "* En savoir plus sur le [\"Sequence Labeling\"](https://courses.engr.illinois.edu/cs447/fa2018/Slides/Lecture07.pdf). \n",
        "* En savoir plus sur le [\"CRF\"](http://www.cs.columbia.edu/~mcollins/crf.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kyEIEbBGMp8j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def log_sum_exp(x):\n",
        "    \"\"\"calculate log(sum(exp(x))) = max(x) + log(sum(exp(x - max(x))))\n",
        "    \"\"\"\n",
        "    max_score = x.max(-1)[0]\n",
        "    return max_score + (x - max_score.unsqueeze(-1)).exp().sum(-1).log()\n",
        "\n",
        "\n",
        "IMPOSSIBLE = -1e4\n",
        "\n",
        "class CRF(nn.Module):\n",
        "    \"\"\"General CRF module.\n",
        "    The CRF module contain a inner Linear Layer which transform the input from features space to tag space.\n",
        "\n",
        "    :param in_features: number of features for the input\n",
        "    :param num_tag: number of tags. DO NOT include START, STOP tags, they are included internal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, num_tags):\n",
        "        super(CRF, self).__init__()\n",
        "\n",
        "        self.num_tags = num_tags + 2\n",
        "        self.start_idx = self.num_tags - 2\n",
        "        self.stop_idx = self.num_tags - 1\n",
        "\n",
        "        self.fc = nn.Linear(in_features, self.num_tags)\n",
        "\n",
        "        # transition factor, Tij mean transition from j to i\n",
        "        self.transitions = nn.Parameter(torch.randn(self.num_tags, self.num_tags), requires_grad=True)\n",
        "        self.transitions.data[self.start_idx, :] = IMPOSSIBLE\n",
        "        self.transitions.data[:, self.stop_idx] = IMPOSSIBLE\n",
        "\n",
        "    def forward(self, features, masks):\n",
        "        \"\"\"decode tags\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "        return self.__viterbi_decode(features, masks[:, :features.size(1)].float())\n",
        "\n",
        "    def loss(self, features, ys, masks):\n",
        "        \"\"\"negative log likelihood loss\n",
        "        B: batch size, L: sequence length, D: dimension\n",
        "\n",
        "        :param features: [B, L, D]\n",
        "        :param ys: tags, [B, L]\n",
        "        :param masks: masks for padding, [B, L]\n",
        "        :return: loss\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "\n",
        "        L = features.size(1)\n",
        "        masks_ = masks[:, :L].float()\n",
        "\n",
        "        forward_score = self.__forward_algorithm(features, masks_)\n",
        "        gold_score = self.__score_sentence(features, ys[:, :L].long(), masks_)\n",
        "        loss = (forward_score - gold_score).mean()\n",
        "        return loss\n",
        "\n",
        "    def __score_sentence(self, features, tags, masks):\n",
        "        \"\"\"Gives the score of a provided tag sequence\n",
        "\n",
        "        :param features: [B, L, C]\n",
        "        :param tags: [B, L]\n",
        "        :param masks: [B, L]\n",
        "        :return: [B] score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        # emission score\n",
        "        emit_scores = features.gather(dim=2, index=tags.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # transition score\n",
        "        start_tag = torch.full((B, 1), self.start_idx, dtype=torch.long, device=tags.device)\n",
        "        tags = torch.cat([start_tag, tags], dim=1)  # [B, L+1]\n",
        "        trans_scores = self.transitions[tags[:, 1:], tags[:, :-1]]\n",
        "\n",
        "        # last transition score to STOP tag\n",
        "        last_tag = tags.gather(dim=1, index=masks.sum(1).long().unsqueeze(1)).squeeze(1)  # [B]\n",
        "        last_score = self.transitions[self.stop_idx, last_tag]\n",
        "\n",
        "        score = ((trans_scores + emit_scores) * masks).sum(1) + last_score\n",
        "        return score\n",
        "\n",
        "    def __viterbi_decode(self, features, masks):\n",
        "        \"\"\"decode to tags using viterbi algorithm\n",
        "        B: batch size, L: sequence length, D: dimension\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        bps = torch.zeros(B, L, C, dtype=torch.long, device=features.device)  # back pointers\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        max_score = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        max_score[:, self.start_idx] = 0\n",
        "\n",
        "        for t in range(L):\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            emit_score_t = features[:, t]  # [B, C]\n",
        "\n",
        "            # [B, 1, C] + [C, C]\n",
        "            acc_score_t = max_score.unsqueeze(1) + self.transitions  # [B, C, C]\n",
        "            acc_score_t, bps[:, t, :] = acc_score_t.max(dim=-1)\n",
        "            acc_score_t += emit_score_t\n",
        "            max_score = acc_score_t * mask_t + max_score * (1 - mask_t)  # max_score or acc_score_t\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        max_score += self.transitions[self.stop_idx]\n",
        "        best_score, best_tag = max_score.max(dim=-1)\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_paths = []\n",
        "        bps = bps.cpu().numpy()\n",
        "        for b in range(B):\n",
        "            best_tag_b = best_tag[b].item()\n",
        "            seq_len = int(masks[b, :].sum().item())\n",
        "\n",
        "            best_path = [best_tag_b]\n",
        "            for bps_t in reversed(bps[b, :seq_len]):\n",
        "                best_tag_b = bps_t[best_tag_b]\n",
        "                best_path.append(best_tag_b)\n",
        "            # drop the last tag and reverse the left\n",
        "            best_paths.append(best_path[-2::-1])\n",
        "\n",
        "        return best_score, best_paths\n",
        "\n",
        "    def __forward_algorithm(self, features, masks):\n",
        "        \"\"\"calculate the partition function with forward algorithm.\n",
        "        TRICK: log_sum_exp([x1, x2, x3, x4, ...]) = log_sum_exp([log_sum_exp([x1, x2]), log_sum_exp([x3, x4]), ...])\n",
        "\n",
        "        :param features: features. [B, L, C]\n",
        "        :param masks: [B, L] masks\n",
        "        :return:    [B], score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        scores = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        scores[:, self.start_idx] = 0.\n",
        "        trans = self.transitions.unsqueeze(0)  # [1, C, C]\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for t in range(L):\n",
        "            emit_score_t = features[:, t].unsqueeze(2)  # [B, C, 1]\n",
        "            score_t = scores.unsqueeze(1) + trans + emit_score_t  # [B, 1, C] + [1, C, C] + [B, C, 1] => [B, C, C]\n",
        "            score_t = log_sum_exp(score_t)  # [B, C]\n",
        "\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            scores = score_t * mask_t + scores * (1 - mask_t)\n",
        "        scores = log_sum_exp(scores + self.transitions[self.stop_idx])\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byc18sR5FNSu"
      },
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Param√®tres > Cocher \"affichage de la num√©rotation des lignes\"\n",
        "\n",
        "* Quel est le nom de la _loss function_ ? A quelle ligne est-ce sp√©cifi√©e ?\n",
        "* En quelques mots, √† quoi sert l'algorithme de Viterbi ? Cherchez sur le web...\n",
        "\n",
        "Pour aller plus loin, en apprendre davantage sur quelques [_loss functions_](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZZnpMvM20E"
      },
      "source": [
        "## Impl. couche _Bi-LSTM CRF_ \n",
        "\n",
        "La classe suivante impl√©mente un mod√®le Bi-LSTM CRF\n",
        "- Construction des embeddings de la s√©quence\n",
        "- Capture du contexte avec une cellule RNN \n",
        "- Pr√©diction de la s√©quence d'√©tiquetage √† l'aide de la cellule CRF qui prend comme input la sortie du RNN\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/model.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FvjvPnHDM3Nx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class BiRnnCrf(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_rnn_layers=1, rnn=\"lstm\"):\n",
        "        super(BiRnnCrf, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tagset_size = tagset_size\n",
        "\n",
        "        #¬†D√©claration d'une couche d'Embeddings  \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #¬†D√©claration d'une couche RNN bidirectionnelle\n",
        "        RNN = nn.LSTM if rnn == \"lstm\" else nn.GRU\n",
        "        self.rnn = RNN(embedding_dim, hidden_dim // 2, num_layers=num_rnn_layers,\n",
        "                       bidirectional=True, batch_first=True)\n",
        "        \n",
        "        #¬†D√©claration d'une couche CRF\n",
        "        self.crf = CRF(hidden_dim, self.tagset_size)\n",
        "\n",
        "    def __build_features(self, sentences):\n",
        "        \"\"\"\n",
        "        sentences contient l'√©quivalent d'un batch de sentences ;\n",
        "        chaque sentence √©tant de dimension max_seq_len \n",
        "        et contenant les indices des mots \n",
        "        type(sentences): <class 'torch.Tensor'>\n",
        "        sentences.shape: torch.Size([1000, 100]) #¬†valeur par d√©faut\n",
        "        More details on Tensors: https://pytorch.org/docs/stable/tensors.html\n",
        "        \"\"\"\n",
        "        #print ('__build_features')\n",
        "        #print(\"type sentences {}\".format(type(sentences))) #¬†<class 'torch.Tensor'>\n",
        "        #print(\"shape sentences {}\".format(sentences.shape)) #¬†torch.Size([1000, 100])\n",
        "        #print ('sentences[0]:', sentences[0]) \n",
        "        \"\"\"\n",
        "        sentences[0]: tensor([27996, 34171, 38501, 49310, 75077, 94514,  7381, 80031, 70853, 80031,\n",
        "        56648, 41074, 75077, 51013, 83722, 91893, 70882,  7213, 55591, 30448,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
        "        device='cuda:0')\"\"\"\n",
        "        \n",
        "        # > identify positions in sentences where there are words\n",
        "        masks = sentences.gt(0) \n",
        "        #print(\"type(masks):{}\".format(type(masks))) #¬†<class 'torch.Tensor'>\n",
        "        \n",
        "        #print ('masks[0]:', masks[0])\n",
        "        \"\"\"\n",
        "        masks[0]: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
        "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False],\n",
        "        device='cuda:0')\"\"\"\n",
        "\n",
        "        # print(\"type(sentences.long()):{}\".format(type(sentences.long()))) #¬†<class 'torch.Tensor'>\n",
        "        #¬†sentences.long() convert the data type of the Tensor to long\n",
        "        #¬†> then return the embedding vector of each word in a sentence \n",
        "        #¬†> set each vector randomly, keeping track of the vector assigned to a given indice   \n",
        "        embeds = self.embedding(sentences.long())\n",
        "        #print(\"type(embeds):{}\".format(type(embeds))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('embeds[0]:', embeds[0])\n",
        "        \"\"\" \n",
        "        embeds[0]: tensor([[ 1.6529, -0.9046,  0.9322,  ..., -0.8712, -1.1555, -1.5031],\n",
        "        [-0.6852,  0.2939, -0.8784,  ..., -0.7400, -0.2376, -1.7276],\n",
        "        [-0.8087,  0.4498, -1.7856,  ..., -1.3986,  0.2591,  0.0371],\n",
        "        ...,\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603],\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603],\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>)\"\"\"\n",
        "\n",
        "        # Returns the sum of each row of the input tensor in the given dimension dim.\n",
        "        #¬†> Summing True and False gives the number of actual words in each sentence\n",
        "        seq_length = masks.sum(1) \n",
        "        # print(\"type(seq_length):{}\".format(type(seq_length))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('seq_length[0]:', seq_length[0])\n",
        "        # seq_length[0]: tensor(20, device='cuda:0')\n",
        "\n",
        "        # Sorts the elements of the input tensor along a given dimension in descending order by value.\n",
        "        #¬†A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.\n",
        "        # > Sort the sentences by their length (descending order)\n",
        "        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n",
        "        #print ('sorted_seq_length[0]:', sorted_seq_length[0])\n",
        "        # sorted_seq_length[0]: tensor(100, device='cuda:0')\n",
        "        #print ('perm_idx[0]:', perm_idx[0])\n",
        "        # perm_idx[0]: tensor(630, device='cuda:0')\n",
        "\n",
        "        # > reorder the embeddings following the sentence length for further processing: packing\n",
        "        # embeds[0] has\n",
        "        embeds = embeds[perm_idx, :]\n",
        "        #print ('embeds[0]:', embeds[0])\n",
        "        \"\"\"\n",
        "        embeds[0]: tensor([[ 0.1470,  1.3863,  0.2156,  ..., -0.1568, -1.1045, -0.1400],\n",
        "        [ 0.3537,  0.2269, -1.4778,  ..., -1.0272, -0.7349,  1.0088],\n",
        "        [-0.4989, -0.1096, -0.6463,  ...,  1.2627,  0.0907,  0.1922],\n",
        "        ...,\n",
        "        [-1.0912,  1.1962, -1.9826,  ..., -0.4356, -1.2736, -1.4505],\n",
        "        [ 0.6587, -1.1465,  1.1382,  ...,  1.4149, -0.6422,  0.2377],\n",
        "        [-0.6448,  1.1332,  1.4744,  ..., -0.7169, -1.2447, -0.5358]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>) \"\"\"\n",
        "\n",
        "        # Packs a Tensor containing padded sequences of variable length.\n",
        "        #¬†input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). \n",
        "        # If batch_first is True, B x T x * input is expected.\n",
        "        #¬†https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "        #\n",
        "        # > the problem is that not all the sentences in the current batch have the same length. \n",
        "        #¬†> Without distinguishing the sentences lengths, to pad all the sequences, \n",
        "        #¬†> you would end up doing max_len * max_len computations, even if you needed less computations wrt the lenght of sentences.\n",
        "        #¬†> PyTorch offers the possibility to pack (group) sentences of the same length \n",
        "        #¬†> and to pass the information to RNN which will internally optimize the computations.\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        # https://stackoverflow.com/questions/59938530/why-do-we-need-pack-padded-sequence-when-we-have-pack-sequence\n",
        "        #¬†TODO use enforce_sorted=False and remove the previous sorting\n",
        "        pack_sequence = pack_padded_sequence(embeds,  lengths=sorted_seq_length,  batch_first=True)\n",
        "        #print(\"type(pack_sequence):{}\".format(type(pack_sequence))) #¬†<class 'torch.nn.utils.rnn.PackedSequence'>\n",
        "        #print ('pack_sequence[0]:', pack_sequence[0])\n",
        "        \"\"\"\n",
        "        pack_sequence[0]: tensor([[ 0.1470,  1.3863,  0.2156,  ..., -0.1568, -1.1045, -0.1400],\n",
        "        [ 0.5597,  2.0953, -0.7236,  ..., -1.4103, -1.6798,  1.3055],\n",
        "        [-0.1927, -0.9563, -0.0153,  ...,  1.2662, -0.6017, -0.1576],\n",
        "        ...,\n",
        "        [-1.6244,  1.0199, -0.1681,  ..., -0.7570, -0.9435, -0.4870],\n",
        "        [-2.3151, -2.2364, -0.4231,  ...,  0.5323, -0.0363, -0.5891],\n",
        "        [ 0.0935, -0.1610, -0.5200,  ...,  0.1851,  0.2965, -0.6004]],\n",
        "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>)\"\"\"\n",
        "\n",
        "        packed_output, _ = self.rnn(pack_sequence)\n",
        "        #print(\"type(packed_output):{}\".format(type(packed_output))) #¬†<class 'torch.nn.utils.rnn.PackedSequence'>\n",
        "        #print ('packed_output[0]:', packed_output[0])\n",
        "        \"\"\"\n",
        "        packed_output[0]: tensor([[ 2.0873e-02,  1.0921e-01, -2.1166e-01,  ...,  4.7033e-02,\n",
        "         -2.1772e-01, -6.2811e-01],\n",
        "        [ 3.9952e-03,  1.4725e-01, -9.1979e-02,  ..., -2.1016e-01,\n",
        "         -1.8077e-01, -1.3867e-01],\n",
        "        [ 1.2813e-02,  4.0637e-02, -2.2237e-01,  ..., -1.9843e-01,\n",
        "          4.1468e-02, -7.4167e-03],\n",
        "        ...,\n",
        "        [ 4.2123e-04,  1.6109e-01, -2.3425e-02,  ..., -7.5010e-02,\n",
        "         -5.0942e-02,  2.3539e-04],\n",
        "        [-3.6322e-01,  1.0884e-01, -1.7367e-01,  ..., -6.3288e-02,\n",
        "         -3.7179e-02, -9.8569e-02],\n",
        "        [ 1.0113e-02,  1.3696e-01, -3.8002e-02,  ..., -2.1368e-01,\n",
        "         -7.6481e-02,  1.1498e-01]], device='cuda:0',\n",
        "       grad_fn=<CudnnRnnBackward>)\"\"\"\n",
        "\n",
        "        #¬†Pads a packed batch of variable length sequences.\n",
        "        #¬†It is an inverse operation to pack_padded_sequence().\n",
        "        #¬†The returned Tensor‚Äôs data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. \n",
        "        #¬†If batch_first is True, the data will be transposed into B x T x * format.\n",
        "        #¬†https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
        "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #print(\"type(lstm_out):{}\".format(type(lstm_out))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('lstm_out[0]:', lstm_out[0])\n",
        "        \"\"\"\n",
        "        lstm_out[0]: tensor([[ 0.0209,  0.1092, -0.2117,  ...,  0.0470, -0.2177, -0.6281],\n",
        "        [ 0.1364,  0.2313, -0.1493,  ...,  0.1805, -0.1467, -0.2619],\n",
        "        [ 0.0967,  0.1473, -0.0139,  ...,  0.0378, -0.2664, -0.3387],\n",
        "        ...,\n",
        "        [-0.3804,  0.0290,  0.0695,  ..., -0.0445,  0.1460,  0.1356],\n",
        "        [-0.2751,  0.2326, -0.0762,  ..., -0.0467,  0.0303,  0.0645],\n",
        "        [-0.2196,  0.1945,  0.0911,  ..., -0.1548, -0.1706,  0.0325]],\n",
        "       device='cuda:0', grad_fn=<SelectBackward>)\"\"\"\n",
        "        \n",
        "        # sort indices perm_idx in ascending order\n",
        "        _, unperm_idx = perm_idx.sort()\n",
        "        # print ('unperm_idx[0]:', unperm_idx[0])\n",
        "        # unperm_idx[0]: tensor(644, device='cuda:0')\n",
        "        lstm_out = lstm_out[unperm_idx, :]\n",
        "        #print ('lstm_out[0]:', lstm_out[0])\n",
        "        \"\"\"\n",
        "        lstm_out[0]: tensor([[-0.3566, -0.0670, -0.0603,  ..., -0.0220, -0.1860,  0.2224],\n",
        "        [-0.1167, -0.1348,  0.0326,  ..., -0.1276, -0.2458, -0.1165],\n",
        "        [-0.0043,  0.0479,  0.2782,  ...,  0.0799, -0.0694, -0.4641],\n",
        "        ...,\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>)\n",
        "        \"\"\"\n",
        "        return lstm_out, masks\n",
        "\n",
        "    def loss(self, xs, tags):\n",
        "        #¬†compute the loss (refers to the crf loss)\n",
        "        features, masks = self.__build_features(xs)\n",
        "        loss = self.crf.loss(features, tags, masks=masks)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, xs):\n",
        "        #¬†construction des features √† partir du batch de sentences\n",
        "        features, masks = self.__build_features(xs)\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        scores, tag_seq = self.crf(features, masks)\n",
        "        return scores, tag_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcA7yrXSDkPP"
      },
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Param√®tres > Cocher \"affichage de la num√©rotation des lignes\". Notez que la section suivante \"visualisation\" peut, via la visualisation \"print\" vous aider √† mieux comprendre l'architecture du r√©seau.\n",
        "\n",
        "* Une couche d'[Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) est une table qui associe √† un mot du vocabulaire (en fait son indice num√©rique) un vecteur d'embeddings. Les valeurs des embeddings sont initialement tir√©es al√©atoirement. Elles peuvent √™tre surcharg√©es en chargeant des embeddings pre-entra√Æn√©es avec Word2Vec, Glove ou FastText par exemple. Par d√©faut (`embedding.weight.requires_grad = True`), ces vecteurs seront consid√©r√©s comme des param√®tres du mod√®le et ils seront \"_fine-tuned_\" durant l'entra√Ænement (`train`) par _\"backpropagation\"_. Indiquez le num√©ro de ligne qui d√©finit la couche d'embedding et celui de la ligne o√π les embeddings sont initialis√©es. \n",
        "Plus d'information sur [embedding-in-pytorch](https://stackoverflow.com/questions/50747947/embedding-in-pytorch) (stackoverflow).\n",
        "* L'impl√©mentation offre deux types de cellules RNN possibles. Indiquez la ligne o√π ce choix est possible. Indiquez la ligne qui sp√©cifie le choix par d√©faut.\n",
        "* Apr√®s la repr√©sentation en embeddings des phrase et avant le passage √† la cellule RNN, quel type de traitement est r√©alis√© ? Indiquez le num√©ro de ligne o√π ce traitement est sp√©cifi√©. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29X-HrgNy8lw"
      },
      "source": [
        "## Visualisation d'un r√©seau dans pytorch\n",
        "\n",
        "Ici sont pr√©sent√©s bri√®vement 2 moyens pour visualiser un r√©seau : `print` et le module torchviz. \n",
        "\n",
        "Visualisation via un `print` d'un objet de la classe. Ici instanciation d'un objet test avec des valeurs de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZchKq1dy8xK",
        "outputId": "a515acb1-9e64-4346-f6cb-82387b1b845b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiRnnCrf(\n",
            "  (embedding): Embedding(20000, 100)\n",
            "  (rnn): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "dummy_vocab_size = 20000\n",
        "dummy_tagset_size = 9\n",
        "dummy_embedding_dim = 100 #100\n",
        "dummy_hidden_dim = 128\n",
        "dummy_num_rnn_layers = 1\n",
        "dummy_rnn = \"lstm\"\n",
        "dummy_batch_size = 1\n",
        "dummy_max_seq_len = 100\n",
        "dummy_model = BiRnnCrf(dummy_vocab_size, dummy_tagset_size, dummy_embedding_dim, dummy_hidden_dim, dummy_num_rnn_layers, dummy_rnn)\n",
        "print (dummy_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualisation d'une couche embedding"
      ],
      "metadata": {
        "id": "p3EgjyHUoOgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_device = 'cpu'\n",
        "dummy_x = torch.randint(0, dummy_vocab_size, (dummy_batch_size, dummy_max_seq_len))\n",
        "print (dummy_x)\n",
        "#dummy_x = dummy_x.to(dummy_device).long()\n",
        "dummy_x = dummy_x.long() \n",
        "print (dummy_x) \n",
        "print ('embedding layer:', dummy_model.embedding(dummy_x))\n",
        "#print (dummy_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BojBDJv-oNHF",
        "outputId": "421d1d23-792f-4593-8e83-5692c60be787"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5485,  3326, 16993, 13009, 18517,  2791,  8920, 14258,  7382, 11825,\n",
            "         18780, 19377,  2827, 13978,  6567, 17210, 18112,  6251, 14769, 12898,\n",
            "         11782, 18073, 19237,  9052, 12133,  3654, 17462,  8081,  7565, 10178,\n",
            "          5267,  5240,  4077, 14258, 16145, 15828,  2134,  7583,   353, 17364,\n",
            "         14090,  8934, 18676, 15896,  3222,  5077, 11283,   579,  5078, 18231,\n",
            "          4049,  8922,   312, 18273, 15863,   665, 19537, 15753,  6760, 12053,\n",
            "         12352, 16911, 17395, 19319,  5938, 13159,  8864,  1901,  2763, 13822,\n",
            "         10850,   763, 12923,  5401,  2778,  3534,  8995, 19229, 19858,  2551,\n",
            "         15993,  1525, 17059, 19567,  3277,  8811, 10362, 18099, 19650,  5774,\n",
            "         15984,  4835, 19030,  1383, 16496,  3452,  7229, 12546, 10725,  4778]])\n",
            "tensor([[ 5485,  3326, 16993, 13009, 18517,  2791,  8920, 14258,  7382, 11825,\n",
            "         18780, 19377,  2827, 13978,  6567, 17210, 18112,  6251, 14769, 12898,\n",
            "         11782, 18073, 19237,  9052, 12133,  3654, 17462,  8081,  7565, 10178,\n",
            "          5267,  5240,  4077, 14258, 16145, 15828,  2134,  7583,   353, 17364,\n",
            "         14090,  8934, 18676, 15896,  3222,  5077, 11283,   579,  5078, 18231,\n",
            "          4049,  8922,   312, 18273, 15863,   665, 19537, 15753,  6760, 12053,\n",
            "         12352, 16911, 17395, 19319,  5938, 13159,  8864,  1901,  2763, 13822,\n",
            "         10850,   763, 12923,  5401,  2778,  3534,  8995, 19229, 19858,  2551,\n",
            "         15993,  1525, 17059, 19567,  3277,  8811, 10362, 18099, 19650,  5774,\n",
            "         15984,  4835, 19030,  1383, 16496,  3452,  7229, 12546, 10725,  4778]])\n",
            "embedding layer: tensor([[[ 2.3755e-01, -1.8148e+00,  8.4090e-01,  ...,  1.2965e-01,\n",
            "          -3.8980e-01, -1.7449e-02],\n",
            "         [-2.0301e-01,  2.2039e+00,  1.6130e+00,  ...,  1.0745e+00,\n",
            "          -1.7371e-01, -6.3110e-01],\n",
            "         [-8.0630e-01,  4.1593e-01,  8.0128e-01,  ...,  1.9949e-03,\n",
            "           8.8846e-01,  1.7105e+00],\n",
            "         ...,\n",
            "         [-1.7269e+00, -1.3936e+00, -9.0724e-01,  ..., -6.3381e-01,\n",
            "           1.3246e+00, -1.7789e-01],\n",
            "         [ 1.9295e+00, -1.8458e-01,  1.2905e+00,  ..., -1.3469e+00,\n",
            "           6.3183e-01,  1.0728e-01],\n",
            "         [ 1.3631e+00, -1.1322e-01,  9.0030e-01,  ...,  2.9770e-01,\n",
            "           1.0195e-01,  5.6682e-01]]], grad_fn=<EmbeddingBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsawG1DhLS9r"
      },
      "source": [
        "**torchviz** requiert l'excution \"forward\" du mod√®le avec des donn√©es (√©ventuellement factices) pour produire un graph du r√©seau.\n",
        "\n",
        "La figure est impitable pour un non initi√©. Ne perdez pas de temps √† essayer de la comprendre... Retenez qu'il existe des outils pour aider √† visualiser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "R2SPqWQJ1nVO",
        "outputId": "55e9285c-9763-45fc-c7bc-e08a165d77d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "<class 'tuple'> 2\n",
            "torch.Size([1]) 1\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.6.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rnn_torchviz.png'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# g√©n√©ration d'un batch d'instances √† traiter (chacune avec un nombre de dimension fixe √©ventuellement tronqu√©e ou \"padd√©e\") \n",
        "# randn(d) returns a tensor filled with d random numbers ;\n",
        "# randn(s, d) returns a list of s tensors filled with d random numbers ;   \n",
        "dummy_batch_size = 1\n",
        "dummy_x = torch.rand(dummy_batch_size, 10)\n",
        "print (dummy_x.shape)\n",
        "\n",
        "# ex√©cution \"forward\" du mod√®le\n",
        "dummy_hyp = dummy_model(dummy_x)\n",
        "print (type(dummy_hyp), len(dummy_hyp))\n",
        "print (dummy_hyp[0].shape, len(dummy_hyp[1]))\n",
        "\n",
        "# visualisation (un fichier png est g√©n√©r√© dans le r√©pertoire courant)\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "make_dot(dummy_hyp[0], params=dict(list(dummy_model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9xTak9DOBRP"
      },
      "source": [
        "Pour aller plus loin¬†: [how-do-i-visualize-a-net-in-pytorch (stackoverflow)](https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGl1Xe3C9rVd"
      },
      "source": [
        "---\n",
        "##¬†Impl. pr√©traitement des donn√©es \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViLNx_efOyez"
      },
      "source": [
        "D'abord la d√©finition de m√©thodes \"utils\" pour la phase de pr√©traitement √† savoir la sauvegarde et le chargement de fichiers de configuration e.g. vocabulaire, jeu d'√©tiquettes, param√®tres du mod√®le neuronal (dimension des embeddings...), partition des donn√©es g√©n√©r√©es...\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hUPqw3j6Oyte"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "OOV = \"<OOV>\"\n",
        "\n",
        "\n",
        "def save_json_file(obj, file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhsxcoePO-4C"
      },
      "source": [
        "Puis la classe de pr√©-traitement des donn√©es qui sera initialis√© √† l'aide des chemins des fichiers contenant le vocabulaire, le jeu d'√©tiquettes et les donn√©es annot√©es (phrases d√©coup√©es en mots avec √©tiquettes). Outre charger ces fichiers de configuration et donn√©es, la classe partitionne les donn√©es en ensemble d'entrainement, de validation et de tests (d'apr√®s les param√®tres sp√©cifi√©s par d√©faut ou √† l'appel du syst√®me). Les donn√©es sont aussi \"vectoris√©es\". Il s'agit essentiellement d'une substitution des mots des phrases par leur identifiant num√©rique correspondant √† une entr√©e dans le vocabulaire donn√©.\n",
        "\n",
        "https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vyRopi-JO799"
      },
      "outputs": [],
      "source": [
        "from os.path import join, exists\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "#FILE_VOCAB = \"vocab.json\"\n",
        "#FILE_TAGS = \"tags.json\"\n",
        "#FILE_DATASET = \"dataset.txt\"\n",
        "#FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, config_dir, save_config_dir=None, verbose=True):\n",
        "        self.config_dir = config_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.vocab, self.vocab_dict = self.__load_list_file(FILE_VOCAB, offset=1, verbose=verbose)\n",
        "        print ('Debug: Preprocessor - __init__ - len(self.vocab):', len(self.vocab))\n",
        "        print ('Debug: Preprocessor - __init__ - len(self.vocab_dict):', len(self.vocab_dict))\n",
        "\n",
        "        self.tags, self.tags_dict = self.__load_list_file(FILE_TAGS, verbose=verbose)\n",
        "        if save_config_dir:\n",
        "            self.__save_config(save_config_dir)\n",
        "\n",
        "        self.PAD_IDX = 0\n",
        "        self.OOV_IDX = len(self.vocab) #¬†NH -1 pour √™tre s√ªr d'obtenir un IDX dans le vocab ; ne vient pas de l√†\n",
        "        self.__adjust_vocab()\n",
        "\n",
        "\n",
        "    def __load_list_file(self, file_name, offset=0, verbose=False):\n",
        "        file_path = join(self.config_dir, file_name)\n",
        "        if not exists(file_path):\n",
        "            raise ValueError('\"{}\" file does not exist.'.format(file_path))\n",
        "        else:\n",
        "            elements = load_json_file(file_path)\n",
        "            elements_dict = {w: idx + offset for idx, w in enumerate(elements)}\n",
        "            if verbose:\n",
        "                print(\"config {} loaded\".format(file_path))\n",
        "            return elements, elements_dict\n",
        "\n",
        "\n",
        "    def __adjust_vocab(self):\n",
        "        self.vocab.insert(0, PAD)\n",
        "        self.vocab_dict[PAD] = 0\n",
        "\n",
        "        self.vocab.append(OOV)\n",
        "        self.vocab_dict[OOV] = len(self.vocab) - 1\n",
        "        print ('Debug: Preprocessor - __adjust_vocab - len(self.vocab):', len(self.vocab))\n",
        "        print ('Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict):', len(self.vocab_dict))\n",
        "\n",
        "    def __save_config(self, dst_dir):\n",
        "        char_file = join(dst_dir, FILE_VOCAB)\n",
        "        save_json_file(self.vocab, char_file)\n",
        "\n",
        "        tag_file = join(dst_dir, FILE_TAGS)\n",
        "        save_json_file(self.tags, tag_file)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"tag dict file => {}\".format(tag_file))\n",
        "            print(\"tag dict file => {}\".format(char_file))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __cache_file_path(corpus_dir, max_seq_len):\n",
        "        return join(corpus_dir, FILE_DATASET_CACHE.format(max_seq_len))\n",
        "\n",
        "\n",
        "    def load_dataset(self, corpus_dir, val_split, test_split, max_seq_len):\n",
        "        \"\"\"load the train set\n",
        "        with \n",
        "        B the batch size (actually the corpus size i.e. the number of sentences)\n",
        "        L max_seq_len\n",
        "        T the length of the longest sequence\n",
        "        :return: (xs, ys)\n",
        "            xs: [B, L]\n",
        "            ys: [B, L, C]\n",
        "\n",
        "        \"\"\"\n",
        "        ds_path = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        if not exists(ds_path):\n",
        "            print(\"building dataset {} ...\".format(ds_path))\n",
        "            xs, ys = self.__build_corpus(corpus_dir, max_seq_len)\n",
        "        else:\n",
        "            print(\"loading dataset {} ...\".format(ds_path))\n",
        "            dataset = np.load(ds_path)\n",
        "            xs, ys = dataset[\"xs\"], dataset[\"ys\"]\n",
        "\n",
        "        #print ('load_dataset')\n",
        "        #print(\"type xs {}, ys {}\".format(type(xs), type(ys)))\n",
        "        # type xs <class 'numpy.ndarray'>, ys <class 'numpy.ndarray'>\n",
        "        #print(\"shape xs {}, ys {}\".format(xs.shape, ys.shape))\n",
        "        # shape xs (132257, 100), ys (132257, 100)\n",
        "\n",
        "        #  print ('load_dataset map torch.tensor')\n",
        "        xs, ys = map(\n",
        "            torch.tensor, (xs, ys)\n",
        "        )\n",
        "        #print(\"type xs {}, ys {}\".format(type(xs), type(ys)))\n",
        "        #¬†type xs <class 'torch.Tensor'>, ys <class 'torch.Tensor'>\n",
        "        #print(\"shape xs {}, ys {}\".format(xs.shape, ys.shape))\n",
        "        # shape xs torch.Size([132257, 100]), ys torch.Size([132257, 100])\n",
        "\n",
        "        # split the dataset\n",
        "        total_count = len(xs)\n",
        "        assert total_count == len(ys)\n",
        "        val_count = int(total_count * val_split)\n",
        "        test_count = int(total_count * test_split)\n",
        "        train_count = total_count - val_count - test_count\n",
        "        assert train_count > 0 and val_count > 0\n",
        "\n",
        "        indices = np.cumsum([0, train_count, val_count, test_count])\n",
        "        datasets = [(xs[s:e], ys[s:e]) for s, e in zip(indices[:-1], indices[1:])]\n",
        "        print(\"datasets loaded:\")\n",
        "        for (xs_, ys_), name in zip(datasets, [\"train\", \"val\", \"test\"]):\n",
        "            print(\"\\t{}: {}, {}\".format(name, xs_.shape, ys_.shape))\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def decode_tags(self, batch_tags):\n",
        "        batch_tags = [\n",
        "            [self.tags[t] for t in tags]\n",
        "            for tags in batch_tags\n",
        "        ]\n",
        "        return batch_tags\n",
        "\n",
        "\n",
        "    def sent_to_vector(self, sentence, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(sentence)\n",
        "        # debug\n",
        "        #for c in sentence[:max_seq_len]:\n",
        "        #  if not(c in self.vocab_dict):\n",
        "        #    print ('c {} not in vocab'.format(c))\n",
        "        vec = [self.vocab_dict.get(c, self.OOV_IDX) for c in sentence[:max_seq_len]]\n",
        "\n",
        "        # ici s'op√®re le¬†padding \n",
        "        return vec + [self.PAD_IDX] * (max_seq_len - len(vec))\n",
        "\n",
        "\n",
        "    def tags_to_vector(self, tags, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(tags)\n",
        "        vec = [self.tags_dict[c] for c in tags[:max_seq_len]]\n",
        "\n",
        "        # ici s'op√®re le¬†padding \n",
        "        return vec + [0] * (max_seq_len - len(vec))\n",
        "\n",
        "\n",
        "    def __build_corpus(self, corpus_dir, max_seq_len):\n",
        "      #¬†remove cache files !!!\n",
        "        file_path = join(corpus_dir, FILE_DATASET)\n",
        "        xs, ys = [], []\n",
        "        with open(file_path, encoding=\"utf8\") as f:\n",
        "            for idx, line in tqdm(enumerate(f), desc=\"parsing {}\".format(file_path)):\n",
        "                fields = line.strip().split(\"\\t\")\n",
        "                if len(fields) != 2:\n",
        "                    raise ValueError(\"format error in line {}, tabs count: {}\".format(idx + 1, len(fields) - 1))\n",
        "\n",
        "                sentence, tags = fields\n",
        "\n",
        "                try:\n",
        "                    if sentence[0] == \"[\":\n",
        "                        sentence = json.loads(sentence)\n",
        "                    tags = json.loads(tags)\n",
        "\n",
        "                    #print ('Debug: sentence', sentence)\n",
        "                    #print ('Debug: tags', tags)\n",
        "\n",
        "                    xs.append(self.sent_to_vector(sentence, max_seq_len=max_seq_len))\n",
        "                    ys.append(self.tags_to_vector(tags, max_seq_len=max_seq_len))\n",
        "                    if len(sentence) != len(tags):\n",
        "                        raise ValueError('\"sentence length({})\" != \"tags length({})\" in line {}\"'.format(\n",
        "                            len(sentence), len(tags), idx + 1))\n",
        "                except Exception as e:\n",
        "                    raise ValueError(\"exception raised when parsing line {}\\n\\t{}\\n\\t{}\".format(idx + 1, line, e))\n",
        "\n",
        "        #print ('__build_corpus')\n",
        "        #print(\"type(xs):{}, type(ys):{}\".format(type(xs), type(ys)))\n",
        "        #print(\"len(xs):{}, len(ys):{}\".format(len(xs), len(ys)))\n",
        "\n",
        "        #print(\"shape xs {}, ys {}\".format(xs.shape, ys.shape))\n",
        "\n",
        "        #print ('__build_corpus asarray')\n",
        "        xs, ys = np.asarray(xs), np.asarray(ys)\n",
        "        #print(\"type xs {}, ys {}\".format(type(xs), type(ys)))\n",
        "        #print(\"shape xs {}, ys {}\".format(xs.shape, ys.shape))\n",
        "\n",
        "        # save train set\n",
        "        cache_file = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        np.savez(cache_file, xs=xs, ys=ys)\n",
        "        print(\"dataset cache({}, {}) => {}\".format(xs.shape, ys.shape, cache_file))\n",
        "        print ('xs', xs)\n",
        "        print ('ys', ys)\n",
        "\n",
        "        return xs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRbruF-2vMBv"
      },
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "* A quelle ligne s'op√®re le padding des phrases ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS9TOT0hPgd4"
      },
      "source": [
        "## Impl. entra√Ænement du r√©seau\n",
        "\n",
        "D'abord l'impl√©mentation de la m√©thode `build_model` qui instancie le r√©seau. Les fichiers associ√©s (_model_ et _arguments_) seront sauv√©s (ou charg√©s si un pr√©c√©dent entra√Ænement a d√©j√† eu lieu)  depuis _model_dir_.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oQjVXdb8Pdjp"
      },
      "outputs": [],
      "source": [
        "from os.path import exists, join\n",
        "import torch\n",
        "\n",
        "#FILE_ARGUMENTS = \"arguments.json\"\n",
        "#FILE_MODEL = \"model.pth\"\n",
        "\n",
        "def arguments_filepath(model_dir):\n",
        "    return join(model_dir, FILE_ARGUMENTS)\n",
        "\n",
        "\n",
        "def model_filepath(model_dir):\n",
        "    return join(model_dir, FILE_MODEL)\n",
        "\n",
        "\n",
        "def build_model(args, processor, load=True, verbose=False):\n",
        "\n",
        "    print ('Debug: build_model - len(processor.vocab):', len(processor.vocab))\n",
        "    # NH FIX not actived rnn_type by adding rnn=args['rnn_type']\n",
        "    print ('Debug: build_model - BiRnnCrf')\n",
        "    model = BiRnnCrf(len(processor.vocab), len(processor.tags), embedding_dim=args['embedding_dim'], hidden_dim=args['hidden_dim'], num_rnn_layers=args['num_rnn_layers'], rnn=args['rnn_type'])\n",
        "    print ('Debug: build_model - model:', model)\n",
        "\n",
        "    # weights\n",
        "    model_path = model_filepath(args['model_dir'])\n",
        "    if exists(model_path) and load:\n",
        "        state_dict = torch.load(model_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        if verbose:\n",
        "            print(\"load model weights from {}\".format(model_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "def running_device(device):\n",
        "    if torch.cuda.is_available():\n",
        "      print ('running_device gpu')\n",
        "    else:  print ('running_device cpu')\n",
        "    return device if device else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjmr60SPtZ4"
      },
      "source": [
        "Puis la d√©finition des m√©thodes d√©di√©es √† l'entra√Ænement du mod√®le\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3zYXtKrJPssX"
      },
      "outputs": [],
      "source": [
        "from os import mkdir\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def __eval_model(model, device, dataloader, desc):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # eval\n",
        "        losses, nums = zip(*[\n",
        "            (model.loss(xb.to(device), yb.to(device)), len(xb))\n",
        "            for xb, yb in tqdm(dataloader, desc=desc)])\n",
        "        return np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "\n",
        "def __save_loss(losses, file_path):\n",
        "    pd.DataFrame(data=losses, columns=[\"epoch\", \"batch\", \"train_loss\", \"val_loss\"]).to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "def __save_model(model_dir, model):\n",
        "    model_path = model_filepath(model_dir)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(\"save model => {}\".format(model_path))\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    model_dir = args['model_dir']\n",
        "    if not exists(model_dir):\n",
        "        mkdir(model_dir)\n",
        "#    save_json_file(vars(args), arguments_filepath(model_dir))\n",
        "    save_json_file(args, arguments_filepath(model_dir))\n",
        "\n",
        "    print ('Debug: train - Preprocessor')\n",
        "    preprocessor = Preprocessor(config_dir=args['corpus_dir'], save_config_dir=args['model_dir'], verbose=True)\n",
        "    \n",
        "    print ('Debug: train - build_model')\n",
        "    model = build_model(args, preprocessor, load=args['recovery'], verbose=True)\n",
        "\n",
        "    print ('Debug: train - model:', model)\n",
        "\n",
        "    # loss\n",
        "    loss_path = join(args['model_dir'], \"loss.csv\")\n",
        "    losses = pd.read_csv(loss_path).values.tolist() if args['recovery'] and exists(loss_path) else []\n",
        "\n",
        "    # datasets\n",
        "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = preprocessor.load_dataset(\n",
        "        args['corpus_dir'], args['val_split'], args['test_split'], max_seq_len=args['max_seq_len'])\n",
        "    \n",
        "    print ('train')\n",
        "    print(\"type x_train {}, y_train {}\".format(type(x_train), type(y_train)))\n",
        "    print(\"shape x_train {}, y_train {}\".format(x_train.shape, y_train.shape))\n",
        "    #¬†shape x_train torch.Size([79355, 100]), y_train torch.Size([79355, 100])\n",
        "    train_dl = DataLoader(TensorDataset(x_train, y_train), batch_size=args['batch_size'], shuffle=True)\n",
        "    valid_dl = DataLoader(TensorDataset(x_val, y_val), batch_size=args['batch_size'] * 2)\n",
        "    test_dl = DataLoader(TensorDataset(x_test, y_test), batch_size=args['batch_size'] * 2)\n",
        "\n",
        "    # initialize the optimizer specifying what parameters (tensors) of the model should be updated (through the backward process)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "    device = running_device(args['device'])\n",
        "\n",
        "    # FIXME\n",
        "    model.to(device)\n",
        "\n",
        "    val_loss = 0\n",
        "    best_val_loss = 1e4\n",
        "    for epoch in range(args['num_epoch']):\n",
        "        # train\n",
        "        model.train()\n",
        "        bar = tqdm(train_dl)\n",
        "        for bi, (xb, yb) in enumerate(bar):\n",
        "\n",
        "            #¬†PyTorch _accumule_ (c'est-√†-dire _somme_) les gradients lors des passages en arri√®re (i.e. lorsque le .backward() est appel√© sur le loss tenseur). \n",
        "            # La mise √† zero (nettoyage) des gradients de toutes les param√®tres dans l'optimizer (i.e. W, b) permet d'√©viter que les gradients pointent dans une direction autre que la direction pr√©vue vers le minimum (ou le maximum , en cas d'objectifs de maximisation).\n",
        "            #¬†Ceci est pratique lors de la formation des RNN lorsque l'on d√©marre la boucle d'entra√Ænement.\n",
        "            model.zero_grad()\n",
        "\n",
        "            #¬†Compute the loss\n",
        "            loss = model.loss(xb.to(device), yb.to(device))\n",
        "            \n",
        "            #¬†R√©tro-prolif√©ration/back propagation\n",
        "            # Compute gradients of the parameters (tensors) w.r.t. the loss\n",
        "            #¬†The gradients will be \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes)\n",
        "            #print ('before loss.backward()', xb.grad)\n",
        "            loss.backward()\n",
        "            #¬†print ('after loss.backward()', xb.grad)\n",
        "\n",
        "            # Update the parameters\n",
        "            # The optimizer iterate over all parameters (tensors). It is supposed to update and use their internally stored grad to update their values.\n",
        "            optimizer.step()\n",
        "            \n",
        "            bar.set_description(\"{:2d}/{} loss: {:5.2f}, val_loss: {:5.2f}\".format(\n",
        "                epoch+1, args['num_epoch'], loss, val_loss))\n",
        "            losses.append([epoch, bi, loss.item(), np.nan])\n",
        "\n",
        "        # evaluation\n",
        "        val_loss = __eval_model(model, device, dataloader=valid_dl, desc=\"eval\").item()\n",
        "        # save losses\n",
        "        losses[-1][-1] = val_loss\n",
        "        __save_loss(losses, loss_path)\n",
        "\n",
        "        # save model\n",
        "        if not args['save_best_val_model'] or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            __save_model(args['model_dir'], model)\n",
        "            print(\"save model(epoch: {}) => {}\".format(epoch, loss_path))\n",
        "\n",
        "    # test\n",
        "    test_loss = __eval_model(model, device, dataloader=test_dl, desc=\"test\").item()\n",
        "    last_loss = losses[-1][:]\n",
        "    last_loss[-1] = test_loss\n",
        "    losses.append(last_loss)\n",
        "    __save_loss(losses, loss_path)\n",
        "    print(\"training completed. test loss: {:.2f}\".format(test_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssoc_QOnVUVH"
      },
      "source": [
        "##  Impl. pr√©diction √† l'aide du r√©seau\n",
        "\n",
        "\n",
        "La classe WordsTagger effectue l'√©tiquetage √† proprement parler d'une nouvelle s√©quence de mots. Elle requiert le chemin vers un mod√®le.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/predict.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7aPsc2UMVTSH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordsTagger:\n",
        "    def __init__(self, model_dir, device=None):\n",
        "        args = load_json_file(arguments_filepath(model_dir))\n",
        "        #args = dict()\n",
        "        args['model_dir'] = model_dir\n",
        "        self.args = args\n",
        "\n",
        "        self.preprocessor = Preprocessor(config_dir=model_dir, verbose=False)\n",
        "        self.model = build_model(self.args, self.preprocessor, load=True, verbose=False)\n",
        "        self.device = running_device(device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def __call__(self, sentences, begin_tags=\"BS\"):\n",
        "        \"\"\"predict texts\n",
        "        :param sentences: a text or a list of text\n",
        "        :param begin_tags: begin tags for the beginning of a span\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not isinstance(sentences, (list, tuple)):\n",
        "            raise ValueError(\"sentences must be a list of sentence\")\n",
        "\n",
        "        try:\n",
        "            sent_tensor = np.asarray([self.preprocessor.sent_to_vector(s) for s in sentences])\n",
        "            sent_tensor = torch.from_numpy(sent_tensor).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                _, tags = self.model(sent_tensor)\n",
        "            tags = self.preprocessor.decode_tags(tags)\n",
        "        except RuntimeError as e:\n",
        "            print(\"*** runtime error: {}\".format(e))\n",
        "            raise e\n",
        "        return tags, self.tokens_from_tags(sentences, tags, begin_tags=begin_tags)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokens_from_tags(sentences, tags_list, begin_tags):\n",
        "        \"\"\"extract entities from tags\n",
        "        :param sentences: a list of sentence\n",
        "        :param tags_list: a list of tags\n",
        "        :param begin_tags:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not tags_list:\n",
        "            return []\n",
        "\n",
        "        def _tokens(sentence, ts):\n",
        "            # begins: [(idx, label), ...]\n",
        "            all_begin_tags = begin_tags + \"O\"\n",
        "            begins = [(idx, t[2:]) for idx, t in enumerate(ts) if t[0] in all_begin_tags]\n",
        "            begins = [\n",
        "                         (idx, label)\n",
        "                         for idx, label in begins\n",
        "                         if ts[idx] != \"O\" or (idx > 0 and ts[idx - 1] != \"O\")\n",
        "                     ] + [(len(ts), \"\")]\n",
        "\n",
        "            tokens_ = [(sentence[s:e], label) for (s, label), (e, _) in zip(begins[:-1], begins[1:]) if label]\n",
        "            return [((t, tag) if tag else t) for t, tag in tokens_]\n",
        "\n",
        "        tokens_list = [_tokens(sentence, ts) for sentence, ts in zip(sentences, tags_list)]\n",
        "        return tokens_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYXTMDv5tQqd"
      },
      "source": [
        "##¬†Impl. m√©thode d'√©valuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x6-Uoe6EtShi"
      },
      "outputs": [],
      "source": [
        "# Measures definition\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def results_per_class(labels, y_ref, y_hyp):\n",
        "  # Inspect per-class results in more detail:\n",
        "  sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        "  )\n",
        "  # print ('y_ref', len(y_ref), 'y_hyp', len(y_hyp), 'sorted_labels', len(sorted_labels))\n",
        "  return classification_report(flatten(y_ref), flatten(y_hyp), labels=sorted_labels, digits=3)\n",
        "\n",
        "import re \n",
        "def normalise_labels(sentences):\n",
        "  # normalise les sorties des √©tiquettes NER utilis√©es par les diff√©rents \n",
        "  # syst√®mes afin de les rendre comparable\n",
        "  new_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    new_sentence = list()\n",
        "    for label in sentence:\n",
        "      if label != 'O':\n",
        "        label = re.sub('^[A-Z]-','', label)\n",
        "      new_sentence.append(label)\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjl9xpPIIj54"
      },
      "source": [
        "---\n",
        "# Pr√©paration des corpus d'entra√Ænement et de test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_WA1wg_JdOP"
      },
      "source": [
        "## Donn√©es d'entra√Ænement WikiNER et fichiers de configuration requis\n",
        "\n",
        "R√©cup√©ration des donn√©es d'entra√Ænement Wikiner et pr√©pation des fichiers de configuration : vocabulaire, √©tiquettes et donn√©es au format du code utilis√©.\n",
        "\n",
        "Apr√®s ex√©cution de la cellule, consulter le r√©pertoire `data` pour y trouver les fichiers tagset, vocab et txt produits pour le syst√®me NER pr√©c√©demment d√©fini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXeEbpt4eJ2C",
        "outputId": "d4d54d06-12b5-4638-ff86-e91d88933ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‚Äòdata/wikiner_ud.joblib.bz2‚Äô already there; not retrieving.\n",
            "\n",
            "bzip2: Output file data/wikiner_ud.joblib already exists.\n",
            "#sentences:  132257\n",
            "#vocab:  108023\n",
            "#tags in tagset:  9\n",
            "first sentence: [('Il', 'PRON', 'O'), ('assure', 'VERB', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('suite', 'NOUN', 'O'), ('de', 'ADP', 'I-PER'), ('Saussure', 'NOUN', 'I-PER'), ('le', 'DET', 'O'), ('cours', 'NOUN', 'O'), ('de', 'ADP', 'O'), ('grammaire', 'ADJ', 'O'), ('compar√©e', 'VERB', 'O'), (',', 'PUNCT', 'O'), (\"qu'\", 'SCONJ', 'O'), ('il', 'PRON', 'O'), ('compl√®te', 'VERB', 'O'), ('√†', 'ADP', 'O'), ('partir', 'VERB', 'O'), ('de', 'ADP', 'O'), ('1894', 'NUM', 'O'), ('par', 'ADP', 'O'), ('une', 'DET', 'O'), ('conf√©rence', 'NOUN', 'O'), ('sur', 'ADP', 'O'), (\"l'\", 'DET', 'O'), ('iranien', 'NOUN', 'O'), ('.', 'PUNCT', 'O')]\n",
            "tagset:  {'I-MISC', 'O', 'I-LOC', 'I-PER', 'I-ORG', 'B-PER', 'B-LOC', 'B-ORG', 'B-MISC'}\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data \n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/wikiner_ud.joblib.bz2 -P data\n",
        "!bzip2 -dk data/wikiner_ud.joblib.bz2\n",
        "\n",
        "# Loading the corpus \n",
        "from joblib import load\n",
        "try:\n",
        "    wikiner_corpus\n",
        "except NameError:\n",
        "    wikiner_corpus = load('data/wikiner_ud.joblib') \n",
        "    pass  \n",
        "    \n",
        "# Aper√ßu du nombre de phrases et d'une phrase annot√©e (liste de tokens compos√©s de la forme, de la cat√©gorie grammaticale et de l'√©tiquette BIO correspondant en l'entit√© nomm√©e.\n",
        "print ('#sentences: ', len(wikiner_corpus))\n",
        "\n",
        "#¬†Constitution du vocabulaire de mots et du jeu d'√©tiquettes\n",
        "# 132257\n",
        "vocab = set()\n",
        "tagset = set()\n",
        "for s in wikiner_corpus:\n",
        "  for w,p,n in s:\n",
        "    # case normalization_to_lower()\n",
        "    #vocab.add(w.lower())\n",
        "    # case no normalization\n",
        "    vocab.add(w)\n",
        "    tagset.add(n)\n",
        "print ('#vocab: ', len(vocab))\n",
        "print ('#tags in tagset: ', len(tagset))\n",
        "print ('first sentence:', wikiner_corpus[0]) \n",
        "print ('tagset: ', tagset)\n",
        "# {'B-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-LOC', 'B-PER', 'I-PER', 'O'}\n",
        "\n",
        "\n",
        "# export du corpus (phrase et √©tiquettes) au format attendu par le chargeur de donn√©es de l'application bi_lstm_crf\n",
        "import json\n",
        "with open('data/wikiner_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(wikiner_corpus):\n",
        "      sentence = list()\n",
        "      tags = list()    \n",
        "      for w,p,n in line:\n",
        "        # case normalization_to_lower()\n",
        "        #sentence.append(w.lower())\n",
        "        # case no normalization\n",
        "        sentence.append(w) \n",
        "\n",
        "        tags.append(n)\n",
        "      f.write('{}\\t{}\\n'.format(json.dumps(sentence), json.dumps(tags)))\n",
        "      # NH small corpus\n",
        "      # if (i>2): break\n",
        "\n",
        "\n",
        "# export tagset au format bi_lstm_crf \n",
        "with open('data/wikiner_corpus_tagset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(tagset), f, ensure_ascii=False)\n",
        "\n",
        "\n",
        "# export vocab au format bi_lstm_crf \n",
        "with open('data/wikiner_corpus_vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(vocab), f, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGLGylxgJ8WP"
      },
      "source": [
        "D√©claration du r√©pertoire de donn√©es et des noms des fichiers de vocab, du jeu d'√©tiquettes et du corpus √©tiquet√©s. En fait les noms des repertoires des donn√©es et du mod√®les sont d√©finis un peu plus bas..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cW6hifzxd4Ra"
      },
      "outputs": [],
      "source": [
        "FILE_VOCAB = \"wikiner_corpus_vocab.json\"\n",
        "FILE_TAGS = \"wikiner_corpus_tagset.json\"\n",
        "FILE_DATASET = \"wikiner_corpus.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ismdbZLeRo"
      },
      "source": [
        "D√©claration du r√©pertoire du mod√®le qui sera g√©n√©r√© "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T177RCDzLXSD"
      },
      "outputs": [],
      "source": [
        "FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "FILE_ARGUMENTS = \"arguments.json\"\n",
        "FILE_MODEL = \"model.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CLRzOY3QZOR"
      },
      "source": [
        "## Donn√©es de tests WiNER\n",
        "\n",
        "Pr√©paration des donn√©es notamment en restreignant le jeu d'√©tiquettes consid√©r√©es dans l'√©valuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8aIpI_0sIqW",
        "outputId": "5e50142e-30a0-400c-e25d-d6e1c5fd86f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‚Äòdata/winer_dev.joblib‚Äô already there; not retrieving.\n",
            "\n",
            "#texts: 600\n",
            "labels: ['ORG', 'Event', 'LOC', 'O', 'PER', 'MISC', 'Hour', 'Date']\n",
            "sample of annotated texts: [('Catch', 'NOUN', 'O'), (':', 'PUNCT', 'O'), ('d√©c√®s', 'NOUN', 'O'), ('de', 'ADP', 'O'), ('Bobby', 'PROPN', 'PER'), ('Heenan', 'NOUN', 'PER'), ('18', 'NUM', 'Date'), ('septembre', 'NOUN', 'Date'), ('2017', 'NUM', 'Date'), ('.', 'PUNCT', 'O'), ('‚Äì', 'PUNCT', 'O'), ('Le', 'DET', 'O'), ('manager', 'NOUN', 'O'), ('et', 'CCONJ', 'O'), ('commentateur', 'NOUN', 'O'), ('de', 'ADP', 'O'), ('catch', 'NOUN', 'O'), ('Bobby', 'PROPN', 'PER'), ('Heenan', 'NOUN', 'PER'), ('est', 'AUX', 'O'), ('mort', 'VERB', 'O'), ('hier', 'ADV', 'O'), ('√†', 'ADP', 'O'), (\"l'√¢ge\", 'ADV', 'O'), ('de', 'ADP', 'O'), ('73', 'NUM', 'O'), ('ans.', 'NOUN', 'O'), ('Il', 'PRON', 'O'), ('est', 'AUX', 'O'), ('c√©l√®bre', 'ADJ', 'O'), ('pour', 'ADP', 'O'), ('son', 'DET', 'O'), ('travail', 'NOUN', 'O'), ('en', 'ADP', 'O'), ('tant', 'ADV', 'O'), ('que', 'SCONJ', 'O'), ('manager', 'VERB', 'O'), ('√†', 'ADP', 'O'), (\"l'\", 'DET', 'O'), ('Americain', 'NOUN', 'ORG'), ('Wrestling', 'X', 'ORG'), ('Association', 'NOUN', 'ORG'), ('(', 'PUNCT', 'O'), ('AWA', 'NOUN', 'ORG'), (')', 'PUNCT', 'O'), ('dans', 'ADP', 'O'), ('les', 'DET', 'O'), ('ann√©es', 'NOUN', 'O'), ('1970', 'NUM', 'Date'), ('et', 'CCONJ', 'O'), ('le', 'DET', 'O'), ('d√©but', 'NOUN', 'O'), ('des', 'DET', 'O'), ('ann√©es', 'NOUN', 'O'), ('1980', 'NUM', 'Date'), ('puis', 'CCONJ', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('World', 'PROPN', 'ORG'), ('Wrestling', 'X', 'ORG'), ('Federation', 'X', 'ORG'), ('(', 'PUNCT', 'O'), ('WWF', 'NOUN', 'ORG'), (').', 'PUNCT', 'O'), ('Au', 'VERB', 'O'), ('sein', 'ADJ', 'O'), ('de', 'ADP', 'O'), ('ces', 'DET', 'O'), ('deux', 'PRON', 'O'), ('f√©d√©rations,', 'PUNCT', 'O'), ('il', 'PRON', 'O'), ('est', 'AUX', 'O'), ('le', 'DET', 'O'), ('leader', 'PROPN', 'O'), ('et', 'CCONJ', 'O'), ('porte', 'VERB', 'O'), ('parole', 'ADJ', 'O'), ('du', 'DET', 'O'), ('clan', 'NOUN', 'O'), ('Heenan', 'NOUN', 'PER'), ('Family', 'NOUN', 'PER'), ('compos√©', 'VERB', 'O'), ('notamment', 'ADV', 'O'), ('de', 'ADP', 'O'), ('Nick', 'PROPN', 'PER'), ('Bockwinkel', 'PROPN', 'PER'), ('√†', 'ADP', 'O'), (\"l'\", 'DET', 'O'), ('AWA', 'NOUN', 'ORG'), ('et', 'CCONJ', 'O'), ('de', 'ADP', 'O'), ('Andr√©', 'PROPN', 'PER'), ('the', 'X', 'PER'), ('Giant', 'VERB', 'PER'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('WWF', 'NOUN', 'ORG'), ('.', 'PUNCT', 'O'), ('Il', 'PRON', 'O'), ('devient', 'AUX', 'O'), ('ensuite', 'ADV', 'O'), ('commentateur', 'ADJ', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('fin', 'NOUN', 'O'), ('des', 'DET', 'O'), ('ann√©es', 'NOUN', 'O'), ('1980', 'NUM', 'Date'), (\"d'abord\", 'NOUN', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('WWF', 'NOUN', 'ORG'), ('puis', 'CCONJ', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('World', 'PROPN', 'ORG'), ('Championship', 'X', 'ORG'), ('Wrestling', 'X', 'ORG'), ('.', 'PUNCT', 'O'), ('Il', 'PRON', 'O'), ('est', 'AUX', 'O'), ('membre', 'NOUN', 'O'), ('du', 'DET', 'O'), ('WWE', 'ADJ', 'ORG'), ('Hall', 'PROPN', 'O'), ('of', 'PROPN', 'O'), ('Fame', 'PROPN', 'O'), ('depuis', 'ADP', 'O'), ('2004', 'NUM', 'Date'), ('.', 'PUNCT', 'O')]\n",
            "sample of tokenized text: ['catch', ':', 'd√©c√®s', 'de', 'bobby', 'heenan', '18', 'septembre', '2017', '.', '‚Äì', 'le', 'manager', 'et', 'commentateur', 'de', 'catch', 'bobby', 'heenan', 'est', 'mort', 'hier', '√†', \"l'√¢ge\", 'de', '73', 'ans.', 'il', 'est', 'c√©l√®bre', 'pour', 'son', 'travail', 'en', 'tant', 'que', 'manager', '√†', \"l'\", 'americain', 'wrestling', 'association', '(', 'awa', ')', 'dans', 'les', 'ann√©es', '1970', 'et', 'le', 'd√©but', 'des', 'ann√©es', '1980', 'puis', '√†', 'la', 'world', 'wrestling', 'federation', '(', 'wwf', ').', 'au', 'sein', 'de', 'ces', 'deux', 'f√©d√©rations,', 'il', 'est', 'le', 'leader', 'et', 'porte', 'parole', 'du', 'clan', 'heenan', 'family', 'compos√©', 'notamment', 'de', 'nick', 'bockwinkel', '√†', \"l'\", 'awa', 'et', 'de', 'andr√©', 'the', 'giant', '√†', 'la', 'wwf', '.', 'il', 'devient', 'ensuite', 'commentateur', '√†', 'la', 'fin', 'des', 'ann√©es', '1980', \"d'abord\", '√†', 'la', 'wwf', 'puis', '√†', 'la', 'world', 'championship', 'wrestling', '.', 'il', 'est', 'membre', 'du', 'wwe', 'hall', 'of', 'fame', 'depuis', '2004', '.']\n",
            "before removing: ['ORG', 'Event', 'LOC', 'O', 'PER', 'MISC', 'Hour', 'Date']\n",
            "after removing: ['ORG', 'LOC', 'PER', 'MISC']\n"
          ]
        }
      ],
      "source": [
        "# load the test corpus\n",
        "!mkdir -p data\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/winer_dev.joblib -P data\n",
        "from joblib import load\n",
        "winer_corpus = load('data/winer_dev.joblib')\n",
        "\n",
        "# get the tokens of each text\n",
        "# liste chaque forme de surface de chaque mot de chaque phrase\n",
        "# case normalization_to_lower()\n",
        "#winer_tokens = [[token.lower() for token, pos, label in text] for text in winer_corpus]\n",
        "# case no normalization\n",
        "winer_tokens = [[token for token, pos, label in text] for text in winer_corpus]\n",
        "\n",
        "# liste chaque √©tiquette (label) de chaque mot de chaque phrase\n",
        "winer_ref = [[label for token, pos, label in text] for text in winer_corpus]\n",
        "labels = list(set(flatten(winer_ref)))\n",
        "\n",
        "#\n",
        "print ('#texts:', len(winer_corpus))\n",
        "print ('labels:', labels)\n",
        "\n",
        "print ('sample of annotated texts:', winer_corpus[0])   \n",
        "print ('sample of tokenized text:', winer_tokens[0])   \n",
        "\n",
        "# Il y a beaucoup plus d'entit√©s 'O' que les autres dans le corpus, \n",
        "#¬†mais nous sommes davantage int√©ress√©s par les autres entit√©s. \n",
        "#¬†Pour ne pas biaiser les scores de moyenne, on retire les √©tiquettes qui ne nous int√©ressent pas.\n",
        "print (\"before removing:\", labels)\n",
        "labels_to_remove = ['O', 'Event', 'Date', 'Hour']\n",
        "for l in labels_to_remove:\n",
        "  if l in labels: labels.remove(l)\n",
        "print (\"after removing:\", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeiBr7WGL3Rf"
      },
      "source": [
        "# Exp√©rimentation d'un r√©seau avec des embeddings al√©atoires  \n",
        "\n",
        "On utilise ici indiff√©rement les termes r√©seau entra√Æn√© et mod√®le. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Couche Bi-LSTM CRF\n",
        "\n",
        "Cette exp√©rimentation suppose la d√©finition de la couche Bi-LSTM CRF ci-avant qui ne pr√©d√©finit pas de mod√®le d'embeddings et qui par cons√©quent retourne des embeddings al√©atoires pour chaque mot du vocabulaire."
      ],
      "metadata": {
        "id": "ooR7sJpT7UIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement effectif du mod√®le \n",
        "\n",
        "‚ö†Ô∏è Attention, l'impl√©mentation cherchera √† charger une configuration existante dans le r√©pertoire du mod√®le sp√©cifi√©. Si vous changez le param√©trage alors supprimer les fichiers sp√©cifiques au mod√®le ou bien sp√©cifier un nouveau r√©pertoire pour le nouveau mod√®le.\n",
        "L'erreur `RuntimeError: Error(s) in loading state_dict for BiRnnCrf:` est retourn√©e quand vous lancez un entra√Ænement (`train`) apr√®s avoir modifi√© des param√®tres qui ne sont plus en coh√©rence avec une configuration d√©j√† pr√©sente dans le r√©pertoire `model_dir`."
      ],
      "metadata": {
        "id": "xqINiac_6PVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-ZDugjeXRt_",
        "outputId": "4ca85421-9eb2-47ac-8452-d7b344d6295a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: train - Preprocessor\n",
            "config data/wikiner_corpus_vocab.json loaded\n",
            "Debug: Preprocessor - __init__ - len(self.vocab): 108023\n",
            "Debug: Preprocessor - __init__ - len(self.vocab_dict): 108023\n",
            "config data/wikiner_corpus_tagset.json loaded\n",
            "tag dict file => model_wikiner_vanilla/wikiner_corpus_tagset.json\n",
            "tag dict file => model_wikiner_vanilla/wikiner_corpus_vocab.json\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab): 108025\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict): 108025\n",
            "Debug: train - build_model\n",
            "Debug: build_model - len(processor.vocab): 108025\n",
            "Debug: build_model - BiRnnCrf\n",
            "Debug: build_model - model: BiRnnCrf(\n",
            "  (embedding): Embedding(108025, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "Debug: train - model: BiRnnCrf(\n",
            "  (embedding): Embedding(108025, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "loading dataset data/dataset_cache_100.npz ...\n",
            "datasets loaded:\n",
            "\ttrain: torch.Size([79355, 100]), torch.Size([79355, 100])\n",
            "\tval: torch.Size([26451, 100]), torch.Size([26451, 100])\n",
            "\ttest: torch.Size([26451, 100]), torch.Size([26451, 100])\n",
            "train\n",
            "type x_train <class 'torch.Tensor'>, y_train <class 'torch.Tensor'>\n",
            "shape x_train torch.Size([79355, 100]), y_train torch.Size([79355, 100])\n",
            "running_device gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 1/5 loss: 12.69, val_loss:  0.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:12<00:00,  6.54it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 15.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 0) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 2/5 loss:  9.55, val_loss: 12.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  6.69it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 14.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 1) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 3/5 loss:  8.35, val_loss:  9.42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:12<00:00,  6.53it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 14.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 2) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 4/5 loss:  6.06, val_loss:  7.41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:12<00:00,  6.57it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 19.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 3) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 5/5 loss:  4.84, val_loss:  6.43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:12<00:00,  6.53it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 14.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 4) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 14.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training completed. test loss: 6.25\n",
            "--- 73.49695992469788 seconds ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "args = dict()\n",
        "args['corpus_dir'] = \"data\"  # the corpus directory\n",
        "args['model_dir'] = \"model_wikiner_vanilla\"       # the output directory for model files\n",
        "args['num_epoch'] = 5 # 5 25 50 500                # number of epoch to train\n",
        "args['lr'] = 1e-3                     #¬†learning rate\n",
        "args['weight_decay'] = 0.             # the L2 normalization parameter\n",
        "args['batch_size'] = 1000             # batch size for training\n",
        "args['device'] = None                 # the training device: \"cuda:0\", \"cpu:0\". It will be auto-detected by default\n",
        "args['max_seq_len'] = 100 #¬†100              #¬†max sequence length within training\n",
        "args['val_split'] = 0.2                  #¬†the split for the validation dataset\n",
        "args['test_split'] = 0.2                 # the split for the testing dataset\n",
        "args['recovery'] = \"store_true\"       #¬†continue to train from the saved model in model_dir\n",
        "args['save_best_val_model'] = \"store_true\" # save the model whose validation score is smallest\n",
        "args['embedding_dim'] = 200 # 100 300 500           #¬†the dimension of the embedding layer\n",
        "\n",
        "args['hidden_dim'] = 128              # the dimension of the RNN hidden state\n",
        "args['num_rnn_layers'] = 1 # 1            # the number of RNN layers\n",
        "args['rnn_type'] = \"lstm\"              # RNN type, choice: \"lstm\", \"gru\"\n",
        "#¬†print(args)\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!rm -r model_*\n",
        "train(args)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 162.1955807209015 seconds --- gpu 5 epochs max_seq_len 100 embedding_dim 100 num_rnn_layers 1 val_loss:  4.47 test_loss: 4.27\n",
        "# 1056/10000 loss: -0.00, val_loss: 20.14:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 72/80 [00:12<00:01,  5.97it/s] ~ 4h08 min et 35s mais d√©connexion\n",
        "# --- 1344.2705941200256 seconds --- gpu 100 epochs max_seq_len 100 embedding_dim 300 num_rnn_layers 1 val_loss:   10.35 test_loss: 11.29 avec loss train √† 0.02 d√®s √©poch 80\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKMQ1SrmTTnl"
      },
      "source": [
        "#### VOTRE TRAVAIL\n",
        "\n",
        "* Si vous avez le temps, comparez les types d'√©cutions \"cpu\" et \"gpu\" en regardant le temps approximatif annonc√© pour un entra√Ænement sur 1 √©poque. \n",
        "* Avec les param√®tres par d√©faut, quelle score de loss obtenez-vous pour les donn√©es de validation suite √† la derni√®re √©poque d'entra√Ænement ? Et sur les donn√©es de test ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w08NdCkzR8mM"
      },
      "source": [
        "## Pr√©diction effective du mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLwtv_AAUuDZ"
      },
      "source": [
        "Pr√©diction sur une phrase exemple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDbE4MwVvtTr",
        "outputId": "3ceac9bf-5d41-4854-fd40-d1746032e130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Preprocessor - __init__ - len(self.vocab): 108023\n",
            "Debug: Preprocessor - __init__ - len(self.vocab_dict): 108023\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab): 108025\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict): 108025\n",
            "Debug: build_model - len(processor.vocab): 108025\n",
            "Debug: build_model - BiRnnCrf\n",
            "Debug: build_model - model: BiRnnCrf(\n",
            "  (embedding): Embedding(108025, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "running_device gpu\n",
            "[['I-LOC', 'I-PER', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC']]\n"
          ]
        }
      ],
      "source": [
        "bilstmcrf_model = WordsTagger(model_dir=\"model_wikiner_vanilla\")\n",
        "tags, sequences = bilstmcrf_model([['George', 'W.', 'Bush', 'fut', 'pr√©sident', 'des', '√âtats-Unis', \"d'\", 'Am√©rique', '.']])  # CHAR-based model\n",
        "print(tags)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qll6LyVLecm"
      },
      "source": [
        "Pr√©diction sur WiNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3aL5ctwsXTc",
        "outputId": "96d31236-b925-405f-c61f-537f62ab29eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Preprocessor - __init__ - len(self.vocab): 108023\n",
            "Debug: Preprocessor - __init__ - len(self.vocab_dict): 108023\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab): 108025\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict): 108025\n",
            "Debug: build_model - len(processor.vocab): 108025\n",
            "Debug: build_model - BiRnnCrf\n",
            "Debug: build_model - model: BiRnnCrf(\n",
            "  (embedding): Embedding(108025, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "running_device gpu\n",
            "--- 18.573519468307495 seconds ---\n",
            "\n",
            "bilstmcrf_hyp ['B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-ORG', 'B-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'B-MISC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'I-ORG', 'B-MISC', 'B-MISC']\n",
            "normalized_bilstmcrf_hyp ['MISC', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'ORG', 'ORG', 'ORG', 'MISC', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'MISC', 'ORG', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'ORG', 'MISC', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'MISC', 'MISC', 'ORG', 'ORG', 'ORG', 'ORG', 'MISC', 'MISC', 'ORG', 'MISC', 'MISC', 'MISC', 'ORG', 'MISC', 'MISC', 'MISC', 'MISC', 'MISC', 'ORG', 'ORG', 'ORG', 'MISC', 'MISC', 'MISC', 'MISC', 'ORG', 'ORG', 'ORG', 'MISC', 'MISC', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'ORG', 'MISC', 'MISC']\n",
            "winer_ref ['O', 'O', 'O', 'O', 'PER', 'PER', 'Date', 'Date', 'Date', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'PER', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O']\n",
            "\n",
            "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 5, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 200, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PER      0.015     0.000     0.000      4483\n",
            "        MISC      0.004     0.637     0.007       443\n",
            "         LOC      0.024     0.004     0.007      4724\n",
            "         ORG      0.029     0.427     0.054      3816\n",
            "\n",
            "   micro avg      0.014     0.143     0.026     13466\n",
            "   macro avg      0.018     0.267     0.017     13466\n",
            "weighted avg      0.021     0.143     0.018     13466\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# predict\n",
        "#from bi_lstm_crf.app import WordsTagger\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "bilstmcrf_model = WordsTagger(model_dir=\"model_wikiner_vanilla\") #_vanilla\n",
        "\n",
        "bilstmcrf_hyp = []\n",
        "# pour chaque phrase de wikiner\n",
        "for text in winer_tokens:\n",
        "    tags, sequences = bilstmcrf_model([text])    \n",
        "    bilstmcrf_hyp.append(tags[0])\n",
        "    #print (tags)\n",
        "    #break\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 40.24239158630371 seconds ---\n",
        "# --- 144.3440752029419 seconds ---\n",
        "# --- 33.92570495605469 seconds ---\n",
        "\n",
        "# normalize the hyp labels\n",
        "print()\n",
        "print ('bilstmcrf_hyp', bilstmcrf_hyp[0])\n",
        "normalized_bilstmcrf_hyp = normalise_labels(bilstmcrf_hyp)\n",
        "print ('normalized_bilstmcrf_hyp', normalized_bilstmcrf_hyp[0])\n",
        "print ('winer_ref', winer_ref[0])\n",
        "print()\n",
        "\n",
        "# Evaluate on data \n",
        "print (args)\n",
        "print()\n",
        "print (results_per_class(labels, winer_ref, normalized_bilstmcrf_hyp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rEMG4w79NWt"
      },
      "source": [
        "---\n",
        "### VOTRE TRAVAIL\n",
        "\n",
        "* Conservez les mesures de performance de la pr√©diction sur les donn√©es de test √† l'aide du mod√®le entra√Æn√© avec les hyper-param√®tres par d√©faut. Sans changer les hyper-param√®tres, ex√©cutez une seconde fois les cellules d'entra√Ænement √† ex√©cution de la pr√©diction. Obtenez-vous les m√™mes r√©sultats ? Pourquoi ? Quelle recommandation pr√©conisez-vous pour rendre plus fiable vos observations ?\n",
        "* Par d√©faut, les mots du corpus d'entra√Ænement WikiNER, le vocabulaire qui en est issu et les mots du corpus de tests WiNER ne sont pas normalis√©s. Normalisez en minuscule ces 3 types de donn√©es. Entra√Æner et √©valuer ce nouveau mod√®le. Que peut-on dire des performances avec/sans normalisation en minuscule ? \n",
        "* Jouez avec les hyper-param√®tres d'entra√Ænement du mod√®le tels que nombre d'√©poque, dimension des embeddings (embedding_dim), nombre de couches RNN (num_rnn_layers), type de cellule RNN (rnn_type). D√©terminer l'apport de chaque param√®tre. Discuter les performances en termes de pr√©cision, rappel et micro/macro-F1. Vous pouvez aussi jouer avec la taille des phrases consid√©r√©es (max_seq_len),  nombre de dimension du RNN (hidden state), le taux d'apprentissage (lr) si vous avez le temps... \n",
        "* Dans vos exp√©riences, rencontrez-vous des limites avec le hardware mis √† disposition par gcolab ? \n",
        "* Faire un retour sur les diff√©rents mod√®les que vous avez impl√©ment√©s (y compris √† base de CRF pur).\n",
        "\n",
        "Ci dessous quelques pointeurs sur comment utiliser des mod√®les pr√©-entra√Æn√©s avec pytorch\n",
        "* https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings\n",
        "* https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76 \n",
        "* https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R√©sultats d'ex√©cution\n",
        "\n",
        "Cette section rapporte des sorties d'ex√©cution selon certaines configurations d'hyperparam√®tres. En ce sens, elle r√©pond (partiellement √† des questions ci-avant). Suivant le temps que vous disposez, vous pouvez regarder cette section. A noter que pour une m√™me configuration, trois ex√©cutions ont en g√©n√©ral op√©r√©es pour consolider l'observation. \n",
        "\n",
        "`running_device gpu`"
      ],
      "metadata": {
        "id": "SanV-Rb7pufN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUf3Jco0ygkW"
      },
      "source": [
        "**Hyperparam√®tres par d√©faut et entra√Ænement, vocabulaire et √©valuation sans normalisation.**\n",
        "\n",
        "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 5, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 100, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
        "\n",
        "--- 19.388370513916016 seconds ---\n",
        "\n",
        "```\n",
        "Ex√©cution1\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.026     0.141     0.044      4483\n",
        "        MISC      0.003     0.056     0.006       443\n",
        "         LOC      0.028     0.047     0.035      4724\n",
        "         ORG      0.024     0.011     0.015      3816\n",
        "\n",
        "   micro avg      0.022     0.068     0.033     13466\n",
        "   macro avg      0.020     0.064     0.025     13466\n",
        "weighted avg      0.025     0.068     0.032     13466\n",
        "\n",
        "\n",
        "Ex√©cution2\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.034     0.323     0.062      4483\n",
        "        MISC      0.008     0.183     0.015       443\n",
        "         LOC      0.056     0.029     0.039      4724\n",
        "         ORG      0.032     0.017     0.022      3816\n",
        "\n",
        "   micro avg      0.030     0.129     0.049     13466\n",
        "   macro avg      0.033     0.138     0.034     13466\n",
        "weighted avg      0.040     0.129     0.041     13466\n",
        "\n",
        "\n",
        "Ex√©cution3\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.040     0.657     0.075      4483\n",
        "        MISC      0.004     0.002     0.003       443\n",
        "         LOC      0.034     0.018     0.023      4724\n",
        "         ORG      0.015     0.001     0.001      3816\n",
        "\n",
        "   micro avg      0.040     0.225     0.067     13466\n",
        "   macro avg      0.023     0.169     0.026     13466\n",
        "weighted avg      0.029     0.225     0.034     13466\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kasiYHT5jf5g"
      },
      "source": [
        "**Hyperparam√®tres par d√©faut et corpus d'entra√Ænement, vocabulaire et tests normalis√©s en minuscule**\n",
        "\n",
        "\n",
        "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 5, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 100, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
        "\n",
        "\n",
        "```\n",
        "--- 19.28504467010498 seconds predict ---\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "         PER      0.255     0.690     0.373      4483\n",
        "        MISC      0.014     0.018     0.016       443\n",
        "         LOC      0.485     0.279     0.354      4724\n",
        "         ORG      0.199     0.246     0.220      3816\n",
        "\n",
        "   micro avg      0.267     0.398     0.319     13466\n",
        "   macro avg      0.239     0.308     0.241     13466\n",
        "weighted avg      0.312     0.398     0.311     13466\n",
        "\n",
        "\n",
        "--- 19.466615200042725 seconds predict ---\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "         PER      0.280     0.650     0.392      4483\n",
        "        MISC      0.010     0.251     0.019       443\n",
        "         LOC      0.349     0.036     0.066      4724\n",
        "         ORG      0.539     0.087     0.150      3816\n",
        "\n",
        "   micro avg      0.156     0.262     0.195     13466\n",
        "   macro avg      0.295     0.256     0.157     13466\n",
        "weighted avg      0.369     0.262     0.197     13466\n",
        "\n",
        "\n",
        "--- 19.388370513916016 seconds predict---\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.549     0.631     0.587      4483\n",
        "        MISC      0.038     0.205     0.065       443\n",
        "         LOC      0.411     0.713     0.521      4724\n",
        "         ORG      0.451     0.186     0.263      3816\n",
        "\n",
        "   micro avg      0.404     0.519     0.455     13466\n",
        "   macro avg      0.362     0.434     0.359     13466\n",
        "weighted avg      0.456     0.519     0.455     13466\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcKdDJWCTUE"
      },
      "source": [
        "**25 and 100 epochs avec normalisation**\n",
        "\n",
        "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 100, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 100, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
        "\n",
        "\n",
        "```\n",
        "--- 322.24554419517517 seconds --- train\n",
        "--- 29.44636583328247 seconds --- predict\n",
        "\n",
        " precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.742     0.566     0.642      4483\n",
        "        MISC      0.016     0.609     0.031       443\n",
        "         LOC      0.627     0.622     0.624      4724\n",
        "         ORG      0.360     0.367     0.363      3816\n",
        "\n",
        "   micro avg      0.249     0.530     0.339     13466\n",
        "   macro avg      0.436     0.541     0.415     13466\n",
        "weighted avg      0.569     0.530     0.537     13466\n",
        "\n",
        "\n",
        "--- 323.0248258113861 seconds to train ---\n",
        "--- 19.44614005088806 seconds to predict ---\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.846     0.541     0.660      4483\n",
        "        MISC      0.054     0.427     0.096       443\n",
        "         LOC      0.571     0.754     0.650      4724\n",
        "         ORG      0.733     0.247     0.370      3816\n",
        "\n",
        "   micro avg      0.513     0.529     0.521     13466\n",
        "   macro avg      0.551     0.492     0.444     13466\n",
        "weighted avg      0.691     0.529     0.556     13466\n",
        "\n",
        "\n",
        "100 √©pochs \n",
        "\n",
        "--- 1502.3446514606476 seconds --- train\n",
        "--- 29.44636583328247 seconds --- predict\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.345     0.774     0.478      4483\n",
        "        MISC      0.011     0.634     0.021       443\n",
        "         LOC      0.797     0.510     0.622      4724\n",
        "         ORG      0.506     0.318     0.391      3816\n",
        "\n",
        "   micro avg      0.179     0.548     0.269     13466\n",
        "   macro avg      0.415     0.559     0.378     13466\n",
        "weighted avg      0.538     0.548     0.489     13466\n",
        "```\n",
        "\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYkXgkMA7LXH"
      },
      "source": [
        "**dimension default embeddings  500**\n",
        "\n",
        "--- 19.915265560150146 seconds predict ---\n",
        "\n",
        "\n",
        "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 5, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 500, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
        "\n",
        "```\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.539     0.690     0.605      4483\n",
        "        MISC      0.030     0.260     0.054       443\n",
        "         LOC      0.638     0.752     0.690      4724\n",
        "         ORG      0.248     0.393     0.304      3816\n",
        "\n",
        "   micro avg      0.390     0.613     0.477     13466\n",
        "   macro avg      0.364     0.523     0.413     13466\n",
        "weighted avg      0.474     0.613     0.531     13466\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.380     0.837     0.523      4483\n",
        "        MISC      0.043     0.214     0.072       443\n",
        "         LOC      0.734     0.646     0.687      4724\n",
        "         ORG      0.603     0.239     0.342      3816\n",
        "\n",
        "   micro avg      0.440     0.580     0.501     13466\n",
        "   macro avg      0.440     0.484     0.406     13466\n",
        "weighted avg      0.557     0.580     0.515     13466\n",
        "\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.776     0.548     0.642      4483\n",
        "        MISC      0.021     0.291     0.039       443\n",
        "         LOC      0.735     0.685     0.709      4724\n",
        "         ORG      0.635     0.203     0.308      3816\n",
        "\n",
        "   micro avg      0.439     0.490     0.463     13466\n",
        "   macro avg      0.542     0.432     0.425     13466\n",
        "weighted avg      0.697     0.490     0.551     13466\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLGmWeG5ooO6"
      },
      "source": [
        "---\n",
        "# Exp√©rimentation avec des embeddings pr√©-entrain√©s\n",
        "\n",
        "Le code suivant suppose va red√©finir la couche couche Bi-LSTM CRF. La pr√©c√©dente couche couche Bi-LSTM consid√©rait une couche d'embeddings avec des vecteurs tir√©s al√©atoirement pour chaque mot du vocabulaire (fonctionnement par d√©faut). Dans la nouvelle d√©finition nous allons charger des embeddings et d√©finir le mod√®le Bi-LSTM CRF pour y int√©grer une couche d'embeddings initialis√©e avec un mod√®le d'embeddings pr√©-entra√Æn√©.\n",
        "\n",
        "Mis √† part les cellules pr√©c√©dentes intitul√©es \"ex√©cution de l'entra√Ænement et \"ex√©cution de la pr√©diction sur les donn√©es de tests\", il est n√©cessaire d'ex√©cuter toutes les cellules qui pr√©c√®dent. Celle d√©finissant Bi-LSTM CRF sera √©cras√©e.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collecte d'un mod√®le d'embeddings\n",
        "\n",
        "Jean-Philippe Fauconnier met √† disposition des [mod√®les d'embeddings pr√©-entra√Æn√©s pour le fran√ßais](https://fauconnier.github.io/#data)."
      ],
      "metadata": {
        "id": "SKU2p2tzsriJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhbccXJpo8vR",
        "outputId": "91be0cbe-b2ec-409f-af6d-7e8997ff5eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‚Äòembeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin‚Äô already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin -P embeddings\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin -P embeddings\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut200.bin -P embeddings\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_no_postag_phrase_500_cbow_cut10.bin -P embeddings  # surcharge la RAM √† l'entrainement\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_postag_no_phrase_1000_skip_cut100.bin -P embeddings # ['</s>', 'le_d', 'de_p', 'et_c', 'de_p+d', 'un_d', '√™tre_v', '√†_p', 'son_d', 'en_p']\n",
        "\n",
        "w2v_pretrained_embeddings_path = \"embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
        "#w2v_pretrained_embeddings_path = \"embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\"\n",
        "#w2v_pretrained_embeddings_path = \"embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut200.bin\"\n",
        "#w2v_pretrained_embeddings_path = \"embeddings/frWac_no_postag_phrase_500_cbow_cut10.bin\"\n",
        "#w2v_pretrained_embeddings_path = \"embeddings/frWac_postag_no_phrase_1000_skip_cut100.bin\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwc1FFUrqeJh"
      },
      "source": [
        "## Du mod√®le au tensor √† la couche neuronale d'embeddings\n",
        "\n",
        "Initialiser une couche d'Embedding √† l'aide d'un mod√®le pr√©-entra√Æn√© (cf. [Charger des embeddings pr√©-entra√Æn√©s dans pytorch](https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WCio316Sps9o"
      },
      "outputs": [],
      "source": [
        "#¬†Chargement du mod√®le d'embeddings pr√©-entra√Æn√©s\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "w2v_pretrained_embeddings = KeyedVectors.load_word2vec_format(w2v_pretrained_embeddings_path, binary=True, unicode_errors=\"ignore\")\n",
        "#print (w2v_pretrained_embeddings.vectors[0])\n",
        "\n",
        "import torch\n",
        "# transformation des vecteurs d'embeddings en tensors\n",
        "w2v_pretrained_embeddings_tensors = torch.FloatTensor(w2v_pretrained_embeddings.vectors) \n",
        "\n",
        "# cr√©ation et initialisation d'une couche d'embeddings √† partir d'un mod√®le d'embeddings\n",
        "#import torch.nn as nn\n",
        "torch_embedding = nn.Embedding.from_pretrained(w2v_pretrained_embeddings_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOmBTSuPT7D3"
      },
      "source": [
        "Retourne la s√©quence d'embeddings correspondant √† une s√©quence d'indices de mot d'une phrase tir√©s al√©atoirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HgVavYcTDR5",
        "outputId": "5efbe59a-6559-46f2-97ff-0487b641e044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8465,  7470, 14144, 17723,  5317, 13371,  5805, 13258, 13149,  7162,\n",
            "          8444, 15853,  7460,  6567, 15170,  8524, 18101,  3741, 17938, 10595,\n",
            "         10700, 10883, 12560, 16727, 19874, 15235, 12576, 17770, 15838,  9250,\n",
            "         13318, 12324,  7622,  7773, 13472,   355,    78, 17608,  8518,  8690,\n",
            "          8435, 10500,   748,  2601,  4822,  6635, 18656, 17234, 19605,  7977,\n",
            "         13427, 19636, 12845,  6950,  7600, 16329, 11356,  9931,  4196, 14934,\n",
            "          4139, 16959,  3260,  2826, 19087, 12938, 15177, 10062,  5315,  3574,\n",
            "          4517,  7723, 10338, 17495,  4528, 12723,  2706,  1292, 17979,   653,\n",
            "          4044, 11993,  1856,  8050, 14756,  1124,  4935, 14325, 10475,  4882,\n",
            "         10892,  5428, 12558,  9029, 13008, 14390,  7441,  7889, 16424, 17586]])\n",
            "tensor([[ 8465,  7470, 14144, 17723,  5317, 13371,  5805, 13258, 13149,  7162,\n",
            "          8444, 15853,  7460,  6567, 15170,  8524, 18101,  3741, 17938, 10595,\n",
            "         10700, 10883, 12560, 16727, 19874, 15235, 12576, 17770, 15838,  9250,\n",
            "         13318, 12324,  7622,  7773, 13472,   355,    78, 17608,  8518,  8690,\n",
            "          8435, 10500,   748,  2601,  4822,  6635, 18656, 17234, 19605,  7977,\n",
            "         13427, 19636, 12845,  6950,  7600, 16329, 11356,  9931,  4196, 14934,\n",
            "          4139, 16959,  3260,  2826, 19087, 12938, 15177, 10062,  5315,  3574,\n",
            "          4517,  7723, 10338, 17495,  4528, 12723,  2706,  1292, 17979,   653,\n",
            "          4044, 11993,  1856,  8050, 14756,  1124,  4935, 14325, 10475,  4882,\n",
            "         10892,  5428, 12558,  9029, 13008, 14390,  7441,  7889, 16424, 17586]])\n",
            "tensor([[[-0.0254, -0.6939, -0.1258,  ..., -0.3285, -0.2897,  0.0565],\n",
            "         [-0.4503,  0.0761,  1.4356,  ..., -1.3241, -2.1471,  0.7041],\n",
            "         [ 0.5671,  1.8321,  0.0309,  ...,  0.0178, -0.2419,  1.8077],\n",
            "         ...,\n",
            "         [-0.1463,  0.2732, -0.5769,  ...,  0.1786,  0.1103,  0.0663],\n",
            "         [-0.3985,  0.0498, -0.7112,  ...,  1.2594, -1.2716,  0.1286],\n",
            "         [-0.9908, -0.0584,  0.5498,  ...,  0.1940,  0.4983,  0.1295]]])\n"
          ]
        }
      ],
      "source": [
        "torch_embedding\n",
        "\n",
        "dummy_vocab_size = 20000\n",
        "dummy_batch_size = 1\n",
        "dummy_max_seq_len = 100\n",
        "dummy_device = 'cpu'\n",
        "dummy_x = torch.randint(0, dummy_vocab_size, (dummy_batch_size, dummy_max_seq_len))\n",
        "print (dummy_x)\n",
        "#dummy_x = dummy_x.to(dummy_device).long()\n",
        "dummy_x = dummy_x.long() \n",
        "print (dummy_x) \n",
        "print (torch_embedding(dummy_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k34xlLUwptIi"
      },
      "source": [
        "Acc√©der au vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS12U-Kjpuat",
        "outputId": "d670b1cc-0534-4674-e59c-4adf1d4f3923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#dimensions 200\n",
            "#vocab 155562\n",
            "les n premiers mots du vocabulaire du mod√®le d embeddings: ['</s>', 'de', 'la', 'et', 'le', \"l'\", 'les', '√†', 'des', \"d'\"]\n",
            "mkdir: cannot create directory ‚Äòdata‚Äô: File exists\n"
          ]
        }
      ],
      "source": [
        "print ('#dimensions', w2v_pretrained_embeddings.vector_size)\n",
        "print ('#vocab',len(w2v_pretrained_embeddings.vectors))\n",
        "w2v_pretrained_embeddings_vocab = list(w2v_pretrained_embeddings.vocab)\n",
        "print ('les n premiers mots du vocabulaire du mod√®le d embeddings:', w2v_pretrained_embeddings_vocab[:10])\n",
        "#print (weights[0])\n",
        "# export vocab au format bi_lstm_crf \n",
        "!mkdir data\n",
        "import json\n",
        "with open('data/w2v_pretrained_embeddings_vocab.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(w2v_pretrained_embeddings_vocab, f, ensure_ascii=False)\n",
        "\n",
        "FILE_VOCAB = \"w2v_pretrained_embeddings_vocab.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observer la qualit√© du mod√®le via un exemple de recherche d'embeddings les plus similaires √† l'embedding d'un mot sp√©cifi√© (ici le 100 i√®me mot)"
      ],
      "metadata": {
        "id": "1-MGPrH0witi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# donne une id√©e du contenu des embeddings (et de leur normalisation)\n",
        "print ('voici les mots les plus similaires au mot \"{}\" : {}'.format(w2v_pretrained_embeddings_vocab[100], w2v_pretrained_embeddings.most_similar(w2v_pretrained_embeddings_vocab[100])))\n",
        "#print (w2v_pretrained_embeddings.most_similar(\"int√©ressant_√†\"))"
      ],
      "metadata": {
        "id": "JRxXvlMJwiNn",
        "outputId": "1fcc2508-f021-4e93-cacf-9859b605b8e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voici les mots les plus similaires au mot \"voir\" : [('ici', 0.5783172845840454), ('regarder', 0.5010303258895874), ('aper√ßu', 0.48547330498695374), ('connaitre', 0.45781832933425903), ('visiter', 0.45318037271499634), ('visionner', 0.451629638671875), ('revoir', 0.42287424206733704), ('consulter', 0.41887110471725464), ('cliquant', 0.41748952865600586), ('voici', 0.4161854386329651)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VOTRE TRAVAIL\n",
        "* V√©rifier que le vocabulaire suit la m√™me normalisation que les corpus d'entra√Ænement et de test"
      ],
      "metadata": {
        "id": "sELjWoTWtvXO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKTnD8Pq6BE4"
      },
      "source": [
        "##¬†Impl. couche BiRnnCrf avec embeddings pr√©-entra√Æn√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RFyakt0F5_o1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "#¬†Chargement du mod√®le d'embeddings pr√©-entra√Æn√©s\n",
        "from gensim.models import KeyedVectors\n",
        "import torch\n",
        "\n",
        "class BiRnnCrf(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_rnn_layers=1, rnn=\"lstm\"):\n",
        "        super(BiRnnCrf, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size # +2\n",
        "        self.tagset_size = tagset_size\n",
        "\n",
        "        print ('Debug: BiRnnCrf - __init__ - self.vocab_size:',  self.vocab_size)\n",
        "\n",
        "        #¬†D√©claration d'une couche d'Embeddings  \n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        print ('Debug: BiRnnCrf - __init__ - w2v_pretrained_embeddings_tensors.size:', len(w2v_pretrained_embeddings_tensors))\n",
        "        # at this point vocab_size includes PAD and OOV words but not w2v_pretrained_embeddings_tensors\n",
        "        # so if necessary we generate dedicated tensors and include them to w2v_pretrained_embeddings_tensors \n",
        "        # (at the position, where they are expected to be )\n",
        "        if self.vocab_size != len(w2v_pretrained_embeddings_tensors):\n",
        "          pad_tensor = torch.randn(1,  self.embedding_dim)\n",
        "          oov_tensor = torch.randn(1,  self.embedding_dim)\n",
        "          extended_w2v_pretrained_embeddings_tensors = torch.cat ((pad_tensor,w2v_pretrained_embeddings_tensors,oov_tensor),0)\n",
        "        self.embedding = nn.Embedding.from_pretrained(extended_w2v_pretrained_embeddings_tensors)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # Get embeddings for index 1\n",
        "        #input = torch.LongTensor([1])\n",
        "        #embedding(input)\n",
        "        \n",
        "        #¬†D√©claration d'une couche RNN bidirectionnelle\n",
        "        RNN = nn.LSTM if rnn == \"lstm\" else nn.GRU\n",
        "        self.rnn = RNN(embedding_dim, hidden_dim // 2, num_layers=num_rnn_layers,\n",
        "                       bidirectional=True, batch_first=True)\n",
        "        \n",
        "        #¬†D√©claration d'une couche CRF\n",
        "        self.crf = CRF(hidden_dim, self.tagset_size)\n",
        "\n",
        "    def __build_features(self, sentences):\n",
        "        \"\"\"\n",
        "        sentences contient l'√©quivalent d'un batch de sentences ;\n",
        "        chaque sentence √©tant de dimension max_seq_len \n",
        "        et contenant les indices des mots \n",
        "        type(sentences): <class 'torch.Tensor'>\n",
        "        sentences.shape: torch.Size([1000, 100]) #¬†valeur par d√©faut\n",
        "        More details on Tensors: https://pytorch.org/docs/stable/tensors.html\n",
        "        \"\"\"\n",
        "        #print ('__build_features')\n",
        "        #print(\"type sentences {}\".format(type(sentences))) #¬†<class 'torch.Tensor'>\n",
        "        #print(\"shape sentences {}\".format(sentences.shape)) #¬†torch.Size([1000, 100])\n",
        "        #print ('sentences[0]:', sentences[0]) \n",
        "        \"\"\"\n",
        "        sentences[0]: tensor([27996, 34171, 38501, 49310, 75077, 94514,  7381, 80031, 70853, 80031,\n",
        "        56648, 41074, 75077, 51013, 83722, 91893, 70882,  7213, 55591, 30448,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
        "        device='cuda:0')\"\"\"\n",
        "        \n",
        "        # > identify positions in sentences where there are words\n",
        "        masks = sentences.gt(0) \n",
        "        #print(\"type(masks):{}\".format(type(masks))) #¬†<class 'torch.Tensor'>\n",
        "        \n",
        "        #print ('masks[0]:', masks[0])\n",
        "        \"\"\"\n",
        "        masks[0]: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
        "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False,\n",
        "        False, False, False, False, False, False, False, False, False, False],\n",
        "        device='cuda:0')\"\"\"\n",
        "\n",
        "        # print(\"type(sentences.long()):{}\".format(type(sentences.long()))) #¬†<class 'torch.Tensor'>\n",
        "        #¬†sentences.long() convert the data type of the Tensor to long\n",
        "        #¬†> then return the embedding vector of each word in a sentence \n",
        "        #¬†> set each vector randomly, keeping track of the vector assigned to a given indice\n",
        "\n",
        "        embeds = self.embedding(sentences.long())\n",
        "        #¬†embeds.requires_grad=False\n",
        "        #print(\"type(embeds):{}\".format(type(embeds))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('embeds[0]:', embeds[0])\n",
        "        \"\"\" \n",
        "        embeds[0]: tensor([[ 1.6529, -0.9046,  0.9322,  ..., -0.8712, -1.1555, -1.5031],\n",
        "        [-0.6852,  0.2939, -0.8784,  ..., -0.7400, -0.2376, -1.7276],\n",
        "        [-0.8087,  0.4498, -1.7856,  ..., -1.3986,  0.2591,  0.0371],\n",
        "        ...,\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603],\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603],\n",
        "        [ 0.1250,  0.4386,  1.4527,  ..., -0.2274,  1.7671, -0.3603]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>)\"\"\"\n",
        "\n",
        "        # Returns the sum of each row of the input tensor in the given dimension dim.\n",
        "        #¬†> Summing True and False gives the number of actual words in each sentence\n",
        "        seq_length = masks.sum(1) \n",
        "        # print(\"type(seq_length):{}\".format(type(seq_length))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('seq_length[0]:', seq_length[0])\n",
        "        # seq_length[0]: tensor(20, device='cuda:0')\n",
        "\n",
        "        # Sorts the elements of the input tensor along a given dimension in descending order by value.\n",
        "        #¬†A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.\n",
        "        # > Sort the sentences by their length (descending order)\n",
        "        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n",
        "        #print ('sorted_seq_length[0]:', sorted_seq_length[0])\n",
        "        # sorted_seq_length[0]: tensor(100, device='cuda:0')\n",
        "        #print ('perm_idx[0]:', perm_idx[0])\n",
        "        # perm_idx[0]: tensor(630, device='cuda:0')\n",
        "\n",
        "        # > reorder the embeddings following the sentence length for further processing: packing\n",
        "        # embeds[0] has\n",
        "        embeds = embeds[perm_idx, :]\n",
        "        #print ('embeds[0]:', embeds[0])\n",
        "        \"\"\"\n",
        "        embeds[0]: tensor([[ 0.1470,  1.3863,  0.2156,  ..., -0.1568, -1.1045, -0.1400],\n",
        "        [ 0.3537,  0.2269, -1.4778,  ..., -1.0272, -0.7349,  1.0088],\n",
        "        [-0.4989, -0.1096, -0.6463,  ...,  1.2627,  0.0907,  0.1922],\n",
        "        ...,\n",
        "        [-1.0912,  1.1962, -1.9826,  ..., -0.4356, -1.2736, -1.4505],\n",
        "        [ 0.6587, -1.1465,  1.1382,  ...,  1.4149, -0.6422,  0.2377],\n",
        "        [-0.6448,  1.1332,  1.4744,  ..., -0.7169, -1.2447, -0.5358]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>) \"\"\"\n",
        "\n",
        "        # Packs a Tensor containing padded sequences of variable length.\n",
        "        #¬†input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). \n",
        "        # If batch_first is True, B x T x * input is expected.\n",
        "        #¬†https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "        #\n",
        "        # > the problem is that not all the sentences in the current batch have the same length. \n",
        "        #¬†> Without distinguishing the sentences lengths, to pad all the sequences, \n",
        "        #¬†> you would end up doing max_len * max_len computations, even if you needed less computations wrt the lenght of sentences.\n",
        "        #¬†> PyTorch offers the possibility to pack (group) sentences of the same length \n",
        "        #¬†> and to pass the information to RNN which will internally optimize the computations.\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        # https://stackoverflow.com/questions/59938530/why-do-we-need-pack-padded-sequence-when-we-have-pack-sequence\n",
        "        #¬†TODO use enforce_sorted=False and remove the previous sorting\n",
        "        pack_sequence = pack_padded_sequence(embeds,  lengths=sorted_seq_length,  batch_first=True)\n",
        "        #print(\"type(pack_sequence):{}\".format(type(pack_sequence))) #¬†<class 'torch.nn.utils.rnn.PackedSequence'>\n",
        "        #print ('pack_sequence[0]:', pack_sequence[0])\n",
        "        \"\"\"\n",
        "        pack_sequence[0]: tensor([[ 0.1470,  1.3863,  0.2156,  ..., -0.1568, -1.1045, -0.1400],\n",
        "        [ 0.5597,  2.0953, -0.7236,  ..., -1.4103, -1.6798,  1.3055],\n",
        "        [-0.1927, -0.9563, -0.0153,  ...,  1.2662, -0.6017, -0.1576],\n",
        "        ...,\n",
        "        [-1.6244,  1.0199, -0.1681,  ..., -0.7570, -0.9435, -0.4870],\n",
        "        [-2.3151, -2.2364, -0.4231,  ...,  0.5323, -0.0363, -0.5891],\n",
        "        [ 0.0935, -0.1610, -0.5200,  ...,  0.1851,  0.2965, -0.6004]],\n",
        "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>)\"\"\"\n",
        "\n",
        "        packed_output, _ = self.rnn(pack_sequence)\n",
        "        #print(\"type(packed_output):{}\".format(type(packed_output))) #¬†<class 'torch.nn.utils.rnn.PackedSequence'>\n",
        "        #print ('packed_output[0]:', packed_output[0])\n",
        "        \"\"\"\n",
        "        packed_output[0]: tensor([[ 2.0873e-02,  1.0921e-01, -2.1166e-01,  ...,  4.7033e-02,\n",
        "         -2.1772e-01, -6.2811e-01],\n",
        "        [ 3.9952e-03,  1.4725e-01, -9.1979e-02,  ..., -2.1016e-01,\n",
        "         -1.8077e-01, -1.3867e-01],\n",
        "        [ 1.2813e-02,  4.0637e-02, -2.2237e-01,  ..., -1.9843e-01,\n",
        "          4.1468e-02, -7.4167e-03],\n",
        "        ...,\n",
        "        [ 4.2123e-04,  1.6109e-01, -2.3425e-02,  ..., -7.5010e-02,\n",
        "         -5.0942e-02,  2.3539e-04],\n",
        "        [-3.6322e-01,  1.0884e-01, -1.7367e-01,  ..., -6.3288e-02,\n",
        "         -3.7179e-02, -9.8569e-02],\n",
        "        [ 1.0113e-02,  1.3696e-01, -3.8002e-02,  ..., -2.1368e-01,\n",
        "         -7.6481e-02,  1.1498e-01]], device='cuda:0',\n",
        "       grad_fn=<CudnnRnnBackward>)\"\"\"\n",
        "\n",
        "        #¬†Pads a packed batch of variable length sequences.\n",
        "        #¬†It is an inverse operation to pack_padded_sequence().\n",
        "        #¬†The returned Tensor‚Äôs data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. \n",
        "        #¬†If batch_first is True, the data will be transposed into B x T x * format.\n",
        "        #¬†https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
        "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #print(\"type(lstm_out):{}\".format(type(lstm_out))) #¬†<class 'torch.Tensor'>\n",
        "        #print ('lstm_out[0]:', lstm_out[0])\n",
        "        \"\"\"\n",
        "        lstm_out[0]: tensor([[ 0.0209,  0.1092, -0.2117,  ...,  0.0470, -0.2177, -0.6281],\n",
        "        [ 0.1364,  0.2313, -0.1493,  ...,  0.1805, -0.1467, -0.2619],\n",
        "        [ 0.0967,  0.1473, -0.0139,  ...,  0.0378, -0.2664, -0.3387],\n",
        "        ...,\n",
        "        [-0.3804,  0.0290,  0.0695,  ..., -0.0445,  0.1460,  0.1356],\n",
        "        [-0.2751,  0.2326, -0.0762,  ..., -0.0467,  0.0303,  0.0645],\n",
        "        [-0.2196,  0.1945,  0.0911,  ..., -0.1548, -0.1706,  0.0325]],\n",
        "       device='cuda:0', grad_fn=<SelectBackward>)\"\"\"\n",
        "        \n",
        "        # sort indices perm_idx in ascending order\n",
        "        _, unperm_idx = perm_idx.sort()\n",
        "        # print ('unperm_idx[0]:', unperm_idx[0])\n",
        "        # unperm_idx[0]: tensor(644, device='cuda:0')\n",
        "        lstm_out = lstm_out[unperm_idx, :]\n",
        "        #print ('lstm_out[0]:', lstm_out[0])\n",
        "        \"\"\"\n",
        "        lstm_out[0]: tensor([[-0.3566, -0.0670, -0.0603,  ..., -0.0220, -0.1860,  0.2224],\n",
        "        [-0.1167, -0.1348,  0.0326,  ..., -0.1276, -0.2458, -0.1165],\n",
        "        [-0.0043,  0.0479,  0.2782,  ...,  0.0799, -0.0694, -0.4641],\n",
        "        ...,\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
        "        device='cuda:0', grad_fn=<SelectBackward>)\n",
        "        \"\"\"\n",
        "        return lstm_out, masks\n",
        "\n",
        "    def loss(self, xs, tags):\n",
        "        #¬†compute the loss (refers to the crf loss)\n",
        "        features, masks = self.__build_features(xs)\n",
        "        loss = self.crf.loss(features, tags, masks=masks)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, xs):\n",
        "        #¬†construction des features √† partir du batch de sentences\n",
        "        features, masks = self.__build_features(xs)\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        scores, tag_seq = self.crf(features, masks)\n",
        "        return scores, tag_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89CiyQG1ZlfI"
      },
      "source": [
        "## Ex√©cution de l'entra√Ænement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymdce0aSY82l",
        "outputId": "43c71c5a-64dc-4804-e7a8-4d9ac2c2a2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: train - Preprocessor\n",
            "config data/w2v_pretrained_embeddings_vocab.json loaded\n",
            "Debug: Preprocessor - __init__ - len(self.vocab): 155562\n",
            "Debug: Preprocessor - __init__ - len(self.vocab_dict): 155562\n",
            "config data/wikiner_corpus_tagset.json loaded\n",
            "tag dict file => model_wikiner_vanilla/wikiner_corpus_tagset.json\n",
            "tag dict file => model_wikiner_vanilla/w2v_pretrained_embeddings_vocab.json\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab): 155564\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict): 155564\n",
            "Debug: train - build_model\n",
            "Debug: build_model - len(processor.vocab): 155564\n",
            "Debug: build_model - BiRnnCrf\n",
            "Debug: BiRnnCrf - __init__ - self.vocab_size: 155564\n",
            "Debug: BiRnnCrf - __init__ - w2v_pretrained_embeddings_tensors.size: 155562\n",
            "Debug: build_model - model: BiRnnCrf(\n",
            "  (embedding): Embedding(155564, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "Debug: train - model: BiRnnCrf(\n",
            "  (embedding): Embedding(155564, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "loading dataset data/dataset_cache_100.npz ...\n",
            "datasets loaded:\n",
            "\ttrain: torch.Size([79355, 100]), torch.Size([79355, 100])\n",
            "\tval: torch.Size([26451, 100]), torch.Size([26451, 100])\n",
            "\ttest: torch.Size([26451, 100]), torch.Size([26451, 100])\n",
            "train\n",
            "type x_train <class 'torch.Tensor'>, y_train <class 'torch.Tensor'>\n",
            "shape x_train torch.Size([79355, 100]), y_train torch.Size([79355, 100])\n",
            "running_device gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 1/5 loss: 13.76, val_loss:  0.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  6.94it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 20.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 0) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 2/5 loss: 12.60, val_loss: 13.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  6.96it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 21.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 1) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 3/5 loss: 10.85, val_loss: 11.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  7.00it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 20.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 2) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 4/5 loss:  9.36, val_loss: 10.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  7.02it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 20.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 3) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 5/5 loss:  9.51, val_loss:  9.17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:11<00:00,  6.95it/s]\n",
            "eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 20.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model => model_wikiner_vanilla/model.pth\n",
            "save model(epoch: 4) => model_wikiner_vanilla/loss.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 14.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training completed. test loss: 8.64\n",
            "--- 64.3680202960968 seconds ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "args = dict()\n",
        "args['corpus_dir'] = \"data\"  # the corpus directory\n",
        "args['model_dir'] = \"model_wikiner_vanilla\"       # the output directory for model files\n",
        "args['num_epoch'] = 5 # # 5 25 50 500                # number of epoch to train\n",
        "args['lr'] = 1e-3                     #¬†learning rate\n",
        "args['weight_decay'] = 0.             # the L2 normalization parameter\n",
        "args['batch_size'] = 1000             # batch size for training\n",
        "args['device'] = None                 # the training device: \"cuda:0\", \"cpu:0\". It will be auto-detected by default\n",
        "args['max_seq_len'] = 100 #¬†100              #¬†max sequence length within training\n",
        "args['val_split'] = 0.2                  #¬†the split for the validation dataset\n",
        "args['test_split'] = 0.2                 # the split for the testing dataset\n",
        "args['recovery'] = \"store_true\"       #¬†continue to train from the saved model in model_dir\n",
        "args['save_best_val_model'] = \"store_true\" # save the model whose validation score is smallest\n",
        "args['embedding_dim'] =  w2v_pretrained_embeddings.vector_size # 100 300 500           #¬†the dimension of the embedding layer\n",
        "\n",
        "args['hidden_dim'] = 128              # the dimension of the RNN hidden state\n",
        "args['num_rnn_layers'] = 1 # 1            # the number of RNN layers\n",
        "args['rnn_type'] = \"lstm\"              # RNN type, choice: \"lstm\", \"gru\"\n",
        "#¬†print(args)\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!rm -r model_*\n",
        "train(args)\n",
        "#¬†155562 vs 155564\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 162.1955807209015 seconds --- gpu 5 epochs max_seq_len 100 embedding_dim 100 num_rnn_layers 1 val_loss:  4.47 test_loss: 4.27\n",
        "# 1056/10000 loss: -0.00, val_loss: 20.14:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 72/80 [00:12<00:01,  5.97it/s] ~ 4h08 min et 35s mais d√©connexion\n",
        "# --- 1344.2705941200256 seconds --- gpu 100 epochs max_seq_len 100 embedding_dim 300 num_rnn_layers 1 val_loss:   10.35 test_loss: 11.29 avec loss train √† 0.02 d√®s √©poch 80\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex√©cution de la pr√©diction"
      ],
      "metadata": {
        "id": "nmFu_iuLZKtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "#from bi_lstm_crf.app import WordsTagger\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "bilstmcrf_model = WordsTagger(model_dir=\"model_wikiner_vanilla\") #_vanilla\n",
        "\n",
        "bilstmcrf_hyp = []\n",
        "# pour chaque phrase de wikiner\n",
        "for text in winer_tokens:\n",
        "    tags, sequences = bilstmcrf_model([text])    \n",
        "    bilstmcrf_hyp.append(tags[0])\n",
        "    #print (tags)\n",
        "    #break\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 40.24239158630371 seconds ---\n",
        "# --- 144.3440752029419 seconds ---\n",
        "# --- 33.92570495605469 seconds ---\n",
        "\n",
        "# normalize the hyp labels\n",
        "print()\n",
        "print ('bilstmcrf_hyp', bilstmcrf_hyp[0])\n",
        "normalized_bilstmcrf_hyp = normalise_labels(bilstmcrf_hyp)\n",
        "print ('normalized_bilstmcrf_hyp', normalized_bilstmcrf_hyp[0])\n",
        "print ('winer_ref', winer_ref[0])\n",
        "print()\n",
        "\n",
        "# Evaluate on data \n",
        "print (args)\n",
        "print()\n",
        "print (results_per_class(labels, winer_ref, normalized_bilstmcrf_hyp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iADTGaLKZFWK",
        "outputId": "59f233e1-3165-481b-b985-1dcf9c9e20bf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Preprocessor - __init__ - len(self.vocab): 155562\n",
            "Debug: Preprocessor - __init__ - len(self.vocab_dict): 155562\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab): 155564\n",
            "Debug: Preprocessor - __adjust_vocab - len(self.vocab_dict): 155564\n",
            "Debug: build_model - len(processor.vocab): 155564\n",
            "Debug: build_model - BiRnnCrf\n",
            "Debug: BiRnnCrf - __init__ - self.vocab_size: 155564\n",
            "Debug: BiRnnCrf - __init__ - w2v_pretrained_embeddings_tensors.size: 155562\n",
            "Debug: build_model - model: BiRnnCrf(\n",
            "  (embedding): Embedding(155564, 200)\n",
            "  (rnn): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
            "  (crf): CRF(\n",
            "    (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  )\n",
            ")\n",
            "running_device gpu\n",
            "--- 17.62063455581665 seconds ---\n",
            "\n",
            "bilstmcrf_hyp ['B-ORG', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-ORG', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC']\n",
            "normalized_bilstmcrf_hyp ['ORG', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'ORG', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'ORG', 'ORG', 'ORG', 'ORG', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC']\n",
            "winer_ref ['O', 'O', 'O', 'O', 'PER', 'PER', 'Date', 'Date', 'Date', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'PER', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O']\n",
            "\n",
            "{'corpus_dir': 'data', 'model_dir': 'model_wikiner_vanilla', 'num_epoch': 5, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1000, 'device': None, 'max_seq_len': 100, 'val_split': 0.2, 'test_split': 0.2, 'recovery': 'store_true', 'save_best_val_model': 'store_true', 'embedding_dim': 200, 'hidden_dim': 128, 'num_rnn_layers': 1, 'rnn_type': 'lstm'}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PER      0.000     0.000     0.000      4483\n",
            "        MISC      0.000     0.000     0.000       443\n",
            "         LOC      0.035     0.968     0.067      4724\n",
            "         ORG      0.029     0.048     0.036      3816\n",
            "\n",
            "   micro avg      0.034     0.353     0.063     13466\n",
            "   macro avg      0.016     0.254     0.026     13466\n",
            "weighted avg      0.020     0.353     0.034     13466\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "* Avez-vous une id√©e de piste o√π chercher pour comprendre ces r√©sultats ? \n"
      ],
      "metadata": {
        "id": "m7EaEP748aME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R√©sultats d'ex√©cution"
      ],
      "metadata": {
        "id": "QrTkEFUm9vFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pretrained embeddings 200 (cf. ci-apr√®s)**\n",
        "\n",
        "\n",
        "--- 64.1135082244873 seconds --- pretrained 200"
      ],
      "metadata": {
        "id": "iaD7GauD98kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.577     0.119     0.197      4483\n",
        "        MISC      0.030     0.018     0.022       443\n",
        "         LOC      0.718     0.297     0.421      4724\n",
        "         ORG      0.872     0.057     0.107      3816\n",
        "\n",
        "   micro avg      0.637     0.161     0.257     13466\n",
        "   macro avg      0.549     0.123     0.187     13466\n",
        "weighted avg      0.692     0.161     0.244     13466\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.000     0.000     0.000      4483\n",
        "        MISC      0.012     0.002     0.004       443\n",
        "         LOC      0.036     0.831     0.069      4724\n",
        "         ORG      0.025     0.193     0.045      3816\n",
        "\n",
        "   micro avg      0.034     0.346     0.062     13466\n",
        "   macro avg      0.018     0.257     0.029     13466\n",
        "weighted avg      0.020     0.346     0.037     13466\n",
        "```"
      ],
      "metadata": {
        "id": "8lyHRNOV-GWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparaison des vocabulaires\n",
        "\n",
        "Afin d'expliquer les r√©sultats obtenus avec les mod√®les d'embeddings pr√©-entra√Æn√©s on peut se poser quelques questions sur le vocabulaire partag√© entre les diff√©rentes ressources. \n"
      ],
      "metadata": {
        "id": "tqS7eQdddIpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))\n",
        "print (len(w2v_pretrained_embeddings.vocab))\n",
        "winer_vocab = set(flatten(winer_tokens))\n",
        "print (len(winer_vocab))\n",
        "winer_not_in_w2v_pretrained_embeddings = list() \n",
        "for w in winer_vocab:\n",
        "  if not(w in w2v_pretrained_embeddings.vocab):\n",
        "    winer_not_in_w2v_pretrained_embeddings.append(w)\n",
        "print (len(winer_not_in_w2v_pretrained_embeddings), winer_not_in_w2v_pretrained_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86tt7bNSc0sU",
        "outputId": "ac9117b1-e68d-4fcc-df4f-2322323641d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108023\n",
            "155562\n",
            "19911\n",
            "7414 ['indlala', 'seigneur,', 'pi√®ce.', 'urvoas.', '(entre', \"(qu'il\", \"l'autobus\", 'graciosa', 'hawi', 'viennent,', 'a)', 'd√©cid√©e.', 'retard.', \"s'emparant\", 'locale)', 'sciurus', 'moments.', '%.', \"l'autocar\", 'youtubeurs', 'civils,', 'agathonisi', '7h20', 'manifest√©,', 'mobilisation.', '32,', 'quasi-parfaits', 'patronats', \"o'riordan\", '(format', 'difficile.', 'europ√©ennes,', '554', 'eux,', \"l'allemande.\", 'depaul', 'toujours.', 'l‚Äôint√©rieur.', 'deux]', 'poupe,', 'iar-conicet', \"l'attaquant\", 'court.', 'victoire,', 'attribution,', 'd√©jouaient', 'interdites;', 'misogynes,', 'libyennes.', '7h00', 'caillassages', 's‚Äôemparant', 'lacrymog√®nes.', '(station)', 'ferm√©es.', '55,3', 'mobilisations,', '34√®me', '5,2.', 'voie,', '√©crou√©s.', 'humain.', \"d'instances\", 'r√©alit√©.', 'berkel', '(leurs', 'kenshu', 'quitt√©.', '√©toile,', 'avions.', 'proc√©dure.', 'mig-29', '25,7', 't√©moin,', \"d'ann√©e,\", 'crime.', 'kellyanne', 'dartout', 'divisions.', 'nous,', 'internet,', 'n‚Äôimpose', 'fouill√©e.', 'makhzoumi', 'touch√©s,', 'titres,', 'qu‚Äôun', 'billings', 'alumni', 'filiales.', 'jaunes.', 'guat√©malt√®que,', 'mesurer.', '511', 'renomm√©e,', 'carri√®re.', '9h45', 'beipanjiang', 'zaferes', '16', '\"la', 'embauch√©,', \"l'extr√©misme\", 'cunnama', 'd√©jou√©e,', 'fois,', '1972', 'd‚Äôexplosifs.', 'renomm√©es,', 'consommation.', 'attentat.', \"s'infiltrer\", 'daca,', 'danger.', '78', '1872', 'instrumentalis√©es.', 'dormaient.', 'convergents.', 'moment-l√†,', 'aid√©s.', 'kilju', 'fouill√©s,', 'b√¢timent.', '√©l√©ments,', 'vichot', '13h25', 'puissant.', 'hauschild', 'buzyn', \"s'attribuant\", 'e.', 'cadres,', '√©clat√©,', 'sobral', 'balis√©s,', \"l'√©lectronique.\", 'd√©toner', 'projet,', 'truqu√©s.', '70.3', 'm√©dailles,', 'op√©ration,', 'rozi√®res', 'vie,', 'paix.', 'exactes.', 'lui.', \"l'autre,\", 'grimpeuses', 'r√©guliers,', 'l‚Äô√©tude,', 'populiste,', 'kolyada', 'partag√©es.', 'invade', 'zone,', 'couteau,', 'salle.', 'd√©partement.', '(188', 'ill√©gal.', 'internes,', 'transition,', 'permanence.', 'stables.', 'fond.', 'historique.', \"d'avance,\", 'browmlee', 'kaliszek', 'hier,', 'mateen', '3h30', \"l'assaillant\", 'assassin√©.', 'm√®re,', 'galaxies,', 'ultraconservateur,', 'fiscal.', \"-l'\", \"l'entraineur\", 'score,', 'sont,', '(-', 'rouverte.', 'raelert', 'bilham', '(toutes', 'propose:', \"(aujourd'hui\", 'absolue.', 'constitutionnelle,', 'an,', 'jammeh', \"s'exilant\", 'nombre.', 'luxembourg-ville', \"d'autocar\", 'autostop-citoyen.fr', '√©coutez,', 'anti-flics', 'betto', 'favorable.', \"d'int√©rim\", 'courte,', \"s'acc√©l√®re\", 'km/h', 'peloton,', 'finalement,', \"l'orque\", 'choisi.', '[on]', 'criaient...', 'apesteguia', 'secrets,', 'abandonner.', 'ballons.', 'trot,', 'tennell', \"d'avenir.\", '7,5', '63', 'intemp√©ries.', 't√©l√©phones,', 'rionegro', 'comp√©titrices.', 'ribei', 'fleurettistes', 'politique,', 'viol.', 'boeselager', 'pride.', '8,3', 'public,', 'kickstarter', \"d'ob√©ir\", 'entendues.', '(au', \"n'accorde\", 'misericordiae', 'baccalaur√©at,', 'finale,', 'kienle', 'entr√©es,', 'gh√¥r', 'tourisme,', 'l√©gitimit√©)', 'balance,', '1965', 'al-hussein', 'oliveria', '10h10', 'pages,', 'ric,', 'loi-travail', 'radicaux.', 'raqqa', 'disput√©e.', 'saint-hilaire-de-voust', 'pompiers,', 'l‚Äôune', \"s'√©teindre\", 'p√©ri,', 'suspendues.', 'parasitologues', 'm2', 'n‚Äôai', 'l‚Äôalourdissement', 'p√©rilleuse.', 'l‚Äô√©vacuation', 'laviolette', 'contrats,', \"d'interdire\", 'solaire,', 'bouthan', 'sournois.', '54', 'ha√Øssait.', '10,5', '528,', \"d'√©p√©e\", 'r√©tirait', 'russe,', \"l'int√©rieur,\", 'dernier,', 'immacolata', 'etc..', 'termin√©e.', '11h00', 'gitega', 'l‚Äôencadrement.', '15,9', '147', \"s'agrandir\", '21,2', '3,8', 'service.', 'doux.', 'r.', \"d'√©lite\", 'hanquinquant', 'terrorisme,', 'connu.', 'brexit', 'hiver.', 'intervention,', 'parisien,', 'butore', 'innocents,', 'novitchok', \"s'√©loigner\", \"d'adieu,\", 'lire.', 'nature.', 'anti-terroristes', 'install√©e,', 'interpell√©.', 'jeux.', 'jaunes,', 'agence,', 'nippes', 'produits,', 'utc+2', 'arbre.', 'd√©partements.', 'minutes)', 'relais,', \"l'√©tat.\", 'moins.', 'gjs,', 'binali', '(27', 'arlux-visotec', 'rest√©,', 'bastrykin', 'audiovisuelles,', '188', 'wonsan', 'capacit√©s,', 'shorten', 'utc-4', 'vid√©o-surveillance', '911', 'colombienne,', 'd‚Äôau', 'terre.', 'h7n9', 'tatp,', 'd√©sendettement.', 'serrat', 'l‚Äôhomme,', \"d'abandon\", '(partiellement)', 'sirgant', 'c‚Äôest', 'uramin', 'branchages,', 'leonora', 'lamell√©s,', 'affaires,', 'am√©ricano-russe.', '570', 'savoir,', 'auteurs.', \"l'alg√©rien\", 'b√¢ton-rouge', 'l‚Äô√©paule', 'alaphilippeet', 'l‚Äô√©tude', 'voies.', \"l'ex-agent\", 'entier.', 'occupante,', 'rel√®ve.', 'qabasin', 'cha√Æne,', 'pl√©ni√®re.', 'orbiterait', \"l'attentat\", \"d'√©voluer.\", 'km).', 'demi,', '875m),', 'honrubia', 'jugand', 'giorgia', '(1226', 'out,', 'a,', 'regrette.', \"l'arme.\", 'espagnole.', 'pied,', 'sous-pr√©fecture.', \"d'arriv√©e.\", 'manifestation,', 'russo-am√©ricain', 'conductrice,', 'mati√®re.', '144', 'audition[n√©]', 'd√©voilaient', 'cesson-rennes', 'ouguergouz', 'toi,', 'd‚Äôinterventions,', 'son...', 'd‚Äôalarme', 'bateaux-mouches', 'pompe]', 'complotistes', 'partiellement,', 's√©questrations', 'mi-temps.', 'avant,', 's‚Äôav√©rera', 'destitu√©e', \"d'examen.\", \"d'enfance.\", 'kurde.', ')', '(wts', 'tir√©,', 'discoth√®que,', \"o'donnel\", 'cuisinait', 'yellen', \"l'epr\", 'gays,', 'recherches,', '23√®me', 'bock-c√¥t√©', 'd‚Äôann√©es', \"n'affichera\", 'fortement.', 'champion.', '(slbm)', 'barrage,', 'l‚Äôoccasion', 'sagesse,', 'manifestement,', 'provisoires,', 'europ√©enne,', 'disparus.', 'd√©truite.', 'turque,', 'vedette,', 'n¬∞1', 'infantino', 'laachraoui', 'al-joulani', 'simple,', 'zabiiako', 'blessant.', 'ealat', '(controvers√©e)', '√©vacu√©.', 'mulato', 'choqu√©s.', 'yousef', '√©crou√©.', '16h40', 'choqu√©es,', 'churyumov', '19h15', \"d'adolescents.\", '10-5', 'dimanche),', 's‚Äôav√©rer', 'dimich√®le', 'khabbaz', '(immigration)', 'vindicte,', '0¬∞c)', 'prison.', 'municipal,', \"s'appellera\", 'm√°rton', 'casseyas', 'bruxelles-nord', '(5,6', 'sopita', \"d'abord,\", 'kaitlin', 'anqi', 'chacune.', 'importante,', 'bifurquent', 'soir√©e.', 'pont-de-beauvoisin', \"l'ouest,\", 'irakiennes,', 'pr√©par√©.', 'l‚Äô√¢ge', 'siduhe', 'essay√©,', 'traitements.', 'tonys,', '12,5', 'interdite.', 'laredo', 'institutionnels.', '10', 'guyanais,', 'l‚Äôentreprise', '19h20', 'blyes', 'confondues.', 'd8', \"s'accumulant,\", 'd900', 'faits,', \"l'avance,\", 'ph√©nom√®ne.', \"l'ourson.\", 'annonc√©s.', 'kathrada', 'priarone', 'gjs.', 'emese', '√©poux.', 'irakienne,', '(soit', 'grave,', 'pr√©sident,', 'confirm√©e.', 'gendarmes.', '128', 't√≥th', 'l‚Äôa', 'strat√©gique,', \"l'√©tape,\", 'sortie\",', '23h45', 'parler,', 'relev[ant]', 'n‚Äôayant', 'piste.', 'haut.', 'lib√©r√©.', \"d'√©toiles\", 'betina', \"d'attentat.\", 'd√©c√©d√©,', 'obertone', 'occuper.', 'poursuit.', 'poste.', 'nom,', 'd‚Äôappareil', \"l'ethnie\", 'patronales.', 'vielha', '‚àí', 'nuit.', 'spodyriew', '15', 'apparues.', '1,6', '1999', \"s'attaquant\", \"l'√©mir\", 'identique.', 'tweet', 'centre-droit)', \"s'appr√™tait\", 'communaut√©s.', 'rabot√©es', 'attaques,', '9,6', 'isra√©liens),', 'am√©ricain,', 'course,', 'effet,', 'a54', '2018', 'l‚Äôagence', 'walkowiak', 'tardive.', 'chalen√ßon', \"l'apartheid\", '53', 'mogherini', 'a89', 'transport,', 'virus.', 'ruyiri', '15e', 'c‚Äòest', 'mesure.', 'min√©ralise', 'stockage,', 'mort.', 's‚Äôagirait', 'm,', 'mandat,', 'russe.', '80,', 'b≈ìuf.', 'civile,', 'travaiill√©', \"d'user\", 'venir.', 'couleur.', '550', 'polici√®re,', \"l'√©v√©nement.\", 'm‚Äôa', 'communautaires,', '√©ducatives,', 'agressions.', 'tendance.', 'parfinenko', 'wikileak', '√©lection,', \"l'ambassadeur\", 'morts.', 'yamachiche', 'adjoint,', '8,9', '65,30', '-l‚Äô', '2,4', 'eremenko', 'km/h,', '90%', 'limites,', 'd42d', '21,8', 'avocat,', '√©quipe.', 'leaders.', 'redessinaient', '126', 'duathl√®tes', 'partie.', 'poursuivent.', \"d'incident\", \"d'assassinat\", '1er', 'rapidement,', '3/4)', 'b√ºndchen', 'suicide,', \"l'avantage,\", 'r√©animation,', 'pronostic,', 'l‚Äôassassinat', 'projets.', 'a41n', 'pilote,', 'd√©placement,', 'vaccination,', '12h00', 're√ßu,', 'souverainet√©,', 'ouvert.', 'climatisation).', 'd‚Äôarme.', \"l'emmener\", 'musique,', 'fr√©quents.', 'bilan.', 'continues.', 'citrouille.', 'd√©bat.', \"l'accotement\", 's‚Äôen', 'parvient,', 'palettes,', 'travailler.', \"d'√©claircir\", 'majka', 'franceinfo', '√©tabli,', 'ind√©pendants,', 'enfants.', 'froid.', \"d'italia\", 'd√©clar√©.', '273', 'd‚Äôexploitation', '(ouverte', 'burundais.', \"d'autoroutes,\", '√©touffante,', '39,4', 'meygal', '21h30', 'l‚Äôincendie', 'mai.', 'totalit√©)', '4x100m.', 'manifestants,', 'lecteurs.', 'langues,', 'conf√©rence.', 'criminalis√©s', 'mahjoubi', 'd√©clar√©es.', 'lev√©es,', '157', 'inc.', 'tarakanova', 'gouvernements]', 'saint-martin-de-crau', 'd√©coupage.', 'id√©ales,', \"l'entretien.\", 'citoyennet√©.', 'aujourd‚Äôhui,', 'septembre,', 'jaune.', '(5,8', 'g√©roudet', 'importante.', '(nord-ouest),', 'jeunesse]', 'bahman', 'd‚Äôapprovisionnement', '20,9', 'pr√©c√©dent.', 'f√©minine.', 'gambienne.', 'confirme,', '000).', '%)', \"l'√©vier\", 'bombs', 'additionnel.', 'caouissin', 'ann√©e.', '10h30', 'ravin.', '√©pargn√©es,', \"l'incident,\", 'klamer', 'kilom√®tre.', \"d'autod√©termination\", 'l‚Äôoccupation', 'carburant.', 'judokate', 'renzi', '168', '\"porte', 'p√©tions,', '√©lev√©es.', 'peshmerga', '(v√©hicules', 'communistes,', \"d'autres,\", 'notamment,', 'foul√©e.', 'anti-sportive', 'roue.', 'novices.', 'sh≈çma', 'discipline,', '57', 'passagers.', 'matrimoniale.', 'coma.', 'course.', 'interrog√©.', 'redevances.', 'et/ou', '232', 'plage.', 'mortel.', 'd‚Äôaujourd‚Äôhui', \"l'ouragan\", 'entourage.', 'pr√©c√©dents.', 'justifie,', \"l'√©tranger.\", 'd√©couvert)', 'abubakar', 's,', 'facult√©,', 'opinion-way', 'l‚Äôencerclement.', 'c≈ìur', 'adriani', 't√©l√©phoniques.', '2011d', 'disparu.', 'sant√©,', 'saint-vulbas', 'feringa', 'gri√®vement.', 'paix,', 'l‚Äôexistence', 'gambiens', 'patinage.', 'd‚Äôhonneur', '(cat√©gorie', 'gondomar', 'col√®re,', '#balancetonporc', 'sanssac-l‚Äô√©glise', 'ville,', 'djinnit', 'gaulliste,', 'puigdemont', 'truqu√©e,', 'ryuichi', '√©coute.', 'concourrait', 'limite,', 'maison.', 'turquo-rebelles.', 'orbite.', 'croix,', 'enbert', '[en', 'mieux,', 'slbm', 'a8', 'd√©faite,', '8h00', 'lourd,', \"d'or,\", 'th√®me).', 'panier,', 'rosique', 'mondial,', \"qu'istanbul\", 'b√ªcher,', 'pill√©e.', 'porte-avion.', 'grave.', \"d'ann√©es,\", 'bennington', 'tricolores.', 'gopro', 'finance.', 'meurtres,', \"s'allongent\", '09', 'd√©faut.', 'rebekah', 'r√©gime.', \"l'ambitieux\", 'l‚Äôencontre', '417', 'restreinte.', 'choses,', 'firme,', 'd√©caleront', 'consid√©rable,', 't√©l√©phone.', 'gouvernement,', 'droulez', 'cat√©nane', '287', 'usurp√©,', 'm√©daille,', 'terrain,', 'a20', 'mortes,', 'n¬∞2', 'boum\"', '(vtt)', 'agit√©e,', 'match-nul', '25-16', 'irakienne.', '200', 'arr√™t√©.', 'croissant,', 'wikinews,', 'avantage.', 'pays,', 'r√©gions,', 'roms,', 'motos/scooters', 'd‚Äôessence,', 'polici√®re.', \"d'actualisation\", \"d'autoroute,\", 'fran√ßaises.', 'isra√©liens,', 'v≈ìux', '20h40', 'historique,', 'd√©placement.', 'octets)', 'd‚Äôartifice,', 'wassfreestyle', 'philosophie,', 'stimpson', '15h00', '(lou√©', 'kg,', '[pas]', '20', 'jeudi)', 'parti.', 'd‚Äôop√©rer', 'utc+01:00', '10,00', 'scandale.', 'sort.', 'd√©g√©n√©r√©,', 'ghjuvan-andria', 'co√ªteux.', 'super-finale', \"l'expulsion\", 'barandagiye', 'minujusth', \"l'hostilit√©\", 'identifi√©.', 'exoplanet', 'ludowski', \"d'ampleur\", 'ambigu√´.', \"d'√©carter\", '10h50', 'k√ºbler', 'majeures.', 'domaine,', 'lucratif.', \"d'adoration\", 'inconnu.', 'cot√©s,', 'saint-√©tienne-du-rouvray', 'jeunes,', 'foul√©e,', 'tuileries,', 'ans).', '√°ron', '22', 'r√©veill√©s.', '1981', 'festing', 'grimpeur)', 'pr√©c√©dente.', 'd‚Äôeffraction', 'daubord', 'dgsi', 'ez-zor', 'bas.', 'touch√©s.', '20,4', 'scheib', \"d'armes.\", 'forenza', 'gaz√©e', '√¢g√©es,', 'fouill√©.', '(de', 'patriotique)', 'balistiques.', 'a-t-elle', 'lib√©ral-d√©mocrate', 'idalys', 'traninger', 'a√©ronavales', '√©dition.', 'faible.', 'communication,', 'a62', 'km/h.', 'collecte.', 'commerce,', 'munitions,', 'mod√©r√©s),', 'brownlee', 'mondiale.', '47', 'd√©molition.', 'anormal.', 'vivre.', 'l‚Äôactrice', 'monistrol-sur-loire', '¬ª,', '162,5', '(espoir)', 'ch‚Äôtiplouf', '√©lu.', '(recherches', 'favori,', 'position,', 'individus,', \"l'est.\", '7h15', 'peuple,', 'jumeau,', 'advenues', 'co-fondateur', 'paradoxalement,', 'tuerie.', '[le]', 'locales.', 'artistique.', 'mobilis√©.', 'rojava', '208', '33', 'suffragants.', 'robuste,', 'n‚Äôavaient', 'ha√Øtiennes.', \"jusqu'alors,\", 'aiguillant', '4,4', 'diplomates,', 'collationne', 'choquant,', '9000', '√©talonne', 'village.', 'synth√®se.', '557', 'ain√©e,', '0,2745', '615', 'prochain,', 'porte-paroles,', 'r√©pond-il', '2370', 'hauteur.', 'nuit,', 'affirm√©,', 'm√•ns', 'normes.', \"n'e√ªt\", 'ensuite,', 'voyageurs.', 'd√©ni.', '2,5', 'seul.', '1:57:17.', 'joueurs.', 'expulser.', 'bruit.', 'territoriales,', 'super-lourds', 'participants,', 'blocage,', 'nucl√©aire.', '782', 'tu√©.', 'triple-tripl√©', 'mobiliserait', 'cristolien', 'proches,', 'stanis≈Çaw', \"d'etat,\", 'gjs', 'indore', 'abballa', 'n.', 'pois.', \"d'hossus\", 'moi,', '(depuis', 'alibi,', 'artistes,', 'challenger,', 'tuer.', \"l'avenir,\", \"qu'autour\", \"l'italienne\", 's‚Äôenfermer', 'citoyens.', 'noir)', 'd√©raill√©.', 'buzzfeed', 'kassim', 'carburants,', '104', 'd√©t√©rior√©es,', \"l'appelle\", 'bargie', 'danioth', 'shahinyan', 'progr√®s,', \"l'√©mission,\", 'renty', 'coureuse', '√©lev√©.', 'loir√©tains', 'demand√©es,', '[jeudi]', 'arrondissement,', 'observ√©.', 'parlement,', '19h30', 'sujet.', 'l‚Äôisolation,', 'ch√¥mage.', '801),', 'guargual√©', 'nord-syrien,', 'gonnez', \"l'homme.\", 'chiffres,', '√©lectoral.', 'intergalactic', 'ligne.', \"d'accusation\", 'd√©c√©d√©es,', 'pr√©fectorale,', 'pr√©tentions.', 'millier,', 'r√©alisant,', '891', 'agbegnenou', 'natation,', 'al-machad', 'walko', 'locaux.', '41√®me', 'site.', \"d'attentat\", 'am√©ricains,', 'analyse,', 'interrog√©,', 'gentil√©', 'jours.', 'perpignan-montpellier', \"d'anomalies,\", 'l‚Äôaffaire', 'th√©matiques.', '[', '√©tude,', 'terrestres.', '√©lectricit√©.', 'chute.', '11,8', 'tomb√©e,', 'politique.', 'd1.', '10e', 'spicer', 'a.b.c.', 'revendication,', 'vis√©e.', 'occasion,', 'r√®gles.', 'syrien,', \"s'aggraver,\", 'h7n4', 'd√©ploy√©s.', 'm).', 'skagway', '(...),', 'exceptionnel,', 'gaz.', \"l'auraient\", 'sulston', 'utc‚àí5', 'gkg.', 'r√©entr√©e', 'frappes,', '20h', 'bless√©s.', 'd√©fendre.', 'paysager,', '102', 'cause.', '≈ìuvres', 'popova', 'bronze.', 'm),', '√©tats.', '2018,', 'koleto', 'question.', 'rails.', 'gratuite,', 'ipad', '(2432', '19,2', 'n√©cessaire.', 'exactement,', '19h10', '(apxs),', 'triathlon:', '[qui]', 'ski),', '16e', 'kirstjen', 'retraites,', 'rochers.', \"l'inclusivit√©\", 'gratuites,', 'notamment)', '758', \"l'entame\", '13h', '(1:12.56)', 'ans.', 'dernier.', 'si√®cle,', '21h00', '26.', 'l‚Äôh√©g√©monie', 'ouvertement,', 'kelmendi', '(0/1)', '¬ª.', 'renoncer,', 'l‚Äôavenir', '\"', 'deodoro', 'mat√©riel,', 'aujourd‚Äôhui', 'jeune.', 'taoiseach', 'enrica', 'qu‚Äôapr√®s', 'woolsey', 'eislauf-union', 'tortures,', 'l‚Äôorganisation.', 'cipr√®s', 'ni√©men', '2015-2016', \"s'appr√™tent\", '7,95', '√©galement,', 'nord-', 'whim', \"l'insu\", 'reprise,', '12h30', 'aristide,', 'mis,', 'vivait,', 'newberry', '(diplomate)', '598', 'financi√®re.', 'mk2', '¬ª),', '(remport√©', \"d'isf\", '(74', 's‚Äôav√®re', 'song-guk', 'kumamoto', 'climatiques.', '(heure', 'draps,', 'artak', 'hommes,', '(bien', 'lawrence-berkeley', '√©trang√®res.', \"pont-d'arc\", 'iraniens.', 'autoris√©e,', 'actuellement,', \"d'obstacles.\", 'v√™tements,', 'municipalit√©,', \"l'injonction\", '88807', 'al-mahmoud', '186', 'n113', 'main,', '2007', '%).', 'mi-journ√©e,', 'majeure,', '544', 'mois,', 'version.', \"l'ascension.\", 'effondrement,', 'auparavant,', 'clo√Ætr√©s', 'impliqu√©e.', 'com√®te.', 'rebega', '000,', 'parcourus,', 'noyaut√©', '41', 'vandaliser', '11h45', 'demi-tour,', '),', 'garretto', 's√©rieuses,', 'gaspards', '√©tudiants.', 'suspect,', '15h30', \"d'achery\", \"d'espoir.\", \"s'inqui√®tent\", 'tantes,', 'prison,', 'aleksey', 'poss√©d√©.', '307', 'menaces.', 'tchiroma', 'bafou√©s.', 'isra√©lienne,', 'demain),', '86', '677', 'prestige.', '(sa', '45', 'non-recevoir', 'r√©v√©lations.', 'embarrassant,', 'populations.', 'nucl√©aire,', 'm√©dicaments.', \"d'attaques\", 'confondues).', \"s'√©crouler\", 'prises.', 'bouchons.', '107', '340', \"l'≈ìil\", '√©co√©pid√©miologie', \"l'universit√©,\", 'comp√©tition.', 'nationale,', 'canadienne,', 'chef,', '83', 'non-assistance', '8h45', '(2', 'ouest-africaines,', 'euros,', '(pour', 'actuelle.', 'chauss√©e.', 'cakrawati', 'matin√©e,', \"d'immigrants\", 'tilikum,', 'ong.', 'femmes,', 'fr√©quentes.', 'cheurfi', \"l'apr√®s-midi.\", 'd‚Äôexplosifs', \"d'histoire,\", 'kantaji', 'hasard.', 'b√∂cherer', 'monter,', 'remarqu√©e.', \"d'aspe\", 'taitoluisteluliitto', 'allure,', 'carrure,', 'kanak.', 'baisser,', \"qu'ailleurs.\", 'perturb√©.', 'christian-bourquin', 'cubaine,', 'fuglsang', 'supermassives', 'maiduguri', '√πu', 'exigence,', 'couronne.', 'militaire,', 'zeid', 'hampson', 'indolore,', 'd‚Äôeffectuer', 'conjoncturel,', 'abelisaurus', 'force,', 'voyageurs,', '1970', 'cal√©donien.', 'p√©riode,', 'jour-l√†,', 'marathon.', '1975', 'korovin', 'espagnol,', 'd√©mocrate.', '(cat√©gories', 'maracan√£', \"l'√©v√®nement\", 'luxford', 'peskov', 'message.', 'pay-per-view', 'fenix', 'tunnel.', 'poursuivi.', '14-12', \"d'√©lectricit√©.\", 'voit,', 'premier.', 'descente.', 'immigration,', 'ducatel', 'ext√©rieur,', '(4/5)', \"l'institut,\", \"l'int√©rieur.\", 'd√©jou√©e', 'assourdissantes,', 'marquina', 'boisson.', 'elles.', 'gyros', 'jama√Øcaine.', 'derri√®re.', 'studio,', 'itin√©raire,', 'curli,', 'abus.', 'messick', '9h30', '(ministre', \"s'imposant\", '5e', 'm√©tro.', 'conspirationniste-complotiste;', \"l'interdiction,\", 'pied.', 'tiboinshape', 'c√©r√©monie.', 's‚Äôeffondrer.', 'stations-services.', 'choura', 'd√©funt.', 'personnes).', '-,', 'produite,', 'sexuels,', \"d'argent.\", 'lui-m√™me,', 'explose.', 'youtube,', 'rapport√©s.', 'seyyar', 'plaisir.', '1,96', 'l‚Äôext√©rieur', 'quarante-cinqui√®me', 'auditionnera', 'moment,', 'gravement.', \"d'emilie\", 's‚Äôav√®rerait', 'perdriau', 'immotique', 'films,', 'somme,', 'al-jaber', 'kurakova', 'kihira', 'cancer.', \"d'obtemp√©rer,\", 'parfois,', 'priorit√©,', 'syrien.', 'blocage.', 'lorimer,', 'suicid√©.', \"qu'ayant\", 'individuel,', 'a10', 'enregistr√©.', 'poursuivants.', 'maitris√©e.', 'pr√©fecture.', 'poule,', 'arr√™ts).', '[des', 'patron.', 'punitive.', 'wikim√©dien.ne.s', 'sites.', 'clame-t-il', '7h', 'nurchi', '(6,4', 'alenxander', 'd√©di√©es,', 'magistrats,', 'propos,', 'poursuivre.', 'guillot-boisset', 'linky', 'vorkoutaougol', 'suffisantes.', 'jean-karl', 'homme,', '71', '[notre]', '16√®me', 'tamim', 'bezat', 'th√©√¢tre,', 'nord-cor√©en,', 'maison,', 'radicaux,', '06', 'efficaces.', 'birtwhistle', 'chercheur.', 'concurrents.', 'jorik', 'troadec,', 'ici,', '0,8', 'citoyens,', \"s'emploie\", '1979', 'avermaet', \"l'√©tape-reine\", 'incendiaire.', 'revendications,', \"d'amas.\", 'journal,', \"d'orbite\", 'absolu,', 'camp,', '29', 'extr√™mes,', '65', \"d'auchan\", '1,2', 'l‚Äôint√©rieur', 'chamaloc', '(1/1)', 'for√™t.', 'vitesse.', 'fillon.', 'personne,', 'automobile.', 'weatherill', '(n¬∞23)', 'namihaya', 'bleu.', \"l'onde\", 'procureur.', 'mozgov', 'ligonn√®s', '(+0,4', 'barr√©e.', 'internet.', 'ainsi,', '1:11.93,', 'minist√®re,', 'schaw', 'parents.', '2010', '8h30', '7,8', 'greektown', 'd√©faire,', 'd√©licate,', \"l'australopith√®que\", '((fr))', 'megalolamna', \"l'olibrius\", 'kendrick', 'd‚Äôentre', 'lav√©ra', 'pr√©sum√©.', '374', 'martelly', 'tenantes', 'heilongjiang', 'd‚Äôint√©r√™t', 'fond,', 'soussiane', 'britannique,', 'chinoises.', 'maksym', 'nepela', 'extr√©mit√©.', 'terrain.', 'rwasa', 'd‚Äôessence', 'rostelecom', 'national),', 'participative,', 'voisins,', 'partis,', 'form√©s,', 'interfamilial', 'inondations.', \"l'examen.\", 'afrin', '19h00', 'sprinteur),', 'points.', 'fermer.', 'l‚Äô√©lection', 'clients.', 'salaire,', 'vergogne.', '9,5', 'naissances,', 'sindumuja', 'permanent.', 'vain.', 'acclam√©.', '(incluant', 'orageuses)', 'luxleaks', 'd‚Äô√™tre', 'football.', \"s'approfondit\", 'classement,', 'annul√©s,', 'ligue.', '40', 'd√©mission.', 'tu√©s.', 'kopaszewski', 'farhan', 'majoritaires,', 'nationaux.', 'plantes.', 'offensive.', 'parlent,', 'honorifique.', '√©trange,', 'hongkongaise.', 'grimpeur.', 'saint-christol', 'dite,', \"l'apog√©e\", 'benalla', '237,5', 'accusation.', 'transparence,', 'purs,', 'squeezie', 'unanime.', 'l‚Äôensemble', '(r√©seau', 'nul,', 'ville-marie', 'kanpur', 'd‚Äôamplifier', 'tradition,', 'pamella', \"l'amiable\", 'autorise,', 'am√©ricaine,', 'adylkuzz', \"d'assaut\", 'autonomes,', \"l'enfarinage\", 'rare.', 'catholique,', 'syst√©matiques.', 'contages', 'kosciusko-morizet', '0,9', '49', \"l'iranien\", 'winterbiketoworkday.org', 'week-end,', 'mocoa', 'inadmissible,', 'mondial)', 'blocages,', 'l‚Äôan', 'cela,', \"d'euros,\", 'plan√®te.', 'comp√©tence.', 'aleksandra', 'montant.', 'r√©instaur√©', 'j‚Äô√©tais', \"s'accumulent.\", \"d'√©teindre\", 'politico-m√©diatiques', '331', 'num√©riques,', '10h', '√©rythr√©en', 'sorte,', 'coups,', 'allemand,', \"l'assistante\", 'd‚Äôarr√™t.', 'fa√ßons.', 'b√©gay√©e', 'rupture.', 'chiffres.', '(fortes', '5),', 'kappelman', 'hulu', 'anti-flics\"', '3%,', 'lendemain,', 'mixte.', 'seniors.', '\"grand', 'd‚Äôenqu√™te', 'l‚Äôind√©pendance', 's‚Äôessouffle,', 'r√©gression,', 'shinz≈ç', 'sorre', 'paratriathlon', 'demoiti√©', 'participation.', '√©tape.', 'renomm√©.', 'qu‚Äôelle', 'utc+1', 'collaborer.', \"d'essence,\", '[2]', 'anglais.', 'lilloise,', 'oeuvre,', 'temps,', 'r√©√©lection,', \"ra'ad\", 'grumier', 'ederer', 'montchavin-la-plagne', 'dissous.', 'ravivement', '(34-27)', '6,665', '1h30', 'cr√©√©s,', 'fragments,', '√©lev√©s.', 'salgar', 'anti-asiatique.', 'ranc≈ìur', 'libert√©,', 'al-mannai', 'slov√®ne.', 'documents,', 'fort-nieulay', 'forme.', '(alena).', 'glaces,', 'britanniques.', 'bri√®vement.', 'vandalisme.', '91e', 'kurdes.', 'muyinga', 'mod√®le.', 'fiscale.', 'pittella', '(5/8)', 'bloqu√©s.', 'mus√©es,', 'rel√¢ch√©.', 'shebab', 'interpell√©s,', '03', 'kalkar', '92,7', 'coffres-forts.', 'minustah', 'gardes-fronti√®res', 'programme.', 'impossible.', 'membres.', 'autoris√©e.', \"l'ordre.\", 'malgaches,', 'am√©rindiens.', 'fourn√®s', 'al-asriya', 'vote,', '317', 'a55', '√©loign√©e,', 'grimmie', 'blessure.', \"s'identifier\", \"l'agresseur)\", 'l‚Äôacte', 'conspu√©es', 'maty√°≈°', 'putschiste;', 'postiglione', 'secondes,', \"d'histoires\", 'lacrab√®re', '(zrs).', 'justice.', \"d'attente.\", 'joulaud', 'barricadent', \"d'incendie,\", 'turcs.', '(23', 'r√©cepteurs,', 'f.', 'chauffage.', '(l‚Äô√©glise', 'l√†.', 'diff√©rents,', 'd√©grisement', 'rel√©gitimer', '(communaut√©s', \"l'hymne\", '22h00', 'glace.', 'mouvements.', 'fuite,', \"d'acter\", 'incendi√©s,', 'n‚Äôa', \"l'autre.\", 'disparus,', 'yihundimpundu', 'automatisables,', 'confisqu√©s.', \"l'ex-\", '48kg', 'chapitre.', 'l‚Äôenqu√™te.', 'jeux,', \"l'inverse,\", \"s'embrase\", \"qu'√©lite.\", 'anstett', 'livraisons.', 'kodiak', \"d'huez\", 'chasse.', \"l'histoire,\", 'x9', 'jeu,', 'pallon', 'ivry.', 'lacrymo,', 'engag√©,', 'amalgame,', \"l'≈ìuf\", 'fonds.', 'sudrie', 'choc,', 'central,', \"d'outils.\", 'distributeurs.', '(plus', 'regev', 'encore.', 'pol√©mique,', 'partout,', '3√®me', 'l‚Äôentra√Ænement.', '3h', 'ludosky', '(qui', \"d'√©lection\", 'conseiller,', '1993', 'avalanche,', '(1967)', 'motos,', 'l‚Äôimpression', 'camion.', \"l'ast√©ro√Øde\", '23h30', 'pollution.', 'soins.', 'r√©sidait.', 'domicile.', 'circuit,', '18h30', '300', 'boundin', 'd‚Äôactualisation', '1961', \"l'√©vang√©lisation\", 'n‚Äôavait', 'candidats,', 'canayer', 'lundi,', '√©poux,', '17', 'l‚Äôhebdomadaire', 'suvrath', 'lanjouan', 't√©moins.', '(sp√©culation,', 'ais√©.', 'exister,', 'd‚Äôinfractions', 'pr√©c√©dents,', 'l‚Äôissue', 'ext√©rieure.', 'vald√¥taines', 'saint-couat-d‚Äôaude', 'nurmi', 'zappone', '254', 'bateau,', 'copte,', \"d'escalier,\", 'kayne', '√©rig√©.', 'mohan', 'bouchkova', 'alternative,', \"l'admiration\", '18', '11h', 'd√©tendeur.', 'haroyan', 'proie,', 'continents,', 'loi,', 'mouvement,', 'n√©anmoins,', 'bonchan', \"d'entrainement.\", 'arme.', 'tir√©s,', 'mesures.', 'papier,', 'couleurs,', '22,6', '39√®me', '242', 'volodymyr', 'intentionnellement,', 'int√©r√™t.', 'bubjumbura', 'j‚Äôai', 'fils,', '103', '332', 'portugais.', 'a50', 'eygui√®res-miramas', 'assourdissantes.', 'autorit√©s.', 'restaurant,', 're√ßus,', 'elle,', 'r√®gles,', 'migrants,', '280', 'zaventem', 'apparu,', 'pr√©c√©demment.', '√©nergie,', '√©v√©nements.', 'arr√™t√©s,', \"l'immigration,\", 'fronti√®res,', 'immerg√©.', '259', '434', 'cherbourg-en-cotentin', 'ferm√©.', 'jou√©s,', 'quart-de-finale', \"d'auteur,\", 'virage,', 'd‚Äôautres', 'constitution,', '(non', 'mahadevan', 'successeur.', \"l'instabilit√©\", 'chauffard,', \"l'√Æle,\", '(2b)', '601', 'odoxa', 'mobilis√©s,', 'r√©pandus.', 'peloton.', 'kamikaze.', \"l'incendie,\", \"s'√©jecter\", '√©conomique.', 'seaworld', 'rejoindre,', '(19', 'sud,', 'l‚Äô√©trier', 'journ√©e,', ';', 'ip.', 'normal,', 'r√©veillonner', 'baisse.', 'conquis.', 'varadkar', 'spatial,', 'an.', 'anakra', '(fuite', 'prot√©ger.', '101', '19h32', '42,195', 'd√©but,', 'proc√©dure,', '412', 'opower', \"d'izoard\", 'sprint)', 'commun.', 'point),', 'pense,', '7,7', 'n‚Äôexiste', 'ind√©pendant,', 'belloubet', \"l'ouvrir\", 'produit.', 'si√®ges,', 'vert).', 'russes]', 'concours.', 'm√©t√©orologiques,', 'sondage.', 'capitale).', '2h20', 'l‚Äôeurovision', \"l'√©taient\", 'publics,', 'astromobile', 'nipawin', '2019', 'fonds,', 'd√©couverte,', 'duathl√®te.', 'britannique.', 'couple,', 'radio.', 'f√©minine,', 'embrunman', \"c√¥tes-d'armor\", 'dessus,', 'publicitaire.', 'comp√©titrices', 'd√©samorc√©e', 'rouvert.', 'dit-il', '11h24', '61%', 'rapproche,', 'arts-loi', '16h15', 'mois.', 'furiana', 'monde,', 'contribuables,', \"l'√©lection,\", 'juges.', 'pav√©,', 'krickel', '160', '3,5', '4h', 'scientifique.', '371', '420', \"l'euphrate\", 'd√©charges.', 'cice', 'cecafa', 'muscles.', '24', \"d'or.\", 'mattarella', '(les', 'parit√©,', 'saint-f√©liu-d‚Äôavall', 'r√©gionales.', \"d'asiatiques\", 'journ√©es,', 'jouer,', 'tch√©tch√®ne,', \"l'instrument,\", 'gendarmerie,', 'stromatolithes', 'avion,', 'mortelle.', '3,3', 'voisins.', 'province,', 'yuli', 'vlerken', 'ensuite.', 'question,', 'vent,', 'nr.', 'poste,', '199', 'r√©habilit√©es,', 'finalis√©.', 'ouverts,', 'conducteur,', 'ferm√©s.', 'purgeait', 'a520', '172,8', 'pts4', 'souvent,', '133', 'policiers.', 'd√©truit.', 'semblables.', 'd√©tachement,', 'rencontre,', '\"gilets', 'ministre,', 'malavis√©e', '156', 'nig√©rian,', 'technologie.', 'pr√©sents.', \"c√¥te-d'or\", '120', 'retrouv√©e.', '66', \"d'opportunit√©\", 'explosif.', '√©tapes,', \"l'accoutumer\", 'n√©lson', 'militaires.', '(318', 'pr√©sidentielle,', \"l'accord,\", 'suomen', 'tonneaux.', 'am√©ricaine.', \"l'√©ducation,\", 'tovi√®re', 'circuit.', 'kanyana', 'tiroir.', \"d'emprisonnement,\", '176', 'hansson.', 'salvaing', 'bruxelles-central', 'mikkel', 'euros.', 'passants,', 'internationales.', 'chinoise,', 'd√©tourn√©s.', '(4,8', '343.', 'suissesse.', '42', '√©lectrique.', 'fusil,', 'vistica', '1998', 'intime,', 'mongolie-int√©rieure', 'sp√©cialit√©.', 'fs2', 'michina', '(20-30', 'an).', 'recevoir,', \"d'abandon.\", 'droit.', 'comp√©titivit√©,', 'vives,', \"d'in√©ligibilit√©,\", 'helissey', 'pic,', 'bauke', 'val-des-pr√©s', '14h45', '√©vacu√©es,', 'internationale,', 'canadiennes.', '170', 'rejet,', 'juif.', 'mairie.', 'mars.', '00', 'groupe.', 'clef,', 'r√©actualis√©,', 'achin', 'sp√©cialit√©,', 'l√†,', 'cd)', 'femme,', \"d'estaing\", 'franchi.', 'court,', '190,5', 'propri√©taire,', 'culturels.', \"d'investiture.\", '(4/4)', 'bras,', '[au]', 'clivages,', \"l'aecg\", 'patick', \"l'int√©ress√©.\", 'nationale.', '8%', 'public.', 'libre,', 'quatre,', 'syriens.', \"d'import-export\", 'd√©fendre,', '11h10', \"puisqu'aucun\", ']', 'op√©r√©.', 'soulor', 'cependant,', '93.', \"d'eux,\", 'neige-verglas', 'd√©bris,', '187', '(enfin)', 'estimations,', \"s'allument.\", 'mokoko', 'd√©voil√©.', '92', 'enti√®re.', 'travail,', '15h05', '375e', 'veille,', \"s'√©crasant\", '√©teinte,', 'a41', 's‚Äôeffondrer', 'cause,', 'v√©hicule.', 'inqui√®te.', 'jusqu‚Äôau', 'r√©vocatoire,', 'a81', '(40', '52,4', 'lorsqu‚Äôil', 'dictera', 'jama√Øcains.', 'r√©glages.', 'mcaleese', 'messieurs,', \"d'incendier\", 'branle.', 'ann√©e,', 'strat√©gie,', 'danforth', 'reddit', 'contrats.', 'blonia', \"l'abdomen.\", 'maintenant,', 'machines,', '11,9', 'decam', '(20', 'd‚Äôeuros', '14h25', 'vigicrues', 'planification.', 't√™te.', 'bobo-assum√©e', '1,', 'connues.', 'villages.', 'saint-c√©saire', 'concern√©e.', 'transition.', 'deux,', '276', \"d'assassinat.\", \"'s\", 'dziwisz', 'bombe,', '27%', 'syst√®me,', 'gerdes', 'volontaire.', 'co-voiturage', '15h40', 'liswati', 'ripost√©.', 'pardonne...', \"l'am√©ricaine\", 'champignon,', 'danoises,', 'sujets.', 'ethnique,', \"d'assassinats\", 'lacertae', 'longue,', '15h15', 'commerces.', 'consolation,', 'netflix', 'luca,', 'enqu√™teurs.', 'populaire.', \"d'hier,\", 'concourir.', '282', 'formul√©es,', '(lire', 'empedocle', 'nadot', 'bonpas', 'nourriture,', 'ann√©es.', 'dopage.', 'tarabeux', 'ashmeade', \"l'√¢ne,\", 'l‚Äôattention', \"l'objet,\", '√©trangers,', '9h50', 'provisoire,', '1969', '\"un\"', 'distance,', 'manifestante.', \"l'article.\", '40,7', 'escalier,', 'danois,', 'r√©sist√©.', '5651,', \"d'inflammation,\", 'interstate', 'd√©blay√©es', 'feux.', 'rembourser.', 'apponter', 'd‚Äôacc√©l√©rer', 'cibles,', 'cryptomonnaies', 'neurod√©g√©n√©rative.', 'active.', '2,3', 'd√©sarticuler', 'chalvet', 'claver', 'venir,', 'texte,', '(9/17', 'malaise.', 'beranek', 'stagiaires,', 'saint-c√©zaire', 'foule,', 'sous-marin.', 'fumeurs,', 'banki', 'n√ºchtern', 'combats,', '905bis', '[en]', 'asserment√©e', 'validait', 'jour.', 'normale.', 'parti,', 'andriy', 'examens,', 'th√©oriciens.', 'pr√©sidence.', 'apr√®s-midi,', 'wikitribune', 'borgel', 'kahlefeldt', '2024', 'randrianarison', \"d'octobre,\", 'groupes.', 'd√©c√©d√©s.', 'migratoire.', 'vernet-les-bains', 'limogeage.', 'professionnels,', 'litvintsev', 'australie-m√©ridonale', 'drame,', 'x5', '111', \"d'√©quipement.\", 'saint-martin-de-valgualgues', 'troisi√®me.', 'pi√©geux', 'justice,', 'climatiques,', 'concert,', 'm√©t√©o-france', 'conseil,', 'identifi√©,', '(nombreuses', '(bus', '(sla),', 'cosmao-dumanoir,', 'revenir.', 'arnaux', 'quartiers.', \"l'enl√®vement.\", \"l'assaillant.\", 'd‚Äôemprisonnement', '1.', 'atlantique.', 'certes,', 'second.', '2025', 'largages', 'm√©di√©vale,', 'podium.', \"l'attentat.\", 'pression,', 'fsprt', \"l'indignation.\", 'drame.', 'd√©sirs.', '8h50', 'contrat.', 'nkurunziza', 'd‚Äôanomalies', '26', '[la]', 'paradis,', 'l√©ger.', 'm√∏llgaard', 'buts,', 'd√©saveux', 'doucement,', 'ailleurs,', '(138', 'annonc√©.', 't√©not', 't√¥t,', 'novembre,', 'g√©n√©ral.', 'population,', 'point,', '24,3', 'l‚Äôh√¥pital.', 'explique-t-il.', 'conditions,', 'victoria,', 'personnes,', 'conservatrice.', 'perspectives,', 'profonde,', 'russes.', 'vote.', 'position.', 'gajiƒá', 'cass√©s,', 'l‚Äôassociation', '√ßa,', 'impr√©vus,', 'services,', 'comp√©titrices,', 'l‚Äôaustralienne', 'r√©gion,', \"d'apartheid\", '552', 'garde-√†-vue.', 'cat√©gorie.', 'senenews', 'augment√©.', '1987', '298', \"d'√©quitation\", 'conclusion,', 'portative.', \"l'√©tage.\", 'g√©ographiques.', 'vaccin,', 'l‚Äôambassadeur', \"n'annonce\", 'd‚Äôint√©r√™ts', \"l'empoisonnement,\", 'd‚Äôinculpation', 'a51', 'ans,', 'h√¥pitaux.', 'navigateur.', 'chapelle-saint-luc', 'institutionnelles.', 'prier.', 'rythme,', 'disciplines,', '346', 'existante,', \"l'arbitre\", 'malveillance.', 'course-loterie', 'd‚Äôasile', '432', 'cons√©quente.', 'v√©hicules,', '227', 'grigorian', 'vtt.', '426', 'e3', 'victoire.', 'tokoro', 'coup√©e.', '\"bruit', 'r√©daction,', 'a52', 'chuckles', 'couple.', 'sang-young', 'd√©c√®s,', 'judiciaire.', '√©preuves,', '290', 'd‚Äôurgence', 'cours.', \"l'√©chapp√©\", 'rouge).', 'boursi√®re.', 't√™tes,', 'interpell√©es.', '100', 'capelette', 'hausse.', 'pour,', 'aclu', 'camion,', 'technologies,', 'r√©f√©rendum,', '19:15', 'arr√™t√©,', 'a42', 'x√©nophobie,', 'franco-chinoise', 'sol,', \"d'ailleurs,\", 'tisdale', 'morsi', '3491', \"d'alaska\", 'polici√®res.', 'assurance.', 'r√©trospective,', 'consulaire.', 'seulement,', \"d'impl√©menter\", 'rachal', 'fer,', '(34', 'infractions,', '700', 'inverse.', 'construction,', 'morte,', 'adopt√©es,', 'solo,', 'sivignon', 'tirs)', \"d'√©tape.\", 'molotov.', '1890', 'bagarre.', 'd‚Äô√©tat', 'stuyven', 'interpell√©,', \"''mythe''\", \"m'tima\", 'lunes.', '492', 'plus,', 'roquettes.', 'd√©nonc√©e.', \"d'altitude,\", 'organis√©es.', '7', 'syndicat.', '≈ìil', 'mi-saison.', 'mojtaba', '0-0,', 'r√©uni.e.s', 'l‚Äôoccupaient', \"l'origine.\", 'vague\"', 'chance,', 'inondations,', 'turquo-syriennes', 'imbonerakure', 'radicalis√©s,', 'a48', 'belhocine', 'effondr√©,', '216', 'd‚Äôitalie', 'bourg-saint-and√©ol', 'pol√©mique.', 'tensions,', 'd√©cembre,', '√©crit.', '350', '148', 'classement.', 'leurs,', 'm√©thode,', 'discourt,', 'pollution,', 'l‚Äô√©tat,', 'nyamitwe', \"d'amende,\", \"l'√©mancipation\", \"d'affil√©.\", 'wikipedia.org', 'lumineux.', '25,', 'm√©dias.', 'sauv√©s.', 'impact√©.', 'forestier.', 'vigueurs,', 'normes,', 'cargo,', '1994', 'warm-hot', '[s]a', '532', '182', 'l‚Äôessai', 'bekir', 'chiffres-l√†', 'sainte-rita', 'champs-elys√©es-clemenceau', 'championnat.', 'l‚Äôhonneur', 'd√©truits.', '(isf).', '30%', \"s'extrait\", 'symbolique.', 'elle.', 'gentiloni', 'toutes,', 'ouvertes,', \"l'√©mission.\", \"d'expulser\", 'coup√©es.', 'contr√¥lait.', 'internews', 'attouchements.', '1927', 'cinq,', 'confi√©,', 'l‚Äôespoir', 'direct.', 'irib', '115', 'cranston', 'lemgada', 'interrompu,', '2,7', 'medell√≠n', 'd√©pass√©s.', '50', 'morale.', 'revenu,', 'rc.', 'duathl√®te', 'candidature,', 'raiffeisen', 'pt5', 'mission.', 'khenchela', 'vie.', '10.', 'longueur,', '√©lection.', 'a6', 'enchaine,', 'bajon', '(voir', '\"ce', '49-3', 'pr√©occupante,', 'chiites.', 'dissimul√©es,', '(femelles)', 'douteuse.', 'dunis', 'mansoura.', \"l'avenue.\", 'dentsu', '5%', '1929', 'r√©ticences.', 'juin,', 'jamala', 'crier,', \"l'automobiliste\", \"d'avances\", '49.3', '27', '√©tait,', \"d'ind√©pendance,\", '[et]', 'financiers.', 'enfreintes', 'senn√®s', 'abedi', '197', \"l'h√¥pital,\", 'moiti√©.', 'cou.', 'interrompue,', 'analyses,', 'ex-beau-fr√®re', 'prochains.', 'emprisonn√©s.', '11e', '105', \"j'emmerde\", 'qualit√©.', 'alourdi.', 'ann√©e-lumi√®res', 'respectivement.', '423', '140', \"aujourd'hui.\", 'abuin', 'tagu√©.', '[mardi]', 'vanderlei', 'surexcit√©,', 'absurde.', 'demi-finale,', '2021', '1400', 'immeuble,', 'touch√©es.', 'absolue,', 'terada', 'all√©gations,', '14', 'inter-am√©ricaine', '(appartenant', '6,10', '28', 'immigrants.', '700)', 'vingts-et-une', 'affect√©es.', 'faits.', 'perpignan-sud', 'peshmergas', \"[d'internet]\", 'l‚Äôint√©gralit√©', 'anahi', 'blocages.', '(+1,8).', \"d'imiter\", 'jet√©s.', 'dudelange', '9', 'evgeny', 'consid√©rables.', 'carfrae', 'signataires,', 'match,', 'bless√©es,', 'resserr√©s.', \"l'impact,\", 'olsson', 'btk', '√©lus.', 'orientations,', 'vorkouta', 'situation.', '1788', 'ex√©cut√©.', 'd√©blayer.', '1,9', 'm√™me.', 'brzegi', 'psathyrella', 'jugel√©', 'd√©cid√©,', '10,4', 'vid√©o,', \"l'√©tablissement.\", '90', '72%', 'niveau,', 'minorit√©s.', 'planches.', \"d'avantage\", 'femme.', 'enfarin√©', 'longtemps.', 'synchronis√©.', 'boyadji', \"d'alcool√©mie.\", 'personne.', 'pays).', '5h45', 'francetv', '[...],', 'aquathlon', 'fan-zone,', 'emport√©s,', 'quotidiennes.', '(1:10.65)', '32', 'exil,', 'vx.', 'sangoyaco', 'r√©pliques.', 'points,', 'peuple.', 't√©raoctet', 'vitales.', 'turnbull', 'stoddart', 'stromatolites', \"l'ann√©e,\", '(m√™me', '26√®me', 's‚Äôoppose', 'pays),', '√©cologiques,', '-20¬∞c', 'sursis.', 'compatriotes,', 'd√©ploraient', 'commenc√©.', 'rond-point,', 'premi√®re.', 'extr√©mistes.', '496', '(+0,6)', 'moins,', 'direction.', 'thourda', 'parrainages,', '228', 'esclaves.', 'voleur.', 'scandale,', 'officiel,', 'violentes.', 'observables,', \"d'idleb\", 'griveaux', 'bagnis', '√©cart√©e.', '√©cologie,', 'poursuivants,', 'wannacry', 'inexpliqu√©e,', 'qu‚Äôont', 'jean-ferr√©', 's‚Äô√©l√®ve', 'arabe).', \"d'expertise,\", 'couteaux,', 'recherch√©,', 'candassamy', '80km/h', 'particulier,', 'voix,', 'quart.', 'a√©rienne.', 'zabdiy√©', \"l'a√©roport.\", 'minute,', '1906', '+4,5', 'sprinter)', 'primaire,', 'novembre.', 'burundaise.', 'courrier,', 'delaplace', 'jeu),', 'passage-d‚Äôagen', 'vald√¥taine', \"l'ex-premi√®re\", 'environ).', 'rond-point.', '7e', 'sieburger', 'libre.', 'n‚Äôaurait', 'seoane', '800', 'matin,', \"l'assassinat\", 'combler.', 'tribune.', 'professionnelles.', 'spivey', '449', 'meliƒá', 'nations.', 'secondes.', '√©lites,', 'a64', \"s'opposaient\", 'diff√©rents.', 'qu‚Äôensemble', 'demaret', 'nouveau,', 'etc.)', '154', '4,2', 'r√©cemment,', 'kouznetsov', 'a2', 'abdulahi', 'inconnues.', 's‚Äôest', 'aviaire.', \"s'entra√Æner\", 'olympiques.', '50m', \"l'avion,\", 'd√©c√©d√©es.', 'imbonerakure.', 'musique.', 'retourne,', 'filles,', 'march√©.', 'grand-anse', 'hypoth√©tique.', 'l‚Äôusine', 'onufriyenko', 'parties.', '155', \"l'encerclement\", '30¬∞,', \"s'extraire\", 'fabriquer,', 'sub-nine', 'burn-out,', '(amas', 'allemandes.', 'noms,', 'n‚Äôest', 'norsup', 'chauffeur.', 'ostapchuk', \"d'annoncer,\", 'b√©n√©voles.', 'maugr√©e', 'd‚Äô√©lectricit√©.', 'sceptique.', '45√®me', 'elabe', 'diemunsch', 'velikaya', \"d'agressions\", 'incendi√©es.', 'accusations,', 'intervention.', 'demain,', '(r√©f√©rendum', 'l‚Äôinsignifiance', 'sondages,', \"l'accus√©\", 'humains,', 'l‚Äôinjustice,', 'chuberre', 'tendance,', \"s'inclinent\", 'accusait,', '9h00', \"l'halt√©rophilie\", 'revendiquaient.', '!¬ª.', 'yuzuru', '14h15', 'sociale.', 'tribunal,', 'choqu√©.', 'airbnb', 'capitale.', 'r√©f√©rence.', \"s'exprimant\", 'veste,', 'persiste,', 'brexit.', 'tajani', \"l'astronomie.\", 't8', 'vivre-ensemble,', '≈ìufs', 'm√®tre.', 'skripal', \"l'assaut\", \"d'eau,\", 'scripts,', 'broncos', 'canellas', \"l'indon√©sienne\", \"l'instant.\", 'amont,', 'lev√©s.', 'importantes.', 'a83', 'population.', '7h40', '2009', 'frelat√©e.', 'laurentine', 'seitel', 'dense,', '200,', 'shimada', 'crise.', '47%', '1934', '(‚Ä¶)', 'm‚Äô√©tais', 'nibali', 'diff√©r√©e,', '113', '10\".', 'thavaud', '7,2', 'professeur.', '565,43', '11h18', \"d'√©tat-major,\", 'l‚Äô√©mission', 'inaper√ßue,', 'd‚Äôinterpr√©tation', 'anadolu', 'd‚Äôagen', 'porte-parole,', 'd‚Äôachat', 'gamesa', 'rien‚Ä¶', 'touch√©e.', 'traitement.', 'r√©sultats.', 'certains,', 'chasse,', 'm√®tres,', 'technique,', 'badigeonn√©', 'choix,', 'pilote.', 'africains,', '67p/churyumov‚Äìgerasimenko', '754,02', 'irako', '74', '613', 'vaccinale,', 'mans√©', '887', 's‚Äôagite', 'celui-ci.', 'l‚Äôavenue.', 'earthquakes', 'gerasimenko', 'tortur√©s.', 'faire,', 'blasph√©matoires.', 'chechar', 'sp√©ciales.', 'extr√©miste.', 'fines.', 'portes-l√®s-valence', 'pill√©s.', \"s'enthousiasme\", 'l‚Äôessentiel', 'nord-katanga', 'recherche.', '2018-2019', '0,1', '√©tay√©es.', 'fuite.', '√©pargn√©s.', '√©crites.', \"d'ondes\", '150', 'prochaine.', 'final.', 'j.', 'mycologique,', 'reprendre,', '20th', 'championne,', 'palombi', \"l'atteindre\", 'franco-canadien', 'geun-hye', 'camps,', 'l‚Äôinvitation', 'post-bac,', '√©preuve.', 'enregistr√©s.', '4,5', 'l‚Äôhistoire,', '51', 'marins-pompiers.', '17h15', 'burundais,', 'contenu,', 't5', '1989', 'icesheffield', 'poursuivante,', 'menace,', 'bengt', \"s'opposant\", 'samedi,', \"s'aggravent\", \"s'immoler\", 'deux.', 'pr√©sidentiel.', '26,', '7h25', 'r√©gionaux,', 'buso', \"d'alcool,\", 'pages.', 'concernant,', 'anti-fascistes.', 'route,', 'clients,', 'mandat.', 'vandalis√©s.', 'poubelles,', 'particuliers.', '571),', '3,4', '80', '37,1', \"d'encouragements\", 'katsalapov', '9h20', '250', 'l‚Äôexplosion', '\"tensions\"', '21h', 'expression,', 'cours,', 'daaka', 'imagin√©.', '(65', '(+1,2', 'rubbia', 'premi√®re,', 'avis.', 'ashleigh', 'rapports,', 'drones,', 'd√©tracteurs.', \"d'≈ìuvres\", '73%', 'fennell', 'farkhad', '6h00', 'commence,', 'malveillant.', \"l'√©t√©.\", 'contaminations,', \"d'achat.\", '2015', 'l‚Äôespace.', 'staffan', 'scientifiques,', \"d'abonn√©s\", 'd‚Äôil', \"s'apparentant\", '16h00', 'ciblait', 'lamiot', '5h30', 'succ√®s,', 'viernheim', 'triathlon.', \"l'imm√©diat.\", 'mentale.', 'campagne,', 'cin√©ma,', '960', 'conditions.', '(6', 'volontaires,', \"l'actualit√©,\", 'siemens-alstom', 'gu√©nois', 'christkindelsm√§rik', 'levenez', 'tir√©.', 'a4', 'interrompues.', 'survivre.', '70', 'oumiha', 'pr√©voir.', 'l‚Äôetoile', 'evgenia', 'avait,', 'pyeongchang', 'radars,', 'humain,', 'vols,', 'derni√®re.', 'couronne,', 'habitants),', 'antis√©mitisme,', 'reprises.', 'rachidine', 'br√ªl√©,', 'mccallum', 'r√©quisitions.', 'cat√©gories,', 'effet.', '√©gorg√©es,', '√©clair,', 'lit,', 'aisha', 'froome', 'gbu-43/b', 'victimes.', 'qu‚Äôil', 'pr√©tendants.', 'aprodh', \"l'opposition,\", 'interdites.', 'protester,', \"d'habitation,\", \"l'accident.\", 'dions', 'gr√®ve,', 'revendiqu√©.', '35√®me', \"l'euro,\", 'r√©unis.', '220', 'a9', 'individu,', 'fiscalit√©,', '√©chou√©.', 'gnurc', \"l'ex-pr√©sidente\", 'civil,', 'najim', 'recens√©s,', 'beatle', 'a26', '2014', 'gifu', 'imprim√©,', 'circulation.', 'contr√¥les.', 'yateset', 'plus.', 'comptes,', 'misato', 'jeu!', '175', 'a87', 'citro√´n.', 'environ.', 'rel√¢ch√©e.', 'avonley', 'al.,', 'l‚Äô√©v√©nement', 'ugj84', 'reconnu,', 'crimolois', 'flamm√®ches', 'lieu.', '√¢pre.', 'karada', \"l'ordre,\", '430', 'bronze,', '(niveau', '11,7', 'elle)', 'm√©tropole,', 'actuel,', '30,', \"l'oligarque\", 'pr√©vu.', \"l'√©v√®nement.\", '(u23),', 'exemple,', '1956', 'volc√°n', 'sokolov', 'l‚Äôobservation', 'l‚Äôaide', 'seniors,', '184,5', 'enfuies,', 'attendues,', 'terroriste,', 's‚Äôaccumuler', 'linsey', 'caract√©rise,', 'iteka', 'allistair', '20h43', 'l‚Äôascension', 'survivre,', 'allemande,', \"[l]'\", 'recal√©s.', '(2-1),', 'jardins.', \"d'abroger\", '10m', 'dames,', 'l‚Äôemploi', 'jong-un', 'r√©gions.', 'ministre.', 'chinois,', '351.', 'critique.', 'l‚Äôarr√™t', 'pirates,', 'peur,', '√©voquait.', \"l'abrogation\", \"d'immigration.\", 'd√©claration.', 'simulaient', 'heenan', 'c√©r√©monies.', '219', 'd√©mocrates,', 'transparent.', 'froid,', '(bacille', '√©lectriques,', 'score.', 'kilom√®tres.', 'margencel', 'sainte-foy-tarentaise', 'ferm√©e,', 'gala.', '626', 'd√©pression,', 'fittest', 'suivantes.', 'cr√©ation.', 'enfants,', 'aavso', 'chimiques,', 'emboite', \"l'autrichienne\", 'retenir.', '1747', \"d'euros.\", 'r√©daction.', 'd‚Äôachat.', 'bellig√©rants.', '11h06', 'rocheuse,', 'd‚Äôeux', '99', 'tir,', 'jacline', 'inquisiteurs,', 'zani', 'lebedev', 'australien.', 'soldeu', 'burundaises', 'entreprises.', \"l'arch√©ologue\", 'repr√©sent√©es.', 'biodiversit√©,', '2b,', 'donn√©es,', 'eeyore,', 'succession.', 'papadakis', 'schoeman', 'soir,', 'hunderook', 'courrai', 'donohue', '79e', 'remorque,', '(-0,2', 'factuel,', 'utile.', '(une', \"d'euros)\", 'lacrymog√®ne.', 'feira', 'f√©minines,', 'voie.', 'zagitova', '13h00', 'somalia', 'jeune,', \"l'ultimatum\", 'fourni,', 'adjacentes.', \"l'inventeur\", 'supernova.', \"l'enclave\", '3,0', 'autonome.', 'ngendahimana', \"d'alli√©s\", '√©voquait,', 'choisie,', 'retard,', 'gurmen√ßon', \"s'affronte\", 'cr√©ations.', 'commentateurs,', 'anti-drogue', 'jean-bertrand', '839', '(-0,3', 'kolesnik', 'ligne¬ª.', 'but.', 'annonce,', 'seegert', 'vague,', 'serment,', '79', 'confiance.', '17.', 'mi-ae', '-7,4', 'remplacement.', 'l‚Äôauteur', 'plafonnements', 'voix.', 'amazon.com', 'gazent,', 'cgt-energie', 'parcours.', 'amylo√Ødes,', 'proposait,', 'doute,', '...', 'r√©formateur,', \"s'essaient\", 'qu‚Äô√†', 'fait,', 'africain,', 'intensifs.', 'l‚Äô√©rosion', 'augment√©e,', 'critiques,', 'accident.', 'enclav√©es,', 'gallican', '6h', '1958', 'gueule?', '22h30', '√©pouse,', 'al-nosra', \"qu'au-del√†\", 'zadig&voltaire', 'iacucci', 'semaine.', 'dominant.', 'kosterlitz', \"qu'impose\", 'accambray', 'exactement)', 'candidature.', 'milliards.', 'l√©gers,', 'pi√©tonne.', 'initiales),', 'bermudienne', 'soir√©e,', \"s'immobiliser\", 'assaillants.', 'emmie', '2h00', '8h15', 'benomar', 'performance,', '362', 'environ,', '19', 'masqu√©,', 'nombre,', 'l‚Äôenseignement', 'plaignant,', 'autoriser,', 'cylindre,', 'substantiel.', 'xxxie', \"d'indemnit√©s\", 'lointaine,', 'd‚Äôinterpellations', 'printemps.', 'l‚Äôarri√®re', 'force.', 'surprendre,', 'domicile,', \"d'inondations.\", 'irakiens,', 'enfant.', 'sources)', 's√©n√©galaise.', 'incidents,', 'championnat,', 'batygin', '16h30', 'al-muftah', '179', 'prom√®nera', 'soutenue,', 'concern√©es.', \"l'encape\", 'prioriserait', 'bris√©e.', 'pr√©visibles.', 'minusma', 'bus,', 'latiboli√®re', 'd‚Äôamener', '12.000', 'irr√©aliste,', 'gauche,', 'grouard-mclennan', 'policier.', '302', 'adversaire.', 'faubourg-saint-honor√©', 'bonnefamille', '√©lus,', 'casseurs,', 'bunyoni', '36,4', '-10', 'journalistes.', 'informations-cl√©s', 'zoufftgen', \"d'affil√©e,\", 'xterra', 'vaudevillains', 'd‚Äôorigine', 'logement,', '√©carts,', 'vergezac', 'chamoix', 'group√©s,', 'euros)', 'tard.', \"l'appareil.\", 'enterrer,', 'ramzan', 'faible,', '5,2', '10h00', 'alimentaire,', \"l'elys√©e\", 'ajout√©.', 'explosion.', \"s'incline\", '[les', 'centre-ville,', 'pr√©isdent', 'prudence.', \"d'inverser\", 'n¬∞20', '10,3', 'd√©pass√©,', 'spektr-r', 'aussi,', 'mobiles.', 'stable.', 'm‚Äôont', \"d'appartenir\", 'homosexuel.', 'tentatives.', 'auparavant.', 'ampahana', 'conmebol', 'suivant,', '2016gkg', 'balajan', \"d'√©jaculateur\", 'supraconductivit√©,', 'j√©rent', 'd√©partements,', 'rapport√©.', 'paradoxodon', \"l'autrichien\", 'l‚Äôabri', 'nord-irlandais', 'mini-', 'v√©lo-boulot', 'bois,', 'd√©finie.', '(m√¢le),', '1995', 'vendredi,', '136', 'd‚Äôarr√™ter', '(15-8).', '≈ìuvre', 'wevelgem', 'pro-d√©mocratie,', 'non-respect', 'r√©alit√©,', 'tauves', '50-60', 'celui,', 'brownsea', 'c√¥t√©,', 'pro-gouvernementales,', \"d'√©vacuer\", 'narbonne-sud', '13e', 'd√©range)', 'insurg√©s.', 'ekaterina', '(57', 'voisine,', \"l'australienne\", '(programme', 'sous-emploi.', 'flammes.', 'vandel', 'trajet.', \"s'absente\", 'rival,', 'campagne.', \"d'√©v√®nements\", 'gr√®ve.', 'combats.', 'repousser.', 'voter.', 'ikiriho', '39,3', 'h.', '95', 'rivo', 'masculine,', 'associations.', 'montcenis', 'tribhuvan', 'instant.', 'manifestants.', \"s'adjuge\", 'short-track', 'tanasan', \"o'shea\", '2005', 'ball,', '√©ditorial,', \"s'√©croulent\", 'civi√®res', \"l'archer\", 'cesse.', '330', 'conc√©d√©.', \"l'individuel\", 'missiles,', 'jambes,', '(1:12.69).', 'd√©border,', '600', 'fins,', \"l'√¢ge.\", '48.', 'non,', 'camionnette,', '530', 'rentrer)', 'l‚Äôacc√®s', 'groenewegen', 'r√©publique.', 'avoisin√©', 'saint-jean-de-thouars', 'combat.', 'central.', 'karlov', '541', 'r√©seau.', 'croire,', 'consultations.', 'rizomm', '44', \"s'empare\", '1982', 'social.', 'manbij', 'jambe.', '‚Äî', '77', 'turco-rebelles', 'sol.', 'pr√©fecture,', 'ksn', 'h724,', \"l'ann√©e.\", 'messing', \"d'expulsion\", 'kalachnikovs', '150‚Ç¨', 'bashar', \"d'inactivit√©.\", 'vide,', 'interdite;', 'menthonnex-en-bornes', 'pr√©cis√©.', 'temp√©ratures.', 'vue,', 'rentable,', 'oscillation,', 'places,', 'd√©tention.', 'ravitailler.', 'sanaullah', 'conteneur.', 'commentaires,', 'kafraya', 'anti-√©meutes', 'reprises,', '(commune', 'suffisantes,', 'burundais).', 'aristide.', 'otage,', 'fakenews', \"s'allier\", 'tournoi.', '(12)', 'opposition,', 'arabe,', 'recruteurs-rabatteurs-endoctrineurs', 'n√©s,', 'benghabrit', '100%', 'boukine', 'partiels,', 'alazra', '4000', 'sang.', 'apparue.', 'osdh', 'moyennes,', '14h00', 'l‚Äô√©tape', \"n'arrivera\", 'pay√©s.', '12e', 'e.coli', 'villageois.', 'fringe', '35e', 'attitude,', '64', 'inverses.', \"d'atterrissage.\", '1862', 'pacifiste,', 'aliona', 'f√©d√©rateur,', '18h03', 'ars√®ne-houssaye', 'candidat.', 'pas,', 'heures.', 'bisexuels', 'l‚Äôattentat', 'non-traditionnelle', 'l‚Äôaccord', 'pompe,', 'marche.', 'supermassifs', '14h20', 'confus.', 'troisi√®mes,', 'olympique,', '17h10', 'casque,', 'famille,', 'pouvoir,', 'kilos.', '(1:11.03).', '√©tat-r√©gion,', 'centenaires.', 'al-chaab', 'loin,', 'total,', 'violence.', '(situ√©', 'date,', 'mercredi,', '42,2', 'cas,', 'identi.ca', 'insurg√©s)', 'irakien.', 'heurte,', 'insistance,', '1944', 'versterby', 'grandeur,', 'cndd-fdd', 'qitaihe', '1,8', 'polo.', 'minutes,', 'sciences,', 'lanneray', 'marie-claudette', 'ressortis,', 'sassou-nguesso', 'sponsor,', 'moyenne.', 'captivit√©.', 'agac√©.', 'intacte,', 'tard,', 'escaldes-engordany', 'droite.', 'croate.', 'interrompu.', 'support√©.', 'pr√©sident.', '(ndlr:', \"d'ann√©es.\", 'rafa≈Ç', 'part,', \"l'emp√™che\", '\"shooter,', 'super-g,', '193,7', '25,9', '25-49', 'talencia', 'mineurs,', 'bless√©es.', 'twitt√©', 'd‚Äôastronomes', 'l‚Äô√©galit√©', 'l‚Äôarm√©e', 'l√∏vset', '(est', 'beau-fr√®re,', \"d'urgence,\", 'n‚Äô√©tait', '388', 'psathyrellaceae', 'rassembl√©s.', 'gravit√©).', 'controverses.', '(24%).', 'sprint.', 'prochain.', \"d'atterrissage,\", 're√ßus.', '15-11.', 'occidentale.', 'proche,', 'contresens,', \"singin'\", '[certaines]', 'sheikha', \"l'embarcation,\", 'mondiales.', 'heure,', 'nouvelles.', 'xuanwei', \"s'enfuyant.\", 'ensuivie', \"l'√©norme\", 'bike-to-work', \"l'ayant\", 'tours.', '9,9', 'battlegrounds', 'locale,', \"s'attendrait\", 'd‚Äôeau', 'm√©canique,', \"n'√©tait.\", \"s'ouvrira\", 'ou,', 'derni√®re,', 'recens√©es.', 'jour,', 'surprenants.', 'effort,', \"d'ann√©es-lumi√®re.\", 'jeudi,', 'armureries', 'abattu.', 's‚Äôy', 'pr√©d√©cesseur,', 'l‚Äôinformer', '4,8', 'ans)', 'territoire,', \"s'ins√®re\", \"l'altercation,\", 'r√©sultantes,', 'communiqu√©,', 'interne.', 'messagerie.', 'vandalis√©e,', \"d'√©puisement\", '66,1', 'vtt,', 'aussi.', 'femmes.', '(n¬∞42),', 'd√©clar√©,', 'sommations,', 'tatous', '(dont', \"d'apr√®s-midi,\", 'fam√©.', 'officiels,', 'evdokimova', \"l'opposition.\", \"d'embaucher\", 'tingmao', 'avoir,', 'secondaires,', 'vot√©,', 'cruaut√©.', 'c√¥t√©.', 'donner,', 'derni√®res,', '1992', 'autres,', \"l'obligeant\", 'd‚Äôalcool√©mie', 'l√©gers.', 'saint-jeannet', 's‚Äôadresse,', '(dtp),', '76', 'bas-c√¥t√©.', 'loin.', '√©mirats-arabes-unis', \"d'accouchement\", '37,5', 'sienne.', 'politiques,', 'gaudill√®re', 'bas-alpin', '√©conomique,', 'am√©liorer.', 'enqu√™te,', \"d'enqu√™te,\", 'ann√©es,', 'concours,', 'elizaveta', '5,', 'huit,', '4e', '1920', '(2/2)', 'l‚Äô√©toile.', 'rebelles,', 'raison,', 'intelligent).', \"l'√©paule,\", 'lrem', 'fortes.', 'sommaires.', '5,8', '14-7.', 'voisin,', 's√©curit√©.', 'fondations,', 'pass√©.', 'interpell√©es,', '3d.', 'farreyrolles', 'spirig', 'reuleuet', 'l‚Äôespace', 'd√©lit√©.', 'tigres,', \"l'incr√©dulit√©,\", 'europ√©ens.', 'deteix', 'anti-corruption', 'soleil,', 'seconde.', \"s'agresser\", 'pr√©-√©lectoral', 'final,', 'chargeant.', 'recherch√©.', 'monde.', 'qualification,', '\"p√©age', '572', 'concern√©s,', 'l√©g√®re.', \"s'attaquaient\", 'habitude,', \"l'arriv√©e)\", 'remont√©e.', 'd√©but√©,', 'caillass√©', 'l‚Äôenqu√™te', 'l‚Äôh√©ro√Øsme', 'chant.', 'superg', 'engag√©.', 'drapeau.', 'bucharest', 'blessure,', 'tiangong-1', 'frb150418,', \"s'enclenchera\", 'r√©cup√©rer,', '94', \"d'artifice\", 'radicalis√©s),', '1988', 'parisienne),', 'd70', 'p√©trole.', '59', \"l'athl√©tisme\", \"qu'assistante\", '73', 'd√©placements.', 'cumhuriyet', 'terroristes,', 'trouvaient.', 'br√ªl√©e,', '0,2', 'vis√©.', 'c√©r√©brale,', 'cosmologiste', 'fonctions,', 'hawayek', 'pr√©sentation,', '1951', 'saint-remy', 'qu‚Äôau', \"l'√©v√©nement,\", 'tournoi,', \"l'escalade\", 'concurrents,', 't√™te,', 'fictif.', 'id√©es,', '√©tape,', 'critiquer.', 'autres.', 'miraculeusement,', 'l‚Äôheure)', \"d'√©lus,\", '2100', 'mollema', 'emprisonnements,', '#balancetonyoutubeur,', 'rattraper.', \"l'espagnole\", 'policiers,', 'larossi', 'physique,', '(ses', 'mineur.', 'nouvelles,', 'centrale,', 'partout.', \"n'exclue\", 'viennent.', 'nous?', 'administratif,', 'dr.', 'prises,', 'alverto', 'aix,', 'op√©ration.', \"d'interrompre\", ').', \"l'appartement,\", 'rj85', '‚Ä¢', '2', \"d'autoroutes\", 'familial,', 'irawan', 'visite,', \"d'effacement.\", \"l'estime\", 'ibigawa', 'trefilov', 'aid√©s,', 'youtubeur', 'masculin,', 'globe.', 'commun,', 'amplifi√©.', 'commission,', '65%', 'm√©canique.', 'shekau', '12h', 'in√©vitable.', 'd√°il', 'papier.', 'pav√©s,', 'gallois.', 'nature,', 'bradie', 'onde,', 'respectives,', \"d'altitude.\", 'ind√©pendantistes.', '2h15', 'imm√©diat.', 'abattue.', '(+0,2', 'institutions.', 'contradiction,', 'fum√©e,', 'individuel.', '10,2', 'chauffeurs.', 'consult√©s.', 'constestent', 'p√¢tissi√®res', '1991', '13', 'dat√©e,', 's‚Äôajoute', 'hanford', 'kilos,', 'd‚Äôeau.', '4km', 'perdues.', 'europ√©ennes.', 'demande,', 'multinationale.', 'l‚Äôinterdiction', 'r√©veillonner.', 'corden', '2344', \"d'inscrit\", 'primats', 'bloqu√©s,', '(meilleur', 'embuscade.', 'cibles.', '1960', 'habiteraient', 'renforce.', '560', '[les]', '2500', 'd‚Äôaccueillir', '130', \"d'emplois.\", 'raph√®le-l√®s-arles', '35', 'l‚Äôentrep√¥t', 'rel√¢ch√©,', 'tuktamysheva', 'menacent,', 'cannabis.', 'similaire,', '15h50', 'si,', 'brdp', 'deniss', '√©voluaient.', 'discipline.', '11,5', 'ce,', 'toxiques.', '√©preuve,', 'lilah', 'gouvernemental,', '√©l√®ves,', 'g20', 'severna√Øa', 'fulgurant,', 'tach√©s,', 'discussions.', 'boetie', 'manoun', 'zone.', '(3/6)', 'artistique,', '(utc)', '249', 'chibok', '270', 'progression.', \"d'aoste\", 'h√©siter,', 'pompiers.', 'd‚Äôor', 'haveluy', 'tawadros', 'ressources.', 'exceptions,', 'euro-star', 'deuxi√®me,', '37', 'transporterait', 'faial', 'obligatoires,', '2003', 'rahmane', 'controvers√©es,', 'd√©mont√©,', 'augment√©,', 'chekatt', 'machinistes,', 'police.', '378', 'lecture.', 'm.', 'habitants.', 'proximit√©,', 'hym√©nophorehym√©nophore', '(34-27).', 'd‚Äôouverture', '1974', 't√©l√©phonique.', 'mimosa,', 'cheminai[t]', 'chalet.', 'r√©sistance.', 'montagne.', 'mine.', 'ultracompacte', '1,4', \"d'enfants.\", '[vendredi]', 'landiers', '88', 'tuktamisheva', 'bombard√©,', '100m', 'attentat,', 'surikova', 'sauvage,', 'total.', '15h10', 'liens.', 'abandonn√©.', 'l‚Äôespagnole', 'fs1', 'imparable.', 'mouradi', 'r√©alisateur,', \"d'assaut,\", 'xv,', 'pays)', 'l‚Äô', '2006', '310', 'complexe.', 'retraite.', \"d'incidents\", '‚Äπla', 'viols,', 'a72', 'theunset', 'ordre.', \"d'√©tonner.\", 'constat√©.', 'salice', 'invisibles,', 'national.', '252,987', 'chercheurs,', 'r√©veillonn√©', 'lieux,', '1200', '15h45', '4x100m,', '20,7', 'kostorna√Øa', 'pape,', \"s'affirment\", '8h40', '(13)', 'p√©riph√©rie.', \"d'√©missions,\", '13,7', 'r√©√©lu.', \"l'alerte,\", 'mini-lunes.', 'rendez-vous.', 'prisons,', 'l‚Äôext√©rieur,', 'a√©rien.', \"l'abandon.\", '(source', 'l‚Äôun', '97', 'd‚Äôidentit√©', 'b√©n√©voles,', 'diff√©rentes,', \"s'offrant\", 'ormillien', '‚Ç¨', 'a36', 'confondues,', 'agricoles.', 'ralentie.', 'rappel,', 'machines.', '10h07', 'phrases-chocs', 'pib,', 'rechercher.', 'concei√ß√£o', 'rivales.', 'c√©r√©monie,', 'monument.', 'responsabilit√©.', 'l‚Äôest', 'interdits.', 'contr√¥le.', 'kirillov', 'semaines,', 'fonc√©,', 'stellaires.', 'pers√©cut√©s.', 'samedi.', 'd√©cennie.', \"d'horreur\", 'terroriste.', \"d'ann√©es-lumi√®re\", 'd√©cret.', 'pacifique,', 'capitale,', \"l'assurance,\", '813', 'mackinnon', '5√®me', 'cheville.', 'chose.', 'renseignement.', 'mondiale,', '1h00', 'nouria', \"d'endiguer\", 'chapecoense', 'u.s.', '7,4', 'l‚Äôarriv√©e', 'inconnus,', 'ont,', 'tourisme.', 'romans,', 'boyau.', 'inusuelles,', 'cube,', 'tu√©s,', 'endah', 'coureurs,', 'oublier.', 'scandales.', 'dictateur,', 'swazi', 'visait.', '12h35', '2b', 'ballons,', 'tcheliabinsk', 'otodontidae', 'lamell√©)', 'l‚Äôeau.', 'paratriathl√®tes', 'bitcoin', 'chauffage,', 'alourdi,', 'subsistance.', 'ousman', '(prot√©geant', 'villalier', 'l‚Äôassistance.', \"d'agripper\", 'universelles,', 'cheremisinov', 'knudsen', 'rendez-vous,', 'iger', 'expos√©s,', 'chiites,', '(pr√®s', 'egdom', 's‚Äôappr√™te', 'ouverte,', 'swazi.', '1', \"d'endosser\", '20h30', 'd1', 'buissi√®res.', \"d'atlanta\", 'renouvelables.', 'km.', 'virtue', 'derniers,', 'condamn√©,', 'touch√©e,', \"d'emmener\", \"qu'accompagn√©\", 'shooter,', 'merci,', '6,1', 'galaxie.', 'orientation.', \"d'inqui√©tudes.\", 'traducteur.', \"l'√©rection\", 'doigt.', 'a1', 'ric).', 'mnuchin', 'castaner', 'aecg', 'r√©giment,', 'affirm√©.', '686', 'nord-syrien.', 'montpelli√©rains.', 'suffrages,', 'place,', '5.', '(alors', '10h45', 'performances,', '1952', 'r√©dacteurs.', '√©carts.', '24√®me', \"l'attaque,\", 'proc√®s.', 'remix√©e', 'perquisitionnait', 'l‚Äôordre,', \"d'aubagne\", 'possible.', 'zhihui', 'f√©in', 'extraordinaires.', 'mauvaises,', 'tu√©e.', 'p√©riode.', '√©mis.', 'l√©gal.', '218),', \"d'art.\", \"l'homosexualit√©\", 'reprendre.', 'championne.', 'police,', 'diff√©rent,', 'yonhap', 'insurrectionnel,', 'inthepanda', 'introductif.', 'portugais,', 'a√©riens,', 'sudamerica', '(√†', \"l'√©tape.\", 'renvers√©es.', '17h30', 'molotov,', \"l'√©toile,\", 'ph√©nom√®nes.', 'faiblir,', 'connu,', 'plou√´r-sur-rance', '452', \"l'automne.\", 'longeville-l√®s-saint-avold', '1938', 'mellett', 'pablo-picasso', '50ene', 'quotidiennement,', '30-19.', 'majlinda', '5', 'estime,', 'anticonstitutionnelle', 'incroyable.', 'incendi√©.', \"l'assassin\", 'mardi.', 'orthographe,', 'chinoise.', 'ivett', 'lors.', \"l'inop√©rance\", 'ressources,', 'pourquoi,', '680', 'faillie', '√©tudiants)', 'solaize', '...).', 'alli√©es,', \"l'enqu√™te,\", \"l'occuper\", 'a430', 'patient.', 'france3', 'australien,', 'gammarth', \"l'afficheur\", 'neige).', 'gaz,', '443', 'simultan√©ment.', \"l'aurait\", '1709', 'contraire.', '254,958', 'fili√®re,', 'provisoire.', \"d'h√©licopt√®res\", 'acte,', 'int√©rieures.', 'intervenu,', 'silence.', 'sabreuse', '110', 'v√©hicule-b√©lier', '-cheminots', 'l‚Äôemp√™che', 'prouv√©e.', 's≈ìur', '3h15', 'mashhad', 'd‚Äôenqu√™ter', 'juive.', \"d'incoh√©rences,\", 'peine.', \"s'ajouteront\", '(remport√©e', 'nuls)', 'compactent,', '6,6', 'grover', 'garozzo', 'voiture,', 'puissance,', 'oga', \"d'explosifs\", 'indispensable,', 'diam√®tre,', 'casseurs.', 'incendi√©e.', 'p√©age.', '20h27', 'sujet,', '√©quipe,', \"l'agresseur,\", \"l'√©cureuil\", 'particip√©,', \"d'observatrice.\", \"l'am√©ricain\", 'difficult√©s.', 'coderre', \"l'h√¥pital.\", '(3', 'cat√©gorie,', 'w.', 'joubert\\u200f', 'd‚Äôalep', 'cath√©drale,', 'reconstitu√©e,', 'transgenres', 'soul√®vement,', 'villegailhenc', \"d'accuser\", \"s'entra√Ænait\", 'presse,', 'giuffrida', 'internautes.', '2e', 'sexe,', 's√©curit√©,', \"d'√©tudiants.\", 'comportement,', 'segment.', 'candidat,', 'commandes.', 'tri)', 'moyens,', \"d'helsinki\", 'gumennik', 'communiqu√©.', \"s'immobiliser.\", \"d'enterrer\", 'reigns', 'sens,', 'l‚Äôordre', 'el-khomri', '[avec]', 'fran√ßaises,', 'sevranais', 'sanguinetti', 'b√©chouanaland', 'mines,', \"d'attitude,\", 'a7', 'restants,', 'mazowiecki', 'tour,', 'sud-est,', 'patna', 'niveau.', 'l‚Äôattaque', 'mars,', \"d'√™tre,\", 'yoka', 'tireur.', '16h46', 'd‚Äô', 'mini-d√©fil√©', 'anthox', \"l'universitaire\", 'lesbiennes,', 'profiter.', 'harasawa', 'mswati', \"l'√©crasement\", '4600', \"l'accuse\", 'beipan', 'leanda', 'zones.', \"qu'habituellement\", \"l'√©ponge.\", 'mince,', 'ouverte.', 'nuages.', 'arpajon-sur-c√®re', 'variables,', 'internationale.', 'l.', 'disparue.', 'remariages', 'caf√©,', 'ndlr).', '82', 'division,', 'mediatransports', 'internationaux,', 'catalanes,', 'pr√©cision.', 'd‚Äôaffaires.', 'robotique,', 'm√©tallique,', 'parall√®le.', '3,1', 'feu.', 'cyberattaque', '12', 'satt', \"d'al-qaida\", '1830', \"l'all√©gement\", 'panique,', \"s'√©chappe,\", '61', 'cristallisent.', 'celles-ci,', 'mer.', '√©cras√©,', 'kittel', \"l'escalier.\", 'parisienne.', 'gratuit\"', '(84', 'canadiens.', 'cousin,', '20h00', 's‚Äôinterposer.', 'd‚Äôann√©es,', 'voitures,', 'touche,', 'courtilles', 'bartali', \"d'hu√Ætres\", 'd‚Äô√©viter', 'conducteurs.', 'agir.', 'unique,', 'drauzou', '¬∞c.', 'proportionnelle,', '68', '√©tant,', 'verruqueuses', 'continent,', 'jaunes\".', 'joueurs,', 'palette.', 'shokrollahi', 'l‚Äôunique', 'embras√©.', '56', 'st√©rile,', 'midi.', 'ma√¢fa', 'karimov', '√©lectorale,', 'poursuivantes', 'adversaires,', 'time-lapse', 'longtemps,', 'judiciaires,', 'collectif,', \"l'auvent\", 'insuffisants,', 'occup√©e.', 'publics.', \"d'autes\", \"d'inscrits\", 'gatignon', 'manzanillo', \"d'octroyer\", 'grotte,', '75,1', 'nord-cor√©ennes.', 'arraisonn√©', 'autant,', 'r√©gime,', 'op√©ratoire.', \"d'√©lection.\", 'd‚Äôaccueil', 'munira', 'citoyenne,', '710', 'accus√©s,', \"d'ins√©curit√©.\", 'lumi√®re.', 'temp√™te.', 'baisser.', '31', 'difficile,', '130,', 'insuffisante.', 'autoroutier,', 'trimestre,', 'coupable,', \"l'enti√®ret√©\", 'frappe,', 'liquide,', '√©c≈ìurante', \"d'astronomes\", \"l'√©lue\", 'liquide.', 'd√©nommerait', 'insuffisant.', \"l'indiquent\", \"d'¬´\", 'noire.', 'affect√©s.', 'pluriel.', \"qu'auparavant.\", \"d'habitants,\", 'secondaires.', 'chimique.', 'seule,', \"qu'√©tant\", '[fusil', 'couture.', \"l'interruption\", \"b'neville\", 'dewalle', 'uz224', 'faudot', 'comp√©tition,', '1984', \"l'explorer\", \"d'embrun\", 'origone', 'dellown', '568,', 'yiwen', 'attaqu√©.', 'convocations.', \"l'investiture.\", '12h45', 'steers', 'iv√°n', \"l'equipe\", 'ultracompact', \"l'objet.\", 'mer,', '62', 'discours,', 'pr√©vues,', 'main-d‚Äô≈ìuvre.', 'habitations.', 'keewatin-le', 'particulier.', 'l√©gislatif,', 'vue.', 'lepromatosis', '879', 'incendi√©s.', 'inchang√©es,', 'marche!', 'pass√©e,', 'institutions,', '07', 'sainte-flaive-des-loups', 'sud-cor√©ennes', '8h25', 'alarza', 'portes,', 'course-poursuite,', '2362', 'percuteront.', 'imm√©diatement,', '11h30', 'exploser,', 'tellement.', '997', \"l'anthropologue\", \"l'obs\", \"l'engin\", 'inconnue,', 'proth√®se.', \"l'inf√©riorit√©\", 'ps4', 'lenlevay', \"l'exploit\", '(h√¥tel', 'kefraya', '180', 'paul-boncour', 'bolivien,', 'u23', 'fortune)', '16h45', 'rafl√©es', 'respect√©es.', 'mede', 'd√©c√©d√©e.', 'fran√ßais,', \"d'otage\", 'effondr√©s.', 'jaune,', 'pr√©m√©dit√©e,', 'villars-les-dombes', 'kait', 'but,', '67)', \"l'attaque.\", 'monsters,', \"l'impact.\", 'solaire.', 'offensives,', 'critiqu√©es.', 'coll√®gues.', 'mission,', 'twitte', 'verre.', 'g√©n√©rale,', '100¬∞c)', 'suisse.', '4', 'sinitsina', 'affect√©e.', '23h00', 'aqpa', 'remarquable,', \"d'encaisser\", '2002', 'tomaszow', 'distance.', 'statistiques.', '240', \"s'emparent\", 'pr√©sent,', 'noir,', 'p√®re,', '(environ', 'adopt√©e.', \"l'hydrog√®ne\", 'l‚Äô√©lectricit√©', 'ont-ils', 'locaux,', 'd√©placements,', \"n'obtient\", 'surprendre.', 'vendredi.', 'suivante.', '(85).', 'sainte-maire', 'l‚Äôinsurrection,', 'semaines.', 'enfant,', 'vadym', 'souhaite.', 's‚Äôinscrira', \"fouquet's\", 'fluides,', 'philippy', 'galaxie,', 'gratuite.', 'hawija', \"d'affil√©e.\", 'sunnites.', 'lightfoot', '22h45', 'pill√©es,', 'inverse,', 'derulo', 'victimes)', 'al-bab', 'belges,', 'd‚Äô√©bri√©t√©.', \"l'oligarchie\", '1,1', '6000', 'marie-florence', 'assaillants).', 'feux,', 'dos,', \"s'√©chappaient\", 'amendements,', 'cop22', \"l'ind√©pendance.\", 'malhonn√™te,', 'anglaise,', 'ljubjana', 'quai,', 'pc,', '2004', \"l'espagnol,\", 'nucl√©aires,', 'l‚Äô√©cole', 'vid√©,', 'nustar', 'l‚Äôorganisation', 'significatif,', 'mesur√©.', \"l'indignation\", 'drogue.', \"l'association.\", 'septuag√©naire.', '13h30', 'sacr√©-c≈ìur', 'solignac-sur-loire', 'homosexuels,', 'loyalistes.', \"l'√©gorgement\", 'bahier', 'planchers.', 'richesse.', 'violences,', 'torrentielles,', '500', 'surface,', \"d'alliance\", 'a√©roport,', 'thouless', '9,7', 'partie,', 'locale.', '(56', 'podium,', '3,6', '(1-0)', 'gona√Øves', '16,', 'participations,', 'n‚Äôallaient', 'mezouar', 'jgp', 'lectorat.', 'donc,', 'travailliste,', 'fait.', 'a63', 'continuent,', 'h√¥te,', 'femmes]', 'portes.', '(la', 'dynamique.', 'retrouv√©,', 'licenci√©s,', 'jambe,', 'recens√©.', 'fin,', 'sens.', '534', 'camion-benne', 'pneus,', '565', 'stades.', 'apr√®s-midi.', \"d'origine,\", 's‚Äôaffronter', 'victime.', 'lierde', 'ph√©nom√®ne,', 'carburant,', '81', 'combattants,', '1000', 'midi,', 'rnc', 'gaber', 'vigueur,', 'produire.', 'percut√©es', '(un', 'filtr√©s.', 'r√©silience.', 'hubbell', 'amisom', 'd√©terminer.', 'gayrussia.ru', 'secouristes,', \"d'√©tat),\", \"s'emparer\", 'argileux,', 'examens.', 'minxia', \"d'escrime\", '8', 'fra√Æchements', \"l'habitude,\", '(chauffage,', 'cr√©ation,', 'l‚Äôain', 'attentats,', 'administratives,', '.', '935', '1973', 'populismes,', '2016-2017', \"l'allemande\", 'kol√©las', 'communaut√©.', 'd‚Äô≈ìuvres.', 'crimes.', \"d'√©mancipation,\", 'apparente,', 'l‚Äôautre.', \"l'escrime\", 'borno', 'records.', 'uberx', 'bluehole', 'b,', 'l‚Äôengagement', \"l'√Æle.\", '3000', 'attaque.', 'peut-t-on', '√©paules.', \"l'ex-premier\", 'l‚Äôar√©na', '183', 'strat√©giques.', 'holmstr√∂m', 'marginales.', \"l'emploi,\", \"qu'ait\", 'officielle.', 'parcours,', '(√¢g√©s', 'sort,', \"l'√©poque,\", 'octog√©naire,', 'd‚Äôhistoire,', \"d'apr√®s-midi.\", 'sz√°sz', 'intentions,', 'calme.', '11h20', \"l'√©ditorial\", 'pr√©cises,', 'mascognaz', 'interpel√©.', '418', 'sinistr√©s.', 'maelbeek', '20,', 'pouvoir.', '(38', 'inconnue.', '13769', 'spectaculaires,', 's√©minariste,', \"qu'am√©ricain,\", '(65,5', 'gm√ºr', 'suresh', 'd√©termin√©e,', 'scandales,', 'inconnues,', 'annon√ßait-il', 'proches.', \"(jusqu'√†\", 'demi-finale.', 'excessive,', 'amie,', 'tous,', 'interpellations.', '9\"81', 'mont√©e,', 'varazdat', 'ahrar', \"d'impasses\", 'centimes,', 'minist√®res.', 'rapide.', 'emploierais', 'attouch√©', 'il-sung', 'thuile', 'plac√©s,', 'l‚Äô√©tat', 'bless√©,', '√©v√©nement.', 'identifi√©s.', 'fentz', 'chevrot', \"l'appareil,\", '(+', 'l‚Äô√©preuve', 'tweet√©', \"l'homophobie\", 'aquatique,', 'chabri√®res', 'reprise.', '148.000', \"[l'\", 'embryonnaires.', 'qatari', \"d'observateurs\", 'jours).', 'visage.', 'lib√©r√©s,', 'instable,', '11h55', 'hammami', 'sandhurst', 'renforc√©es.', 'suzaki', \"d'√©cart.\", \"l'organisateur,\", '[1]', 'activit√©,', 'd‚Äôhiver', 'penseur,', '12h10', 'l‚Äôordre.', \"s'envoler\", 'revendiqu√©e,', 'd‚Äôapr√®s', 'nageur.', 'l‚Äôantenne', 'jusqu‚Äô√†', 'cimeti√®res.', \"l'arraisonner.\", 'introuvable.', 'assasinnat', 'r√©sultats,', '905', \"l'aissaillant\", '√©quilibr√©,', 'populaires,', 'figueroa', 'm√©diatique.', 'tu√©,', \"l'√©tat,\", 'automne.', 'et,', 'chelem.', 'pr√©sidentielle.', '40√®me', 'borghild', 'l‚Äô√©glise', '√©changer,', '568', 'd√©part,', 'plaissan', '√©maillant', 'l‚Äôappellation', '√©cologique,', 'patineuses', 'pr√©tendants,', '93', 'salaheddine', 'dur√©e.', 'publi√©s.', 'r√©daction)', '[situ√©]', '3,7', 'n√©cessaires.', 'corps,', 'd√©faite.', 'anticonstitutionnel.', 'marib', 'd√©nivel√©,', 'majeures,', 'seconde,', 'paille.', 'mythique.', \"l'√©preuve.\", 'ordinateur.', 'd‚Äôextr√™me', '4500', 'macronleaks.', '42%', 'guerbeur', '237', 'terrorisme.', 'messe,', 'pr√©pay√©es,', 'robart', \"l'accoutum√©\", 'amirabad', '1,00', 'majorit√©,', '67', '√©tablie.', 'sociaux,', 's√©curitaires,', 'pooley', 'tonqu√©dec', 'bolopion', 'l‚Äôarsenal', 'l‚Äôautoroute', 'sollicit√©s,', 'd√©n√©s', '-syrienne', '(vers', \"d'attaquer\", \"l'arrestation\", 'handballeuses', '8,2.', 'mouraud', 'uxio', 'auvergne-rh√¥ne-alpes', 'quartiers,', 'ministres,', 'enawo', '38', 'turquo-syrienne', '19h25', 'accompagnatrices.', 'pillages,', 'maizi√®re', 'cass√©e.', 'n√©o-z√©landais', 'relais.', 'argoun', 'parfaite,', 'sacs.', 'divergentes.', 'lapins,', 'intervenaient.', 'turque.', 'jeudi.', 'r√©publique,', 'place.', '(u23)', \"l'agression\", 'sous-investissement.', 'expert,', 'anniversaire,', '318', 'actuelle,', 'victoires,', 'd‚Äôatterrir', 'chauss√©e,', 'd√©cision.', \"d'extinction\", '1986', 'interfamilial,', 'd√©claration,', 'ostr√©icoles.', '121', 'inqui√©t√©s.', 'l‚Äôas', 'finalement.', \"d'alerte.\", 'abandonne.', 'g√©n√©rations,', 'nord-est.', 'mc-130', '3.', 'responsables.', '0,4', 'l‚Äôentr√©e', 'pr√©c√©dente,', '(tous', 'd√©missionn√©,', 'juniors,', 'trente-deuxi√®me', \"l'intersection\", 'instructions.', 'administrent.', 'intoxiqu√©s,', '75', 'quinquennat,', 'sprint,', '(apr√®s', '5,1', 'examin√©e.', 'sexes).', 'thermique,', 'surclass√©e', 'd√©chets.', 'ind√©cise,', '30', 'd√©truit,', 'notamment.', 'l‚Äôaccident', 'd√©crite.', 'r√©gion.', 'armes.', \"l'√©tude.\", 'haut-vol', '(majorit√©', 'grand-fort-philippe', 'silence,', 'mosqu√©e.', 'longeville-l√®s-st-avold', 'd‚Äôadministration', 'l‚Äôimmeuble', 'm√©faits.', 'affirme-t-il.', 'sortante.', \"d'ironman\", 'troadec.', '(3,07', 'international,', \"s'√©crasent\", 'plupart,', 'barguil', 'd√©tiennent.', 'banjul', 'l‚Äôincendie.', 'r√©parages', 'montrevel-en-bresse', 's√©rieux.', '035', 'ligo', 'rapides,', 'racistes.', 'l‚Äôannulation', 'ordinateur,', \"d'√©nergie.\", 'ryf', 'victimes,', '26-49', 'europ√©enne.', '√©l√®ves.', 'al-ansari', 'bƒõlohradsk√Ω', 'compte.', 'vaste.', 'tarah', 'dodet', 's√©cher.', 'd√©j√†.', 'anniversaire.', 'confrontation,', \"l'investiture\", '173', 'l‚Äôesp√®ce', 'mineur,', 'matin√©e.', '$.', 'qualifi√©s,', \"d'halt√©rophilie\", 'tricolore,', 'maire.', 'am√©ricain.', 'cyberattaques', 'l‚Äôhum√©rus', 'd√©clench√©.', \"l'√©viction\", 'pedersoli', 'usain', 'espoir.', 'pts2', 'utc-5', '(1/2)', \"d'√©oliennes\", 'cabine,', '751', 'temporairement.', \"qu'aujourd'hui.\", 'international.', 'nouvelle,', \"s'√©tablissant\", 'tweets', '22h26', '1976', 'exercutive', '23', 'choc.', 'sn1987a', 'retour,', 'rapport√©e.', 'divers,', \"s'arroge\", 'appara√Ætre,', '70%', 'parisienne,', \"s'√©carter.\", 'tchourkine', 'd√©gag√©.', 'j‚Äôaga√ßais.', 'moteurs,', '750', 'a13', 'faites.', 'benn√©es', '527', \"l'essayiste\", \"d'affrontements\", 'pinheiro-croisel', 'atca', 'a43', 'jeu.', \"d'ambassadeurs\", 'secours.', 't√¥t).', \"s'entretiendront\", 'consp√©cifique', 'antiballes', 'nangarh√¢r', '200m', \"d'indemnisation,\", 'annul√©es.', 'mobilisation,', 'continue,', \"l'ascendant.\", 'supermarch√©,', \"d'il\", '1977', '3h00', 'capacit√©,', \"d'hommages\", \"l'apl.\", 'chauffeur,', 'tendues,', 'barkhane', 'tortur√©es,', 'eelv', '(16-16),', 'akbhar', 'd√©cennie,', 'mckeon', '(1780', 'moretz', '201702477', 'acte.', 'renseignement).', '281', 'tragiquement,', 'g√©ant.', \"d'orques\", 'pelos', 'soutenues,', '√©tudi√©.', 'barrage.', '2933', 'faldum', 'collectivement.', 'prix,', \"d'antis√©mitisme\", 'temp√©rature.', 'lancastel', '260', 'd√©sertes.', 'doorbin', 'radars-auto.com', 'shaoyo', \"l'humanit√©.\", 'nord-cor√©enne.', 'a71', 'douteusement', 'suite.', 'ponts,', 'plainte.', \"s'attribuer\", 'ans),', 'l‚Äôeffondrement', 'm√©dias,', 'r√©habilitation,', 'vircoulon', 'serflex', \"d'ind√©pendance.\", \"s'affronteront\", 'l‚Äôartisan-ma√ßon', 'd‚Äôaust√©rit√©,', 'policier,', '16.000', 'pontal', '17h55', 'contr√¥lent.', 'yanovskaya', 'g√©n√©ral,', 'd‚Äôassassinat', '14h', 'c√¥tes,', 'pro-r√©gime,', 'garde-fronti√®res,', 'duathl√®tes.', \"l'actrice\", 'deckers', 'secteur.', '2001', 'br√ªl√©es.', 'or,', '261', 'd√©truites.', '22e', 'aequo', 'commune.', 'd√©raillement.', 'draghi', 'buralistes,', 'mis-parcours', 'a645', '(auteur', 'nu,', 'images,', \"l'ex√©cutif,\", 'cigarettes,', 'minute.', 'km,', 'poursuivraient', 'accords,', 'attaques.', '91', \"d'aurores\", 'troublant,', 'ensemble.', '11h35', ':', 'bareigts', 'racisme,', 'subis.', 'rebelles.', 'r√©sidait,', 'pourtant,', '1-1,', '36', 'constructive...', 'diam√®tre.', 'habitants,', '3d', \"d'otages\", 'menace.', 'a709', 'd√©veloppement,', 'mener.', 'submersions', '4.000', 'gouvernement.', '\"tellement', \"d'important\", 'province.', 'g√©nelard', '27√®me', 'all√©gations.', 'anti-balles', 'neutralis√©.', 'm√©galodon', 'd‚Äôaccusations', 'routes.', 'darmanin', 'navire,', 'messages,', 'pill√©s,', 'shoemaker-levy', 'kharlan', 'd√©pos√©,', '178', 'anglais),', 'jamais,', '2008', 'morts,', 'expos√©e.', 'd√©gag√©es,', 'd‚Äôautrui', 'hexavalents', 'sociale,', \"s'enfonce\", 'naesen', 'cons√©cutif.', 'vid√©o.', 'arrogant,', '(29', 'canop√©e.', 'conclu,', 'mondial.', 'fouet.', 'obamacare', 'homosexuels.', '20h45', 'motifs,', 'eleonora', \"s'accrochent\", 'provinces,', 'change)', 'parlementaire,', 'qui,', 's√©rie,', 'proportions.', 'm√™me,', 'direct-live', '1789', 'proposition,', '84', 'rebelle.', 'ondrej', '(4/8)', 'avance.', 'janvier,', 'skichko', '(4,6', 'aboubakar', 'ridicule,', \"d'essence.\", 'br√©silienne.', '584', 'fini.', 'rempla√ßant,', 'exhort√©s', 'm√©diateur,', 'compl√®tant', 'f√©vrier,', 'incontr√¥lable.', 't√¥t.', 'trouv√©s.', 'fukushima', '17h00', 'th√©√¢trale.', '2011', 'l‚Äôattentat.', 'neige,', '√©g√©enne.', '6,2', 'thrones', 'fa√ßade.', 'jong-oh', 'livois', 'italienne.', 'confi√©.', \"l'√©loignement\", 'l‚Äôantis√©mitisme', 'est,', '533', \"d'√©vacuation.\", 'kurdes,', 'sortie,', '82%', 'languedoc-roussillon-midi-pyr√©n√©es', 'retourn√©.', 'horvitz', 'r√©ellement,', '√©t√©,', 'brasero,', \"d'assistante.\", 'man≈ìuvre', 'pr√©sident-wilson', \"s'allonger\", 'palmar√®s.', 'figurantes', 'bonne.', 'idlib', \"s'opposait\", 'komatsubara', 'aurait,', 'destruction.', \"d'espionnage.\", '132,8', 'b√©quilles,', 'keat', 'cyclistes,', 'manifestant,', 'l‚Äô√©toile', 'lemoussu', '√©mirati,', 'permis,', \"qu'avant,\", 'kadyrov', 'taponier', \"d'agen\", '(4', 'num√©riques.', \"s'am√©liore.\", \"l'antenne.\", 'furman', 'vouloir,', 'ruzafa', 'fiamingo', 'semaine,', 's‚Äôagit', 'hospitalis√©.', 'm√©nag√®res.', 'rotaxane', '√©trang√®res,', 'exp√©rience,', 'gu√©my', '1931', 'l‚Äôappui', 'cauchi', \"d'amortir\", '6,7', 'avanc√©es.', 'mois)', 'al-ra√Ø', 'chrissie', 'reyntjens', 'bodnar', 'afghane,', 'alentours.', 'poursuivante.', 'dit.', 'r√©serv√©s,', 'cons√©quent,', \"l'√©tranger,\", 'non-inscription', 'ypg', 'syndicats.', 'ferdi', 'pr√©vu,', 'bourgines', 'pri√®re,', \"d'enqu√™ter\", '106', 'obtemp√©r√©.', 'militaires,', '(responsable', 'agressions,', 'laissez-nous', 'serri√®res', 'cela.', 'entrave,', 'palestiniens,', '(2b),', 'marie-alta', 'r√©action,', '10h20', 'moteur.', 'djihadistes.', 'subit,', '(rurale)', 'ncsm', 'ind√©pendance,', '21,9', 'rundstadler', 'd‚Äôachat,', 'gras,', 'maladie.', 'inqui√®te,', 'connue,', '950', 'physicien.', 'valoir,', 'collonge', 'salafistes.', 'candidats.', 'assourdissant\",', 'saint-hippolyte-du-fort', 'protestataires,', 'gravitationnelles.', 'olympiques,', 'sociaux.', 'recoller,', 'veille.', 'coma)', '.¬ª.', '(+0,1', 'cela-dit', 'chrabot', '(23,25', '√©lite.', '18h00', '78%', 'bagarr√©s', 'qatari,', 'technique.', 'eux.', 'continu.', 'd√©cider,', 'rapide,', 'dangereux,', 'manquante.', \"qu'animateur\", 'silicone)', 'bzaah', 'dolenec', 'rebaudengo', '2.600', '616,93', 'camps.', 'suite,', '351', \"d'√©preuve\", '(1/3)', 'puis,', '(5/8', \"l'√©olien\", 'gratuit.', 'r√©mun√©rateur.', 'examen.', 'point).', 'fleury-en-bi√®re', \"l'√©p√©e\", 'kablan', \"s'exprimer,\", '√©claircie,', 'steadman', \"d'√©vapotranspiration\", 'que,', 'manifestations,', 'fran√ßaise.', '20¬∞c', '√©tat,', 'arabes.', 'envergure,', '(p√©ages', 'fusil.', 'personnes.', 'record.', 'dissimul√©s.', 'mergitur', 'd√©finitif.', 'finale.', '(46', 'd√©put√©.', 'jocelerme', 'oleksandr', '262', '190', 'chute,', 'juifs,', '[...]', 'implication.', 'd√©j√†,', 'primaire.', 'k.', 'droite,', \"s'√©crase\", 'p√©choux', 'tf1', 'dollars.', 'r√¢ni', 'finisher', 'pr√©sident).', '1,5%.', 'agress√©.', 'armes,', 'chaolin', '9,8', '6,9', \"l'administration.\", 'photo-finish', \"l'arr√™ter\", 'v√©t√©ran,', 'mois-ci,', 'difficiles.', 'n√©gatif.', 'popescu', 'netanyahu', 'parlementaires.', 'hessa', 'domaine.', 'visiteurs,', 'tu√©es,', 'route.', 'pr√©occupant.', 'suspect.', 'boko', '(198', 'd√©sormais,', 'l‚Äôaurait', '10,', 's‚Äôamplifier.', 'frodeno', 'rycroft', 'crs.', 'd√©p√©nalis√©e', 'foua', \"l'√©quipe.\", \"collonges-au-mont-d'or\", 'belgo-fran√ßaise', 'connus.', 'aix-en-diois', 'ypj', 'henao', 'd√©fense,', 'sci√©,', '1950', 'fekl', 'avignon-nord', 'maigres,', 'infiltrator', 'azaz', '9,4', 'tronc,', 'interpel√©es.', 'scolaire,', 'cheikhoune', 'clandestine,', 'anticipaient', \"s'aggraver\", 'd‚Äôacc√®s', 'avocats,', '58', 'socio-professionnelles', '15h49', 'champs-de-mars', 'boucl√©e.', 'gogolev', 'neurones.', '31e', \"l'entrep√¥t.\", 'bodoland', 'pi√©tons,', '21st', 'affectionnent,', \"d'art,\", 'baudreix', 'parents,', 'rembi', 'groupe,', 'sud.', 'roquebrune-cap-martin', '11', '10h35', 'rohingya', 'l‚Äôaustralie', 'voitures.', 'suspects,', '748', 'tableau.', 'indiquent-ils.', 'graves.', 'irakien,', 'wts', '1985', 'blanches,', '2011a', 'pubg', 'olucome', 'respectable,', '(sur', 'l√©gale,', '√°d√°m', 'ghalia', '98', 'intervenir.', 'd‚Äô√©tudier', \"d'opposants\", 'heures,', \"l'√©vacuation.\", 'ferm√©e.', 'crapuleux,', 'chifeng', 'pratique.', 'caldoches', 'd‚Äôapr√®s-midi.', 'jours,', 'frommhold', 'l‚Äôarm√©e,', 'existence.', 'valladont', '√©nerg√©tique,', 'pr√©cise-t-elle,', 'pr√©caution.', 'bombe.', 'performant.', 'dispositif,', 'pass√©,', 'dernier)', 'c),', '(banques', 'l‚Äô√©toile,', '34', 'inter.', 'abiy', 'zeltveg', 's‚Äô√™tre', '[num√©ro', 'trevet', 'siemens-gamesa', 'accrochage,', 'vainqueurs,', '235', 'second,', 'l‚Äô√©dition', '\\u200b\\u200bhuy', 'd√©fections,', 'changiez.', 'exceptionnel.', 'locomotive,', '12h05', 'club,', 'al-zenki', 'coll√®gues,', 'vori', '√©crit,', 'presse.', \"bug's\", 'l‚Äôinnovation', 'octobre).', 'l‚Äôon', 'national,', 'demande.', 'rectorat,', 'werzy', 'enqu√™ter.', 'mbonimpa', \"d'arrosage\", 'amis,', 'croissance,', 'agustiani', 'l‚Äôendroit', 'exp√©rience.', 'hospitalis√©s.', 'gauche.', 'ndlr]', '54%', 'privil√©gi√©e.', 'serr√©s.', 'tous.', 'frigo.', '25', '69', 'uae', 'alaphilippe', 'explosion,', 'br√ªle,', \"d'am√©nager\", 't√©l√©phone,', \"l'h√©micycle\", 'alisa', 'touristes.', 'nicalek', 'franco-britannique', '68,', 'lib√©rales,', 'journalistes‚Ä¶', 'semi-marathon,', \"s'affrontant\", 'interrog√©s,', 'fonctionnaires.', 'caisses.', \"d'enseignes\", 'magumeri', 'jarablus', 'p√©nale.', '208,5', 'repr√©sentatifs.', 'sondages.', 'sp√©cialis√©.', 's‚Äôopposer', 'd‚Äôimmigration', 'coquillat', 'terceira', 'd‚Äôalerte', \"d'immatriculation.\", 'mori√®res-l√®s-avignon', 'poursuivantes.', 'unit√©,', 'altitude,', 'pr√©cise-t-il.', '165', 'sauf.', 'fronti√®re,', 'complotistes,', \"l'infraction.\", 'lauren≈£iu', 'mesures,', 'haringer', 'syriens,', 'al-cham', 'learmonth', 'porcari', 'poignet.', 'saint-christol-lez-al√®s', 'joudi', 'kavtiƒçnik', 's√©curis√©,', \"l'ascendant\", \"l'√©tait.\", 'alvi', 'mesure,', 'endroit.', '185', 'kitzb√ºhel', \"l'ouest.\", '2941', 'reem', \"l'explosion,\", \"s'alourdir\", 'municipaux,', '-62kg', \"l'ambassadrice\", '341', 'lundi.', 'liupanshui', 'passion,', '√©pisode.', 'tr√¥ne.', 'nucl√©aris√©', '(n√©', 'quotidien.', 'etc.).', '√©v√©nement,', 'ult√©rieurement.', 'canzio', 'minniti', 'chang√©,', 'vues.', '17h50', 't√©moins,', '8h20', 'hospitalis√©es.', 'claire,', 'lembach', 'baiss√©,', 'l‚Äô√©v√®nement', 'd‚Äôaller', '\"colors\"', 'inond√©s,', 'boulevard,', 'martrenchard', '2012', 'd√©cr√©t√©.', '\"haine', 'cheikhoun', 'nord-cor√©ens,', 'minuit,', 'l‚Äôann√©e', '17h21', 'mitb', 'ferm√©s,', '0,', 'l‚Äôid√©e', 'redstorm', '33√®me', 'maladie,', 'soir.', 'kayanza', '203,', 'l‚Äôam√©ricaine', 'sources,', '(photographiques,', 'fusillade.', 'football,', 'feu,', 'militaire.', '(binaire', '56.', 'extr√©mit√©s.', '1500', 'bourgogne-franche-comt√©', 'rafistol√©e', 'golbarnejhad', '345.60', \"s'alourdit\", 'automatique.', 'match.', 'passager,', 'gorbunova', '(...)', 'bilan,', 'vasilievich', '400', 'patiente,', 'fois.', '2016', 'attentats.', 'projet√©s.', 'rotations.', 'lyc√©ens,', 'publique.', 'indisponibles.', 'l‚Äô√©tablissement', \"l'immigration.\", \"l'accident,\", 'possible,', 'fatale.', 'd√©-polluer', '6h30', 'distan√ßant', 'marathon,', 'britnannique', '6600', 'contexte,', 'hany≈´', 'transport√©e,', '44,4', 'r√©forme,', 'leprae', 'canadiens,', '√©vit√©e,', 's‚Äôil', 'mounzer', 's‚Äô√©tendre.', 'faute.', 'dimanche.', 'axe.', 'bloqu√©.', 'lieu,', 'enedis', 'train.', 'expliqu√©,', 'polyansky.', 'senior,', 'propositions.', 'l‚Äôhiver.', 'projet.', \"p'tits\", 'radio-t√©l√©vision', 'repouss√©,', 'humains.', 'karroum', 'guilherand-granges', 'novices,', 'ch√¢tellera', 'hausse,', 'neutraliser.', 'habitations,', '(des', 'racistes,', 'a344', \"s'appliquera\", 'gazeuse,', 'tarasova', 'emport√©.', 'arm√©,', \"d'exemple,\", '√©lections,', 'malgaches.', '6,4', 'usines.', 'coursan', \"s'attendra\", '1926', \"d'affaires,\", 'c.', 'valdigui√©', 'l‚Äôimmigration', '14h50', 'soldats.', 'entretien,', '√©conomiques.', 'face,', 'floret', '(il', 'aquaris', 'raciale,', 's‚Äôapproprier', 'peinture,', \"l'entreprise,\", 'opportuniste,', \"l'aube,\", 'telegram', 'l‚Äôinterroger.', '√©galement.', 'correcte.', \"l'astrophysicien\", 'apr√®s,', \"d'affiche\", '13h55', 'ossements,', 'partir.', \"d'avocate\", 'grand,', 'lendemain.', '2011,', '(nord-est', \"l'obligation,\", 'a40', 'l‚Äô√©tranger', 'difficult√©.', 'daech', '193', 'murashov', '225', 'privert', 'rentr√©e.', 'usine,', \"d'atterrissage\", 'egorian', 'd√©cision,', 'd‚Äôinformations', 'moment.', 'poisson.', 'travail.', '√©lectorelle.', 'parquet.', 'cordova', \"d'enqu√™te.\", '\".', '(1', \"l'innocence\", 'aquatica', 'd‚Äôint√©r√™ts,', 'flou.', '11h15', 'organisateur.', 'eternalblue', 'r√©sidences,', 'luckey', 'abstenus.', 'judo,', 'r√©alis√©,', 'anena', 'hayk', '2200', 'd√©bris.', 'gazeuse.', 'territoire.', '1980', 'australie-occidentale', 'hydrog√®ne.', '(en', 'a66', 'brusquement.', \"d'embarquement.\", '(2/4)', 'v√©hicule,', '6.000', 'portes-voix', 'd√©fense.', '(le', 'd‚Äôimp√¥ts', 'menacent-ils', 'mirinda', 'stations,', \"d'abus\", 'acteur.', 'fragments.', 'bukin', 'terre,', 'corrects,', 'besoin,', 'gafilo', 'a837', 'frelat√©es', 'rivi√®re.', 'incertain.', 'eswatini', 'informations,', 'syrienne,', 'conviviale.', 'rendu.', 'yildirim', \"l'athl√©tisme.\", 'salafistes,', '1900', 'pr√©c√©dent,', 'solaires.', '(maillot', 'survenue,', '2022', 'suffisant.', 'l‚Äôinitiateur', 'suppl√©mentaire,', 'fronts,', 'lice.', 'follebin', 'parquet,', 'compteur,', '23h', 'colaboy', \"d'√©craser\", 'chahut√©.', 'formation.', 'a61', 'buteuse,', 'ind√©pendance.', 'effondr√©es.', 'puissants,', '207', 'l‚Äôassistanat', '1,7', 'facundo', '12h40', '21', 'arbres,', 'chancelier,', 'sport.', 'neutraliseurs', \"d'embarcations\", 'minsk-arena', \"d'enrichissement\", 'relativis√©,', '55', 'supprim√©es.', 'cryptomonnaie,', 'occansey', \"d'autobus\", 'spectropolarim√®tre', 'v√™tements.', 'm√¢les,', 'd‚Äôo√π', 'radamus', 'mort,', \"l'australien\", '(moins', 'syndicats,', 'stations-service.', 'm‚Äôen', 'capitaine,', '900,', 'aisen', \"l'√©galit√©,\", 'robertis', 'communes.', \"l'hommage\", 'inversement,', 'alain-guillaume', \"d'inondation.\", '(', 'plein.', 'd√©ception,', 'journ√©e.', 'seyssuel', 'egor', \"l'industrie.\", 'bozdag', 'enlev√©es.', 'camionnette.', 'salari√©s,', 'anti-immigration', 'd√©fier,', \"sant'eugenio\", 'recharg√©s', 'd√©battre.', 'chercheurs.', 'a√©ronefs,', 'animateur,', 'rohingyas', \"l'homme,\", 'g√∂k√ßek', 'baisse,', '2,2', '2017,', 'humvees', 'centre,', 'nebelhorn', 'gendt', 'recadr√©s', '87', \"s'affrontent\", 'observ√©e.', 'am√©ricains.', 'barnaba', 'mattew', 'intimit√©.', 'lions,', 'pr√©sentes,', \"d'avance.\", 'nolis√©', 'anglais,', 'bless√©.', 'imm√©diate.', 'hanyu', 'tippler', 'terme,', 'blessures,', \"s'attirant\", 'salaire),', '[n.d.r.', '10h15', 'automobile,', 'djihadiste.', \"l'habitacle\", '133,3', 'com√®te,', 'exig√©es.', '(3/3)', 'intemp√©ries,', \"l'auto-stop\", 'acides.', 'tr√©molet', 'mercredi.', \"s'arr√™ter,\", 'coop√©rer.', 'suspendu.', 'personnels,', 'qu‚Äôils', 'installations]', 'd‚Äôun', 'enqu√™te.', '18.', '4,6', 'celle-ci.', 'l‚Äôimage', 'demi-heure.', 'exemplaire.', \"d'espionner\", 'neid', 'vivantes,', 'lieux.', '11h50', 'actrice,', 'lointain.', 'budg√©taires.', '8e', 'bousrez', 'rue.', 'fusils-mitrailleurs,', 'sanitaires,', 'l‚Äôop√©ration', '(1/5)', 'a16', 'olha', \"d'affil√©e\", '56,5', 'l‚Äôenqu√™te,', 'besbes', 'l‚Äôenfouissement', 'assi√©g√©es', '7,9', '≈ìuf', 'tbritannique', 'invit√©,', 'burundaise,', '√≥scar', 'anim√©,', 'criminels.', \"d'australie\", \"playerunknown's\", '96', '(82', 'champoluc', 'messages.', 'guerre.', 'l‚Äôagglom√©ration', \"l'apr√®s-midi,\", 'fumer,', 'scrutin.', \"l'emporter.\", 'villes,', 'conflit,', 'hors-piste.', 'hier.', 'chacun,', 'gendarmerie)', 'temps.', 'lumineuses),', \"(l'aile\", 'pr√©caires.', 'searchlight', '0,3', 'exprim√©,', 'publicitaires,', 'o‚Äôdonnel', \"l'atmosph√®re,\", 'combattants.', 'moisin', 'r√©sultat,', '20.000', 'guyanais.', 'urgences.', 'russo-am√©ricain.', 'r√©publicain,', 'organisateurs,', '1370', 'd√©mocratie-forces', 'codens', 'ritq', \"s'effondre\", 'mensuels.', 'vacances,', 'n√©e.', 'vicaut', 'salari√©s.', 'l√©ger,', \"d'artillerie.\", 'siffert', 'voisinage.', 'recul,', 'brut.', '558', 'lentement.', 'kharki', '72', '10h25', 'larem', 'l√®pre.', '1983', 'populaire,', 'logiciel,', 'ericka', '241', 'keppe', 'aveux.', 'usuel,', 'ville.', 'musaga', '112', '1891', '‚Ä∫', '1978', 'civile...', '(quartier', 'little,', \"s'accordant\", '0,6', 'a),', 'couru,', 'nova√Øa', '2000', 'solaires,', '1996', '221', 'dit,', '15-14,', '√©mission.', 'villardonnel', 'd√©couvrir),', 'psychologie.', 'remont√©e,', 'cons√©cutif,', '000', 'journal.', \"pessa'h\", 'politiques.', 'mathpodcast', \"s'autod√©truira.\", 'ivanovich', 'l‚Äôaccident,', 'zigzagu√©', \"l'√©vapotranspiration\", '16h18', \"l'√©picentre\", 'survie.', 'tests,', 'indaba', 'anglais)', 'piste,', 'a319', 'ind√©pendante.', 'melih', 'compatriote.', 'part.', 'contradictions.', 'pause.', 'ciel.', 'l‚Äôinstabilit√©', '311', 'nouville', 'idleb', 'boyang', '[‚Ä¶]', '2020', 'nouvelle-aquitaine', 'sant√©.', '527)...', 'long.', 'script,', 'ghazarian', 'onub', \"aujourd'hui,\", 'hexavalent,', 'concurrentes,', '[mais]', 'r√©fl√©chi.', 'septembre.', 'bockwinkel', '√©limant', 'g√©za', \"l'assaillant,\", 'maintenant.', 'fettell', 'portant,', '28,2', 'hypoth√®se.', '(ce', 'utc+3', '%),', 'pontet.', 'vienne-reventin', 'fatsah', '4,9', '153,7', \"ja'bar\", 'dessus.', 'souligne-t-il.', 'stepanova', 'stade.', 'liste,', '465', 'f√©d√©rations,', 'v√©lo.', \"d'extr√™me\", '5500', 'p√©tition,', '1968', 'histopathologiques', 'p√©tards.', 'journalistes,', \"l'emballement\", 'ville-close', 'mai,', \"s'envole\", 'gendarmes,', 'suivi,', 'savait,', 'scrutins.', \"l'interview\", 'korovine', '17h25', '14h30', '[un]', 'l√©sions.', \"d'identit√©,\", 'witkowski', \"s'allouant\", 'h√©ritiere', 'strekalin', '1re', '$us', 'haldane', 'poursuivantes,', 'guy-brice', 'crispr', 'abdella', 'affaires.', 's‚Äôagit,', \"l'indon√©sien\", '(cons√©quence', 'mais,', 'r√©approvisionn√©es', 'etc.', 'l√©gitime.\"', \"d'apparence\", 'ram√≠rez', 'enfin,', '[survenue]', 'd‚Äôune', 'juillet,', '√©v√®nements.', '(197', 'reconstruction.', \"l'enqu√™te.\", '46', 'uwamahoro', ',', 'croitre,', 'taschereau', 'cr√©ance).', '159', 's‚Äô√©lever', '√©mission,', 'net.', 'b√¢timent,', 'ravitaillement,', 'manifestante', 'visionnaire,', 'incessamment.', \"l'extr√™me-gauche\", 'harakat', 'fangeas', 'cons√©quence,', 'instagram', '-10¬∞c', 'a11', 'possession.', 'capitale),', 'laurenne', '(39%).', 'rapids', \"l'agresser\", 'arenka,', 'petite-finale,', \"l'hum√©rus\", '205', \"l'√©lection.\", 'trinita', 'gambien', '...)', 'junji', 'terminale.', 'interpellations,', 'interpellent.', \"l'explosion.\", 'r√©ellement.', \"l'astre.\", \"l'avocate\", 'mineures.', '5,5', 'normalement.', 'suppl√©mentaires,', '-63', 'kermode', 'cezais', \"s'effondrer\", 'constate,', 'financi√®res.', 'al-mansoori', \"d'immense\", 'prabhu', 'annul√©.', 'recueillis,', \"sables-d'olonne\", 'investiture.', '785', 'courber,', 'brignone', \"l'instant,\", '0', 'japonais,', 'l√©gitimes.', 'extr√™mes.', \"s'alliant\", \"l'interrogeaient\", 'maternelle,', 'mouvement.', 'cercueil.', 'trstenjak', 'canadairs', 'r√©gress√©,', 'd‚Äôaccepter', 'r√©glementaire,', 'pas.', 'retire,', 'slavinec', 'anti-√©meute.', '√©vapor√©.', 'obry', 'hisayoshi', '19h40', 'boechecher', 'sportova', 'l‚Äô√©tat.', 'citoyenne),', \"d'hugues\", 'frustr√©s,', 'vraie,', 'resserre,', 'depuis,', 'avril.', \"l'astromobile\", 'ayas', '37e', 'gratuits).', '(dieu', 'musulmans.', 'mat√©riels,', '000.', 'd‚Äôattaques', 'chemin.', 'c‚Äô√©tait', 'matin.', '8h55', 'n‚Äôavons', 'pro-gouvernementales', 'civile.', '411', '8,', 'trente-septi√®me', 'd√©grad√©es.', 'kwizera', '√©chapp√©e.', 'rats.', 'pi√©g√©e.', 'pr√©cis√©ment,', 'profondeur,', 'titre,', '29,1', 'adolescents.', '√©galement).', 'secours,', 'b√©nnage', '38√®me', 'fructueuse,', 'bakraoui', 'p√©nurie.', 'cultures.', 'wikinews', 'balistique.', '1,5', 'montpelli√©rain,', 'courrier.', \"l'√©l√©gance\", 'athl√©tisme.', '√©voqu√©e,', 'anonymes,', 'cons√©quences.', 'policiers)', 'zelmerl√∂w', 'forme,', 'tour.', '(pib),', 'voiture.', 'explos√©.', 'moir', 'claps', 'barre.', 'aussonne', 'revanche,', 'jugez-moi', 'suissesse,', 't√©l√©vis√©,', '(ou', \"s'agira,\", 'gibertine', 'association.', '491', 'mi-temps,', \"d'arriv√©e,\", 'alli√©s,', '‚Ç¨.', 'appel√©es,', 'semnan', 'coup.', 'trente.', 'membre,', '0,5', 'salaheddine,', 'ad√©quate,', 'recommand√©s).', 'peine,', 'r√©apparus.', 'constitution.', '√©coles,', 'fauchant', 'recreuser', 'arrive,', 'm)', 'annonc√©,', 'apr√®s.', 'tilikum', 'fonctionnalit√©s.', 'ch√¥mage,', 'alliance.', 'blainville-sur-orne', 'hauts-de-france', 'radioastronom√≠a', 'mari.', \"d'intimidation\", 'sindimwo', 'dimanche,', 'minutes.', \"l'√©preuve,\", 'bruxelles-midi', \"d'≈ìufs\", 'objectif.', 'gardien,', '143,72', 'saccag√©e.', '(interwiki,', 'professionnelle.', 'd‚Äôhier', 'participaient,', \"l'agresseur.\", 'jusqu‚Äôen', 'disque,', 'efimova', '0,7', '9e', \"d'euros),\", 'comesa', 'a57', 'corruptions,', 'hebdomadaires.', '1:10.62,', 'unkrich', 'no√´l.', '%,', 'adopt√©e).', '(2/3)', '(18', 'd√©blay√©e,', 'contribution,', 'qu‚Äôen', '√©conomies.', 'd√©ploy√©s,', \"s'amplifier\", \"d'incertitudes\", 'contre,', 'ann√©es\"', \"l'ong\", \"d'euros).\", 'l‚Äôincin√©rateur', 'faveur.', 'soci√©t√©.', '(d√©gagement', '2017', '12,3', 's‚Äôentretient', 'shuicheng', 'astronomique,', '1,3', 'lors,', 'saison.', 'cr√©anciers.', 'd√©put√©,', '√©corne', \"qu'estim√©\", \"l'industrie,\", 'amaq', 'maragua', 'eau.', 'bacille.', '19e', 'observ√©,', 's‚Äôagisse', 'centrale.', 'tiangong-2', 'divorc√©s.', 'voyageur.', 'fonctions.', '(¬´', '43', 'blummenfelt', 'stade,', 'bahre√Øni', 'famille.', 'skieurs,', 'drone.', 'moellhausen', 'kilom√®tre,', 'laquelle,', \"l'appel.\", 'tangente.', 'assaillants,', 'base,', 'cons√©cutives.', 'd.', '2013', 'bancaire,', 'saisi.', 'bourse)', '√©missions,', 'pallant', 'sauraient,', 'kentaro', 'bouteilles.', 'combin√©).', 'boule-d‚Äôor', 'mensuelle,', 'espagnole,', 'contre-attaques.', 'percut√©s', '6', 'humaines.', 'fils.', 'portable.', 'culpabilit√©,', '(deux', '49.3,', \"l'acheminer\", 'blanche.', 'dr√¥mois.', 'nyassaland', 'stable,', 'd√©couvert,', 'marier.', \"d'opposition.\", 'n√©gociations,', 'l‚Äôidentit√©', '√©vacuation.', 'terrible.', \"d'abrogation\", '%', 'gendarmerie.', 'bras.', 'd√©signer,', 'r√©duit,', 'a29', '!', 'd√©lits,', \"l'atmosph√®re.\", 'moniteur,', 'morozov', 'minoritaire,', \"l'entourloupe\", 'fran√ßaise,', 'd√©tail,', '1098', \"l'organisation,\", 'l‚Äôopposition.', 'abedini', 'marxiste.', 'destructions,', 'amsterdam-schiphol', 'nuls,', '200.000', '1963', 'non-gouvernementale', 'jaeghere', 'pourparlers.', '13h50', 'ryabova', 'orages.', 'lui,', 'r√©el,', '7,65', 'olympique.', '7h540', 'ving-quatre', 'constat√©es.', 'vandalis√©es', 'louanges.', 'rien,', 'terroristes.', '60', '√©pais.', '1959', '13h40', '(163', \"l'√©quipage.\", 'm-4', 'bon,', 'hivernales.', 'villeminot', '6,5', 'carnage.', 't√©l√©vis√©e,', 'kihara', 'rajouter.', '700,', 'd‚Äôindemnit√©s', 'braun-pivet', \"l'¬´\", 'v√©lo,', \"d'anesth√©siques.\", 'plaine.', 'szil√°gyi', '(et', 'maghr√©bine,', 'bless√©e,', 'chant√©.', 'dellow', '23h0', 'p√©tition.', 'cizeron', \"d'intervalles\", '11h40', 'contre-offensives', 'sinjar', '39', 'archidioc√®ses', \"s'entourer\", 'primates.', 'charayron', 'dicker', 'secteur,', 'battante,', 'd‚Äô√©change', 'qu‚Äôon', 'fum√©e.', \"l'orient-le\", 'toutefois,', 'gratuitement.', 'info)', 'd√©licat,', 'roman.', 'accident,', 'l‚Äôex√©cutif', '89', '12h20', \"s'isoler\", \"d'√©tat.\", \"d'intervalle\", '√©cologique.', 'r√©solution.', 'camions-bennes', 'l‚Äôheure,', 'chaos,', 'gouvernementales.', '53,5', 'pr√©sidentiels.', '√©chec,', \"l'hebdomadaire\", '48', 'foule.', 'budget,', 'vasiƒºjevs', \"l'instruction.\", 'nucl√©aire)', 'surtout,', 'flammes,', '1990', 'totale.', 'exceptionnelle.', 'info,', 'postes,', 'ptwc', 'comp√©tente.', 'impliqu√©es,', 'monbalen', '16h10', '116', 'sa√¥ne-et-loir', 'routiers,', \"l'esclavagisme\", '22h25', 'acpm', 'int√©r√™ts,', 'rebondissements.', 'exfiltrer', 'qu‚Äôune', 'ndayishimiye', 'meureudu', 'massialas', 'lamastra', 'maheu', 'minuit.', 'ordino-arcalis', '(fausse)', 'retrouv√©.', '30%.', 'technologies.', 'vbrg', '85', 'avanzi', 'd‚Äô√©chapper', 'musculo-squelettique).', \"d'avalanche.\", 'taxi.', 'centristes.', 'fantastique,', 'pays.', 'd√©sorbitage', 'mat√©riaux,', 'courriel,', 'convoqu√©,', '(4,1', 'yahoo!', '(etc,', \"l'attendait\", '211', 'primaires,', 'jong-nam', 'combat,', 'p√®lerins.', 'hashtag', '2f', '?', 'pt2', 'courantes.', 'objet,', 'emploi,', 'carambolage,', '4c,', \"d'√©vasion\", 'mecafor', 'fur√©', 'grivko', 'conformit√©.', \"l'arc-en-ciel\", 'n√©o-z√©landaise', 'champignon.', 'l‚Äôanthropologue', \"qal'at\", 'test√©s.', '‚Äì', 'd√©rout√©e', '(droite', 'jeunes.', 'rohingya.', 'sang,', 'humaine.', 'bless√©s,', 'perrichon', \"s'aggrave,\", 'mardi,', 'concr√®tement,', 'a.', 'radicalisation,', 'd√©nud√©s,', 'l‚Äôart', 'pr√©sents,', \"d'exfiltrer\", 'amincissent', 'solide,', 'anti-criminalit√©', 'kilom√®tres,', '3', 'moniteur.', 'a√©riennes,', \"d'annoncer\", \"clancy's\", 'incendi√©e,', '√©tage,', 'jeux),', '(imp√¥t', 'dev√®s', \"l'agresseur\", 'plan√©taires.', \"l'isf\", 'titre.', 'sinahagera', 'europ√©ens,', 'recherches.', 'publiques,', 'emaswati', 'l‚Äôh√¥tel', 'd√©clin,', 'autorit√©s,', '900', 'mkapa', 'fauves.', \"l'entente\", 'sainte-julie', 'avro', 'basidiomyc√®te', 'essence.', 'italiens.', 'avril,', \"l'am√©ricano-britannique\", 'd√©cret,', 'faire.', 'galliamov', 't√©lescope.', 'privatis√©e.', 'tib√©tains.', '1997', 'incendie.', \"s'affronter\", 'inscrits,', 'bons.', 'l‚Äôapaisement,', 'mobilis√©s.', 'fran√ßais.', '5000', 'inattendue,', 'explosions,', 'battante.', 'bihlam', \"l'entresol\", \"l'a√©roport,\", 'oberstdorf', 'militants.', 'm√®tres.', \"l'√©olien,\", 'chalan√ßon', 'ma√Ætris√©,', '√©choue,', \"l'express\", \"l'√©charpe\", '52', 'telle,', 'rapidement.', 'out\".', 'l‚Äôh√¥pital', 'marginalis√©s,', '¬ª).', 'formations,', 'calcin√©s.', \"l'√©mirat\", '234', 'james-webb', 'gazeux)', '(wts)', 'fuit√©', 'musculation,', 'lorcey', '2,8', 'garnavich', \"s'affichait\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DeQaqt-pSeu"
      },
      "source": [
        "#### VOTRE TRAVAIL\n",
        "* Le mod√®le est construit sur les mots du corpus d'entra√Ænement. Est-ce que le vocabulaire du corpus d'entra√Ænement est pr√©sent dans le mod√®le d'embeddings ? Est-ce que le vocabulaire du corpus de test est pr√©sent dans le mod√®le d'embeddings ? Les corpus d'entra√Ænement et de test partagent-ils le m√™me vocabulaire ? \n",
        "* Le vocabulaire partag√© est-il une piste explicative √† la qualit√© des performances obtenus avec les mod√®les d'embeddings pr√©-entra√Æn√©s ?\n",
        "* L'impl√©mentation courante \"fine tune\" aussi les embeddings. Si vous ne souhaitez pas que les gradients soient calcul√©s pour le tensor des embeddings il faut lui sp√©cifier sa propri√©t√© `.requires_grad=False` (par d√©faut √† `True`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp√©rimentation d'autres architectures\n",
        "...\n",
        "\n",
        "Suivant votre avancement,\n",
        "- d'autres word embeddings peuvent √™tre test√©s (e.g. glove), \n",
        "- une autre architecture [BERT-CRF](https://github.com/jidasheng/bi-lstm-crf) (cf. fin du README)...\n",
        "- ajouter les traits sur la surface des mots..\n",
        "\n"
      ],
      "metadata": {
        "id": "DUzB6zaU7thk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "M2-ATAL-2021-22_02_NER with BiLSTM-CRF.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXU0sIzYshYxd0g/Y+k+3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}