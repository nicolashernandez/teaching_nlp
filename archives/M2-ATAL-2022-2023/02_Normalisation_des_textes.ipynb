{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYLcjEsnL7wAE21Cr2l/PT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/02_Normalisation_des_textes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmNr9PDicuOn"
      },
      "source": [
        "# Normalisation des textes\n",
        "\n",
        "La normalisation d'un texte a pour objectif de diminuer le \"bruit\" et √† amplifier ce que l'on cherche √† √©couter. Par bruit, on entend les variations linguistiques qui sont rares ou non n√©cessaires √† consid√©rer pour la t√¢che que l'on souhaite r√©aliser.\n",
        "\n",
        "La **normalisation** d'un texte consiste donc √† \n",
        "- supprimer ce qui est rare ou non porteur de sens et qui apporte du bruit, \n",
        "- mettre en avant ce qui est porteur de sens en r√©duisant/unifiant les variantes.\n",
        "\n",
        "En pratique cela signifie : supprimer les balises  (e.g. _`<div>`_->_‚àÖ_), remplacer les variantes d'un mot par une forme r√©f√©rente par expansion syst√©matique des contractions (e.g. _l'_->_le_) ou par lemmatisation ou stemming (e.g. _la_->_le_), supprimer les caract√®res qui ne sont pas lettres, supprimer les mots vides (e.g. _le_->_‚àÖ_), substituer toutes les occurrences d'hashtags par un unique token `#hashtag`, √©galement pour les urls...\n",
        "\n",
        "Suivant la t√¢che, on peut s'int√©roger si la normalisation doit aussi **corriger** les mots mal orthographi√©s ou les r√©p√©titions de caract√®res qu'ils soient alphab√©tiques ou autres. En effet la r√©p√©tition d'une m√™me lettre ou d'un caract√®re de ponctuation peut marquer l'emphase dans un propos e.g. \"suuuuuuuuuuuper !!!!!\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUwkUR5ScfiP"
      },
      "source": [
        "## Installation de l'environnement : chargement des mod√®les et des donn√©es\n",
        "\n",
        "Nous utiliserons un corpus de romans de Jules Verne de la litt√©rature fran√ßaise. En pratique le corpus est un r√©pertoire, et les fichiers sont des oeuvres distinctes de Jules Verne. Ex√©cuter le code suivant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBuM_wSby6-5"
      },
      "source": [
        "# get the corpus and install in local\n",
        "!mkdir data\n",
        "!wget -nc https://raw.githubusercontent.com/nicolashernandez/teaching_nlp/main/data/JulesVerne.zip -P data\n",
        "!unzip data/JulesVerne.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_1T3AtFqa2b"
      },
      "source": [
        "Dans le cadre de ce travail nous utiliserons un seul roman (fichier) √† savoir _20000 Lieues sous les mers_. Le code suivant permet de v√©rifier le contenu du fichier ainsi que de d√©finir des variables qui seront utilis√©es par la suite pour illustrer le fonctionnement des diff√©rents codes fournis : `novel_name` et `document`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTQGFlaRsIAG"
      },
      "source": [
        "# have a look at a sample of one particular file\n",
        "novel_path = \"data/JulesVerne/Jules Verne_20000 Lieues sous les mers.txt\"\n",
        "\n",
        "import regex as re\n",
        "novel_name = re.search(\"_(.+)\\.txt\", novel_path).group(1)\n",
        "print (novel_name, novel_path)\n",
        "\n",
        "with open(novel_path, encoding='utf8') as f: \n",
        "  document = f.read()\n",
        "  print (document[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVEwgdpkH6ik"
      },
      "source": [
        "## Mots vides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEHQKPXT3GWW"
      },
      "source": [
        "### QUESTION \n",
        "\n",
        "* Executer le code ci-dessous pour v√©rifier que la liste de mots vides de nltk pour le fran√ßais contient bien que des mots vides. Y-a-t-il des mots \"non porteur de sens\" que vous auriez pens√© retrouver ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwl0TDi89v_b"
      },
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "# retrieve a list of french stop words¬†\n",
        "fr_stop_words = nltk.corpus.stopwords.words('french')\n",
        "print ('fr_stop_words:',fr_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6AgN8TFWDi"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XAZiKmWIAdw"
      },
      "source": [
        "##¬†Op√©rations de normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLXCj8w8iypQ"
      },
      "source": [
        "###¬†QUESTION \n",
        "\n",
        "La m√©thode ci-dessous pr√©traite un document (une cha√Æne de caract√®re) en r√©alisant un certain nombre d'op√©rations de normalisation. Replacer les commentaires suivant au bon endroit dans le code.\n",
        "\n",
        "```\n",
        "    # filter tokens with small length\n",
        "    # tokenize document\n",
        "    # filter stopwords out of document\n",
        "    # substitute all characters which are not a whitespace or a letter by a whitespace\n",
        "    #¬†to lower case\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvHJ8ehzbvtk"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfRO8m_0dCqB"
      },
      "source": [
        "import nltk\n",
        "\n",
        "#¬†the native re module does not handle \\p{} unicode property \n",
        "import regex as re\n",
        "\n",
        "# define a tokenizer which tokenizes a text by splitting at each whitespace character\n",
        "ws_tokenizer = nltk.WhitespaceTokenizer()\n",
        "\n",
        "# normalize a string of text\n",
        "def normalize_characters(doc):\n",
        "    # TODO \n",
        "    doc = re.sub('[^\\p{L}\\s]', ' ', doc)\n",
        "\n",
        "    #¬†TODO\n",
        "    doc = doc.lower()\n",
        "  \n",
        "    # TODO \n",
        "    return ws_tokenizer.tokenize(doc)\n",
        "\n",
        "#¬†normalize tokens\n",
        "def normalize_tokens(tokens):\n",
        "    # TODO \n",
        "    tokens = [token for token in tokens if token not in fr_stop_words]\n",
        "\n",
        "    # TODO \n",
        "    tokens = [token for token in tokens if len(token) >2]\n",
        "\n",
        "    # re-create document from filtered tokens\n",
        "    #doc = ' '.join(tokens)\n",
        "    #return doc\n",
        "    return tokens\n",
        "\n",
        "# all in one\n",
        "def preprocess_document(document):\n",
        "  return normalize_tokens(normalize_characters(document))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO7sdJ2zAtcf"
      },
      "source": [
        "# le code suivant permet d'illustrer le fonctionnement de la normalisation\n",
        "sample_document = \"Tu pourras (c'est ouf gr!) m√™me go√ªter au gin de Marina,et mon rhum arrang√©!!! üòúü§ùüè©üè£üëç\"\n",
        "sample_preprocessed_document = normalize_tokens(normalize_characters(sample_document))\n",
        "print ('document:', sample_document)\n",
        "print ('preprocessed_document:', sample_preprocessed_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDNwUxkZIMUr"
      },
      "source": [
        "##¬†Taille du texte vs. taille de vocabulaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1bfSjvwHRU"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code suivant pr√©traite le document sans et avec filtrage de tokens (notamment les mots vides). Pour chaque r√©sultat de pr√©traitement, le nombre total de tokens et la taille du vocabulaire sont affich√©s.\n",
        "\n",
        "* Comparer les tailles de vocabulaire et le nombre total de tokens avant/apr√®s filtrage des mots vides de sens. Que traduisent ces chiffres ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiVRLLCVstAI"
      },
      "source": [
        "# suppression de certains caract√®res, passage de tous les caract√®res en minuscule, tokenization\n",
        "normalized_characters = normalize_characters(document)\n",
        "print ('Before filtering - number of tokens:', len(normalized_characters), '; vocabulary size:', len(set(normalized_characters)))\n",
        "\n",
        "#¬†suppression de certains tokens\n",
        "normalized_tokens = normalize_tokens(normalized_characters)\n",
        "print ('After filtering - number of tokens:', len(normalized_tokens), '; vocabulary size:', len(set(normalized_tokens)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rrCtcGiw4pY"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjvQz0ILcZhY"
      },
      "source": [
        "## V√©rifier si la loi de Zipf s'applique.\n",
        "\n",
        "La loi de zipf :\n",
        "\n",
        "> En classant les mots d‚Äôun texte par fr√©quence d√©croissante, on observe que la fr√©quence, `freq(m)`, d‚Äôun mot `m` dans un\n",
        "texte est corr√©l√©e √† son rang `rang(m)` par une\n",
        "loi du type : `freq(m) * rang(m) = K` avec `K`\n",
        "une constante. \n",
        "Peu de mots fr√©quents et grand nombre de\n",
        "mots rares.\n",
        "\n",
        "La _fr√©quence d'un mot_ est son _nombre d'occurrence_ sur la somme totale des occurrences de tous les mots. La fr√©quence constitue une forme de normalisation num√©rique. Une tendance observ√©e avec le nombre d'occurrences \"devrait\" √™tre la m√™me que celle observ√©e avec une fr√©quence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3k7KxBdgbsY"
      },
      "source": [
        "### QUESTION : mots vides vs. mots porteurs de sens\n",
        "\n",
        "Le code ci-dessous traite les mots avant filtrage des mots vides. Il utilise la m√©thode `FreqDist` d'un paquet de nltk pour calculer la fr√©quence des tokens du corpus d'un des auteurs, puis affiche les `n` tokens les plus _fr√©quents_ ; on dit aussi les plus _communs_ (`most_common`). \n",
        "\n",
        "* Ex√©cutez-le. Est-ce bien la fr√©quence qui est observ√©e ?\n",
        "* Faire varier le nombre de mots les plus fr√©quents √† observer en jouant avec la variable `most_common_n`. Essayer des valeurs de 10 √† 1000.  Quand vous regardez les extr√™mes (les plus fr√©quents vs. les moins fr√©quents des plus fr√©quents...), que pouvez-vous dire sur la capacit√© des mots √† d√©crire un contenu (√† donner du sens) ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_LjYwd-fTqO"
      },
      "source": [
        "# import required libraries \n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# find the frequency \n",
        "fdist = FreqDist(normalized_characters)\n",
        "\n",
        "most_common_n = 10\n",
        "\n",
        "# print the n most common \n",
        "print ('Les', most_common_n, 'mots les plus fr√©quents sont :', fdist.most_common(most_common_n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkK6-8i9j_mh"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNUA8q03wJd6"
      },
      "source": [
        "### QUESTION : observation de la normalisation des mots les plus communs\n",
        "\n",
        "Le code suivant trace une courbe de la \"fr√©quence\" des mots ordonn√©e sur le rang de la fr√©quence.\n",
        "\n",
        "* Dans le code ci-dessous, rajoutez quelques lignes pour visualiser \"aussi\" la \"fr√©quence\" des mots apr√®s normalisation des tokens (i.e. apr√®s application de la m√©thode `normalize_tokens`). Trouvez-vous que la m√©thode `normalize_tokens` a bien fait son travail ?\n",
        "* Comment caracterisez-vous la 2nd courbe par rapport √† la 1√®re ?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx2dLH333xlw"
      },
      "source": [
        "# plot the graph of frequency for n most common  :\n",
        "import matplotlib.pyplot as plt\n",
        "most_common_n = 40\n",
        "\n",
        "fdist = FreqDist(normalized_characters)\n",
        "print ('Les', most_common_n, 'mots les plus fr√©quents sont :', fdist.most_common(most_common_n))\n",
        "fdist.plot(most_common_n)\n",
        "\n",
        "fdist = FreqDist(normalized_tokens)\n",
        "print ('Les', most_common_n, 'mots les plus fr√©quents sont :', fdist.most_common(most_common_n))\n",
        "fdist.plot(most_common_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRJdRmv-z3LG"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOZOjGrcb28g"
      },
      "source": [
        "### QUESTION : observation de la fr√©quence √† grande √©chelle\n",
        "\n",
        "Le code ci-dessous fait globalement la m√™me chose que ci-dessus mais il offre davantage de contr√¥le sur le calcul de la fr√©quence. Cela permet aussi de jouer plus finement avec le tra√ßage de la courbe (qui ici permet de consid√©rer tous les mots et non seulement les plus communs). \n",
        "\n",
        "* Executez le code et visualizez la courbe. Eventuellement commenter/d√©commenter les lignes qui vont bien pour passer √† une √©chelle logarithmique et plus ais√©ment observer des valeurs tr√®s petites. Est-ce que la loi de Zipf s'applique sur ce document ? \n",
        "* Remarquez que le code vous permez aussi d'observer la courbe avec le nombre (ou compte) d'occurrences plut√¥t que la fr√©quence..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbLplm15V_4T"
      },
      "source": [
        "from collections import Counter\n",
        "# We use reduce to concatenate all the lists in tokenized_author_data, but we don't use \"set\" \n",
        "# so that we can count occurencies with a Counter object \n",
        "# Count the number of occurrences of each token\n",
        "occurrences_counter = Counter(normalized_characters)\n",
        "\n",
        "# Formating the counter object to a proper dataset\n",
        "import pandas as pd\n",
        "d = pd.DataFrame(occurrences_counter, index=['occurrences']).transpose().reset_index()\n",
        "d.columns=['word', 'occurrences']\n",
        "\n",
        "# Computing frequencies instead of occurrences\n",
        "nb_total = d.occurrences.sum()\n",
        "d['freq'] = d.occurrences.apply(lambda x: x/nb_total)\n",
        "# d['freq'] = d.occurences.apply(lambda x: x) #¬†<- nombre d'occurrences\n",
        "\n",
        "# Sorting by frequency, most frequent word at the top of the df\n",
        "d = d.sort_values('freq', ascending=False)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.grid()\n",
        "plt.xscale('log')  # Using log scale\n",
        "plt.yscale('log')  # Using log scale\n",
        "plt.xlabel('log(rank)')\n",
        "plt.ylabel('log(frequency)')\n",
        "#plt.xlabel('rank')\n",
        "#plt.ylabel('frequency')\n",
        "#plt.ylabel('count')\n",
        "plt.title(\"Zipf law : {0}\".format(novel_name))\n",
        "x = list(range(d.shape[0]))\n",
        "plt.plot(x, d.freq)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax9WJYAOboev"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    }
  ]
}