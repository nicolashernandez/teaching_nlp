{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRBLDGIoK6zRadT/7vkXTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2-CN-2021-22_03_repr%C3%A9sentation_des_textes_sac_de_mots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmNr9PDicuOn"
      },
      "source": [
        "# Modélisation du contenu des textes - approches traditionnelles\n",
        "\n",
        "Représentation du sens au niveau des mots, et des documents - approche naive à la Salton et Spark Jones\n",
        "\n",
        "\n",
        "## Modèle \"sac de mots\"\n",
        "\n",
        "Le [**sac de mots** ou *BOW* pour *Bag Of Words* en anglais](https://fr.wikipedia.org/wiki/Sac_de_mots) est un modèle classique utilisé en Recherche d'Information (RI) pour représenter le contenu d'un document. Chaque document est décrit vis-à-vis d'un vocabulaire donné commun. \n",
        "\n",
        "Différentes vues sont possibles : \n",
        "- compter binairement si les mots du vocabulaire sont présents dans le document, \n",
        "- compter les occurrences des mots du vocabulaire dans le document, \n",
        "- pondérer les mots en tenant compte de leurs spécificités dans le document vis-à-vis des autres documents (on parle de pondération _tf-idf_).\n",
        "\n",
        "La **vectorisation** est le processus qui désigne transformation des textes en vecteurs (de mots selon une modélisation *bow*).\n",
        "\n",
        "On doit à [Karen Spärck Jones](https://fr.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones) la proposition de la pondération _tf-idf_ des termes.\n",
        "> « La spécificité d'un terme peut être quantifiée comme une fonction inverse du nombre de documents dans lesquels il apparaît. » \n",
        "\n",
        "[Gérard Salton](https://fr.wikipedia.org/wiki/Gerard_Salton), quant à lui, est reconnu comme étant le père de la recherche d'information en ayant proposé une modélisation des documents dans un espace vectoriel. \n",
        "\n",
        "\n",
        "* Karen Spärck Jones, « A statistical interpretation of term specificity and its application in retrieval », Journal of Documentation, vol. 28, no 1,‎ 1972, p. 11–21 (DOI 10.1108/eb026526)\n",
        "* G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing, Communications of the ACM, v.18 n.11, p. 613-620, novembre 1975\n",
        "\n",
        "Par la suite nous utiliserons le module python sklearn qui offre des facilités pour pré-traiter (normaliser) et vectoriser aisément les textes, puis de réaliser des opérations de calcul de similarités inter-documents ou de clustering au sein d'un ensemble de documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPtnCbGo1-IZ"
      },
      "source": [
        "## Préparation d'un jeu de données \"jouet\"\n",
        "\n",
        "Le code suivant définit un corpus de **huit documents**, chacun pré-classé dans une catégorie thématique. Lisez les phrases et constatez qu'il y a **trois catégories** thématiques qui émergent de ce corpus. Les documents sont ici simplifiées à une seule phrase. On peut aussi constater que les documents ne sont pas normalisés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GECHxKx82PiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b8c331d9-b0da-4d7d-a595-f3ea2e14d85a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200\n",
        "import numpy as np\n",
        "\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Document Category\n",
              "0                                      The sky is blue and beautiful.  weather\n",
              "1                                   Love this blue and beautiful sky!  weather\n",
              "2                        The quick brown fox jumps over the lazy dog.  animals\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
              "4                         I love green eggs, ham, sausages and bacon!     food\n",
              "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
              "6            The sky is very blue and the sky is very beautiful today  weather\n",
              "7                         The dog is lazy but the brown fox is quick!  animals"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBUAsxjCKEe9"
      },
      "source": [
        "## Vectorisation avec occurrences\n",
        "\n",
        "Le code suivant réalise une vectorisation du corpus en comptant les occurrences des mots. La vectorisation prend en charge la construction d'un vocabulaire sur le corpus ainsi que quelques pré-traitements de normalisation linguistiques. Enfin le résultat de la vectorisation est affiché sous la forme d'une matrice **document-terme** (les documents sont en ligne et les mots en colonne). \n",
        "Nous aurons une matrice de dimension _nb de documents * taille du vocabulaire_.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5faoVA63PSE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "72be09e4-d336-455a-e409-ceff3fe70c95"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# define the vectorizer\n",
        "c_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# display the configuration of the vectorizer\n",
        "print (c_vectorizer)\n",
        "\n",
        "# perform the vectorization\n",
        "c_matrix = c_vectorizer.fit_transform(corpus)\n",
        "print ('Matrix dimensions:', c_matrix.get_shape())\n",
        "\n",
        "# get all unique words in the corpus (the vocabulary and also the names of the matrix columns/features)\n",
        "vocab = c_vectorizer.get_feature_names()\n",
        "print ('Vocabulary size:', len(vocab))\n",
        "\n",
        "# show document-term matrix\n",
        "c_matrix = c_matrix.toarray()\n",
        "pd.DataFrame(c_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(stop_words='english')\n",
            "Matrix dimensions: (8, 20)\n",
            "Vocabulary size: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bacon</th>\n",
              "      <th>beans</th>\n",
              "      <th>beautiful</th>\n",
              "      <th>blue</th>\n",
              "      <th>breakfast</th>\n",
              "      <th>brown</th>\n",
              "      <th>dog</th>\n",
              "      <th>eggs</th>\n",
              "      <th>fox</th>\n",
              "      <th>green</th>\n",
              "      <th>ham</th>\n",
              "      <th>jumps</th>\n",
              "      <th>king</th>\n",
              "      <th>lazy</th>\n",
              "      <th>love</th>\n",
              "      <th>quick</th>\n",
              "      <th>sausages</th>\n",
              "      <th>sky</th>\n",
              "      <th>toast</th>\n",
              "      <th>today</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bacon  beans  beautiful  blue  breakfast  ...  quick  sausages  sky  toast  today\n",
              "0      0      0          1     1          0  ...      0         0    1      0      0\n",
              "1      0      0          1     1          0  ...      0         0    1      0      0\n",
              "2      0      0          0     0          0  ...      1         0    0      0      0\n",
              "3      1      1          0     0          1  ...      0         1    0      1      0\n",
              "4      1      0          0     0          0  ...      0         1    0      0      0\n",
              "5      0      0          0     1          0  ...      1         0    0      0      0\n",
              "6      0      0          1     1          0  ...      0         0    2      0      1\n",
              "7      0      0          0     0          0  ...      1         0    0      0      0\n",
              "\n",
              "[8 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7aCCq_A6wr-"
      },
      "source": [
        "### QUESTION : configuration du vectorizer\n",
        "\n",
        "Pour vous aider dans les questions suivantes, consulter l'affichage du paramétrage du `CountVectorizer` (via la ligne `print (vectorizer)`). Consulter aussi la documentation https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "Si besoin, vous pouvez aussi consulter une aide sur les expressions régulières en dessous des questions.\n",
        "\n",
        "Répondez aux questions suivantes :\n",
        "* Quels prétraitements de normalisation sont opérées (mentionner les paramètres impliqués) ? \n",
        "* D'autres pré-traitement de normalisation sont-ils possibles même s'ils ne sont pas utilisées ici (mentionner les paramètres impliqués) ? \n",
        "* Comment expliquez-vous que les mots du vocabulaire ne comptent de marques de ponctuation accolées ? \n",
        "* Le paramètre `max_df` permet de fixer une fréquence documentaire maximale pour les mots à considérer dans le vocabulaire. Expliquez pourquoi les valeurs `1` et `1.0` n'ont pas la même signification.\n",
        "* Comment paramétrer le `CountVectorizer` pour qu'il binarise les valeurs au lieu de compter les nombres d'occurrences ?\n",
        "\n",
        "Explicitation de quelques séquences spéciales de caractères dans les expressions régulières :   \n",
        "* `(?u)` switches on the re.U (re.UNICODE) flag for this expression https://docs.python.org/2/library/re.html#re.U. Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn’t allowed for bytes).\n",
        "* `\\w` represents a word character. For Unicode (str) patterns: \n",
        "matches characters considered alphanumeric in the ASCII character set; this is equivalent to `[a-zA-Z0-9_]`. For 8-bit (bytes) patterns: Matches Unicode word characters; this includes most characters that can be part of a word in any language, as well as numbers and the underscore. https://docs.python.org/3/library/re.html\n",
        "* `\\b` represents a word boundary between a word character and a non-word character. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDEKVUL75aw1"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL5ftk5iK9Ag"
      },
      "source": [
        "### QUESTION : matrice creuse ou matrice dense\n",
        "\n",
        "\n",
        "Une [**matrice creuse**, ou _sparse matrix_ en anglais](https://fr.wikipedia.org/wiki/Matrice_creuse), est une matrice qui contient beaucoup de valeurs nulles. A l'inverse on parle de **matrice pleine** (ou _dense matrix_). \n",
        "\n",
        "* Selon vous, la matrice obtenue après vectorisation est-elle creuse ou pleine ? Pour vous aider dans votre réponse, diriez-vous que la majorité des mots du vocabulaire se produit dans chaque document ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_mmjqxN2gr"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIuWsNULPULG"
      },
      "source": [
        "## Vectorisation avec TF-IDF\n",
        "\n",
        "La vectorisation avec occurrences présente des limites lorsqu'elle est utilisée sur de large corpus. En effet, le modèle présume que l'importance des mots est fonction de sa fréquence et qu'un mot plus fréquent qu'un autre dans un document est plus discriminant que l'autre. Le problème intervient quand un mot fréquent, supposé important, apparaît dans plusieurs documents. Le fait qu'il apparaisse dans plusieurs documents peut au final le rendre moins discriminant que d'autres pourtant moins fréquents. Le problème vient du fait que l'on prenne des valeurs absolues. \n",
        "\n",
        "Le modèle **TF-IDF** vise à solutionner ce problème en normalisant le compte des occurrences. TF-IDF correspond à _Term Frequency-Inverse Document Frequency_. \n",
        "\n",
        "On définit le TF-IDF comme suit: `tfidf = tf x idf`\n",
        "* `tfidf(w, D)` est le score TF-IDF du mot `w` dans le document `D`\n",
        "* `tf(w, D)` représente le nombre d'occurrence du terme `w` dans le document `D` \n",
        "* `idf(w, D)` représente la fréquence inverse documentaire du terme `w`, qui peut être calculée comme le log du nombre total de documents dans le corpus `C` divisé par la fréquence documentaire du terme `w` (i.e. le nombre de documents du corpus `C` dans lequel le terme `w` se produit).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKloRqeX_RN"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "La vectorisation TF-IDF est prise en charge par la classe `sklearn.feature_extraction.text.TfidfVectorizer` décrite dans la documentation :\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "* Complétez la cellule de code ci-dessous pour opérer une vectorisation de type TF-IDF, et afficher le résultat. La ligne de code suivante permet d'arrondir des valeurs réelles à deux chiffres après la virgule `matrix = np.round(matrix, 2)`, ce qui facilite la visualisation.\n",
        "* Après avoir consulté la documentation, est-il possible d'implémenter des variantes au calcul du TD-IDF ? \n",
        "* Par défaut les modèles sac de mots ne capturent pas l'ordre des mots. Que cela soit pour la classe `CountVectorizer` ou `TfidfVectorizer`, quel paramètre offre un moyen de considérer l'ordre des mots dans une modélisation ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ5IFFd_WG2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "dc5ffb06-12c0-4b2b-a8d3-8f4b8c539763"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TODO\n",
        "# tfidf_vectorizer = \n",
        "# tfidf_matrix = \n",
        "# ...\n",
        "\n",
        "# define the vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# display the configuration of the vectorizer\n",
        "print (tfidf_vectorizer)\n",
        "\n",
        "# perform the vectorization\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "print ('Matrix dimensions:', tfidf_matrix.get_shape())\n",
        "\n",
        "# get all unique words in the corpus (the vocabulary and also the names of the matrix columns/features)\n",
        "vocab = tfidf_vectorizer.get_feature_names()\n",
        "print ('Vocabulary size:', len(vocab))\n",
        "\n",
        "# show document-term matrix\n",
        "tfidf_matrix = tfidf_matrix.toarray()\n",
        "pd.DataFrame(tfidf_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer(stop_words='english')\n",
            "Matrix dimensions: (8, 20)\n",
            "Vocabulary size: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bacon</th>\n",
              "      <th>beans</th>\n",
              "      <th>beautiful</th>\n",
              "      <th>blue</th>\n",
              "      <th>breakfast</th>\n",
              "      <th>brown</th>\n",
              "      <th>dog</th>\n",
              "      <th>eggs</th>\n",
              "      <th>fox</th>\n",
              "      <th>green</th>\n",
              "      <th>ham</th>\n",
              "      <th>jumps</th>\n",
              "      <th>king</th>\n",
              "      <th>lazy</th>\n",
              "      <th>love</th>\n",
              "      <th>quick</th>\n",
              "      <th>sausages</th>\n",
              "      <th>sky</th>\n",
              "      <th>toast</th>\n",
              "      <th>today</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600978</td>\n",
              "      <td>0.526925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493162</td>\n",
              "      <td>0.432394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.571505</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493162</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380362</td>\n",
              "      <td>0.380362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.525949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.321164</td>\n",
              "      <td>0.383215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383215</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.394554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.394554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.470784</td>\n",
              "      <td>0.394554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.394554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.394554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365048</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.416351</td>\n",
              "      <td>0.416351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.416351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.416351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.416351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.360826</td>\n",
              "      <td>0.316365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.721652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.498935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      bacon     beans  beautiful  ...       sky     toast     today\n",
              "0  0.000000  0.000000   0.600978  ...  0.600978  0.000000  0.000000\n",
              "1  0.000000  0.000000   0.493162  ...  0.493162  0.000000  0.000000\n",
              "2  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "3  0.321164  0.383215   0.000000  ...  0.000000  0.383215  0.000000\n",
              "4  0.394554  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "5  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "6  0.000000  0.000000   0.360826  ...  0.721652  0.000000  0.498935\n",
              "7  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[8 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tXwnnSMYiZn"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Sl28YhUf3I"
      },
      "source": [
        "## Partitionnement sur la base d'une représentation bow des documents avec la méthode des k-moyennes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLADoE6qXRIw"
      },
      "source": [
        "Le partitionnement en [**k-moyennes** (ou _k-means_ en anglais)](https://fr.wikipedia.org/wiki/K-moyennes) est une des méthodes de partitionnement les plus populaires. \n",
        "> Étant donnés des points et un entier k, le problème est de diviser les points en k groupes, souvent appelés clusters, de façon à minimiser une certaine fonction. On considère la distance d'un point à la moyenne des points de son cluster ; la fonction à minimiser est la somme des carrés de ces distances. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8NSC9WXkVK"
      },
      "source": [
        "Ci-dessus, les processus de vectorisation vous ont conduit à avoir 2 matrices document-terms : \n",
        "- dans la première, `c_matrix`, les termes sont représentés par leur nombre d'occurrences\n",
        "- dans la seconde, `tfidf_matrix`, par un score tf-idf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fu0kUpgYkly"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "Ci-dessous un code qui réalise un clustering kmeans sur une matrice document-term donnée en entrée, à savoir la matrice `c_matrix`.\n",
        "Le nombre de catégories étant connu (3), il est spécifié en paramètre (`n_clusters=3`).\n",
        "\n",
        "* Dupliquez le code et modifiez le pour considérer comme input,`dt_matrix`, au clustering kmeans, la matrice document-term `tfidf_matrix`. Pour chacune des deux représentations (`c_matrix` et `tfidf_matrix`), en regardant le `ClusterLabel` produit, pouvez-dire que le clustering produit est correct ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "LeFj5jXaUeDu",
        "outputId": "7138ef65-a425-4daf-cda4-9fada7effac4"
      },
      "source": [
        "dt_matrix = c_matrix \n",
        "\n",
        "# clustering des documents selon la méthode kmeans \n",
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "km.fit_transform(dt_matrix)\n",
        "\n",
        "# affichage des clusters\n",
        "cluster_labels = km.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "      <th>ClusterLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Document  ... ClusterLabel\n",
              "0                                      The sky is blue and beautiful.  ...            2\n",
              "1                                   Love this blue and beautiful sky!  ...            2\n",
              "2                        The quick brown fox jumps over the lazy dog.  ...            1\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans  ...            0\n",
              "4                         I love green eggs, ham, sausages and bacon!  ...            0\n",
              "5                    The brown fox is quick and the blue dog is lazy!  ...            1\n",
              "6            The sky is very blue and the sky is very beautiful today  ...            2\n",
              "7                         The dog is lazy but the brown fox is quick!  ...            1\n",
              "\n",
              "[8 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpEnQFHrap4Z"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubj7O7wAcnqr"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecXXZGnUfEOz"
      },
      "source": [
        "## Similarité entre documents \n",
        "\n",
        "Une représentation vectorielle des documents offre la possibilité d'utiliser des mesures de distance ou de similarité pour comparer les documents un à un.\n",
        "\n",
        "Si le corpus compte C documents, une comparaison de chaque paire de documents produira une matrice de dimensions C x C où chaque intersection d'une ligne et d'une colonne donne un score de similarité entre une paire de documents.\n",
        "\n",
        "Parmi les mesures les plus communes on retrouve la similarité cosinus, la distance euclidienne, la distance manhattan, la similarité BM25, la distance Jaccard...\n",
        "\n",
        "Ci-dessous nous utilisons la mesure de cosinus sur une modélisation TF-IDF des documents. La mesure de cosinus représente la valeur du cosinus de l'angle entre les vecteurs de deux documents. Moins l'angle entre les vecteurs est grand, plus proches et similaires sont les documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyhlqukAjJ0E"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code ci-dessous permet de calculer la matrice de similarité entre paires de documents. \n",
        "* En analysant la matrice, quels ensembles de documents vous semblent similaires ? A minima vérifier votre intuition à la lecture des documents du corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcIUjd53fCQA"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "similarity_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGO_LIxfjCgb"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmQ5roGcmKWV"
      },
      "source": [
        "##Partitionnement hiérarchique des documents sur la base de la matrice de similarités inter-documents \n",
        "\n",
        "Le [**partitionnement**, ou _Clustering_ en anglais,](https://fr.wikipedia.org/wiki/Partitionnement_de_donn%C3%A9es) désigne des techniques non supervisées qui permettent de grouper des points (ici des documents) en groupes ou clusters.\n",
        "\n",
        "Il existe deux types d'algorithmes de partionnement hiérarchique : agglomératif ou clivant.\n",
        "\n",
        "Nous utiliserons un algorithme hiérarchique de clustering agglomératif qui débutera avec 1 document dans chaque cluster et aggrégera deux clusters à chaque itération. Nous utiserons le [critère de liaison de Ward ](https://fr.wikipedia.org/wiki/M%C3%A9thode_de_Ward) pour choisir des paires de clusters à lier. \n",
        "> La méthode de Ward consiste à regrouper les classes de façon que l'augmentation de l'inertie interclasse soit maximale, ou, de façon que l'augmentation de l'inertie intraclasse (la variance intra-cluster) soit minimale.\n",
        "\n",
        "Le code ci-dessous produit la matrice de liaison (_linkage_). Chaque ligne représente une étape qui indique quels points (clusters) sont fusionnés ensembles. Les deux premières colonnes indiquent les identifiants des clusters fusionnés, la troisième colonne est la distance entre les deux clusters fusionnés, et la quatrième colonne indique le nombre total d'éléments ainsi regroupés.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznwkc_P2Iwu"
      },
      "source": [
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "Z = linkage(similarity_matrix, 'ward')\n",
        "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
        "                         'Distance', 'Cluster Size'], dtype='object')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blxzfWvy2MrS"
      },
      "source": [
        "La construction de cette matrice peut est visualisée sous la forme d'un dendogramme. La ligne en pointillée représente la valeur maximale de distance à partir de laquelle il ne faut plus chercher à regrouper. Elle est arbitrairement fixée ici à `0.1`. On remarque qu'avec cette valeur, on obtient autant de clusters que de documents dans le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bZORtA53dcY"
      },
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# seuil maximale pour la mesure de distance \n",
        "max_dist = 0.1\n",
        "\n",
        "# plot the dendogram\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('Distance')\n",
        "dendrogram(Z)\n",
        "\n",
        "# plot the threshold (seuil de distance)\n",
        "plt.axhline(y=max_dist, c='k', ls='--', lw=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQwfVg2H8Clg"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code ci-dessous vous permet de stopper le clustering à un seuil maximal de distance préfixé.\n",
        "\n",
        "* En jouant avec la valeur de la distance maximale, y-a-t'il moyen d'identifier nos principaux clusters ? Si oui autour de quelle valeur ?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBLmGVtz6KNK"
      },
      "source": [
        "max_dist = 0.1\n",
        "\n",
        "# Flatten clusters from the hierarchical clustering defined by the given linkage matrix\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
        "\n",
        "# Concaténation de la colonne des labels des clusters avec le corpus et les catégories manuelles \n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3LWq-fx2JYn"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA5jFb1I5fRm"
      },
      "source": [
        "## Partitionnement des documents sur la base de la matrice de similarités inter-documents avec la méthode des k-moyennes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5FWFDnzW9lf"
      },
      "source": [
        "La matrice de similarités cosinus inter-documents peut être vue comme matrice _document-similarity_score_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IVKYzTP_ngg"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "* En regardant le `ClusterLabel` produit, pouvez-vous dire si la catégorisation est correcte ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaUsj4Qc98FE"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# clusetering\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "km.fit_transform(similarity_matrix)\n",
        "\n",
        "# affichage\n",
        "cluster_labels = km.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi-TMeTX_a-3"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKta7IOqFAZN"
      },
      "source": [
        "## Topic modeling with LDA's gensim \n",
        "L'[**allocation de Dirichlet latente** (de l'anglais _Latent Dirichlet Allocation_) ou LDA](https://fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente) est ...\n",
        "> ... un modèle génératif probabiliste permettant d'expliquer des ensembles d’observations, par le moyen de groupes non observés, eux-mêmes définis par des similarités de données. \n",
        "\n",
        "Dans ce modèle chaque document consiste en une combinaison de plusieurs thèmes et chaque terme peut être assigné (avec une certaine probabilité) à un certain thème. \n",
        "L'algorithme est itératif. \n",
        "- Initialement les thèmes sont assignés aléatoirement à chaque terme pour chaque document. \n",
        "- Puis pendant un nombre d'itération prédéfini, on répète pour chaque document :  \n",
        "  - calculer `P(T|D)`, la probabilité du thème `T` donné la proportion de termes associés à ce thème `T` dans le document `D` \n",
        "  - calculer `P(W|T)`, la probabilité d'assigner le thème `T` au terme `W` donné la proportion de thèmes `T` assignés au terme `W` dans le corpus\n",
        "  - réassigner le terme `W` au thème `T` avec la probabilité  `P(T|D)*P(W|T)`\n",
        "\n",
        "\n",
        "Pour en savoir plus consulter le [tutoriel de Christine Doig](http://chdoig.github.io/pygotham-topic-modeling/) \n",
        "\n",
        "Les modules [gensim](https://radimrehurek.com/gensim/models/ldamodel.html) et [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) offrent des implémentations de l'algorithme LDA pour générer des topiques. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aHntHjvPLV0"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Ci-dessous nous mettons en oeuvre le module gensim qui offre une API de haut niveau. Le paramètre \"important\" est le nombre de thèmes (`num_topics=3 \n",
        "`) souhaités en fin de processus. En connaissance du corpus, nous le fixons à 3.\n",
        "\n",
        "La méthode `ldamodel.show_topics()` affiche les thèmes et les poids associés à chaque terme pour chaque thème. Plus le poid est élevé plus le terme est significatif pour ce thème.\n",
        "* Observez les résultats retournés par la méthode `ldamodel.show_topics()`, les poids vous semblent-ils cohérents ? \n",
        "* Relancer plusieurs fois la modélisation. Vous pouvez aussi utiliser le module PyLDAvis pour visualiser le topic model généré (cf. ci-dessous).  La modélisation est-elle stable ? Pourquoi ? Spécifier les paramètres suivants `iterations=10000, random_state=1234` dans la classe `LdaModel`. Le modèle est-il plus stable ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E5bXGBBFM3_"
      },
      "source": [
        "from gensim.models import LdaModel\n",
        "from gensim import corpora\n",
        "\n",
        "# récupérer une version normalisé du corpus\n",
        "preprocessed_corpus = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
        "\n",
        "# build a dictionary\n",
        "dictionary = corpora.Dictionary(preprocessed_corpus)\n",
        "# and a doc2bow gensim corpus\n",
        "# doc2bow Convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
        "doc2bow_corpus = [dictionary.doc2bow(document) for document in preprocessed_corpus]\n",
        "print (doc2bow_corpus)\n",
        "\n",
        "# then compute a topic model with topics based on gensim LDA\n",
        "num_topics=3 \n",
        "ldamodel = LdaModel(doc2bow_corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Get a representation for selected topics\n",
        "ldamodel.show_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EASbsMEcFRQK"
      },
      "source": [
        " Vous pouvez aussi utiliser le module PyLDAvis pour visualiser le topic model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9771TUjvFWNs"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYDxaTzSFaQQ"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# feed the LDA model into the pyLDAvis instance\n",
        "lda_display = gensimvis.prepare(ldamodel, doc2bow_corpus, dictionary, mds='mmds')\n",
        "#print (lda_display)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJmSAyDRriw"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3eI6o_fq6A"
      },
      "source": [
        "## Partitionnement des documents sur la base de la matrice document-topic avec la méthode des k-moyennes\n",
        "\n",
        "Il s'agit de lancer un clustering k-means des documents non plus sur une représentation des documents en termes d'un sac de mots pondérés avec un score TF-IDF mais d'utiliser la distribution des thèmes générés par l'algorithme LDA sur chaque document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cMigh2khY-m"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code suivant permet de faire un partitionnement des documents en 3 groupes  sur la base d'une représentation des documents à base des thèmes qui les composent. Pour chaque document les thèmes ont un score de probabilité différent. Ce nombre 3 correpond au nombre de catégories de documents que nous connaissons être présentes dans ce corpus.\n",
        "\n",
        "* En utilisant le code de la section précédente (Topic modeling with LDA's gensim) pour générer des modèles de thèmes, générer des modèles avec différents nombre de thèmes générés : 3, 8 ou 10 par exemples. Cela va conduire à représenter chaque document avec 3, 8 ou 10 thèmes par exemples.\n",
        "Avec combien de thèmes dans la représentation des documents, obtenez-vous le meilleur clustering ? Les vecteurs des thèmes ont-ils toujours un sens pour vous ? Que pouvez-vous dire de cette représentation par rapport à la représentation BOW qui avait pour dimension le vocabulaire ? FIXME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMjP8BD-gT9e"
      },
      "source": [
        "# Récupère une matrice de distribution des thèmes pour chaque document\n",
        "dt_matrix = ldamodel.get_document_topics(doc2bow_corpus)\n",
        "\n",
        "# sklearn.cluster.KMeans utilise une matrice des scores alors que la matrice \n",
        "# retournée par gensim contient des tuples (topic_id, score) \n",
        "# le code suivant fait la conversion de format\n",
        "new_dt_matrix = list()\n",
        "for d in dt_matrix:\n",
        "  new_dt_matrix.append([c for i, c in d])\n",
        "print (new_dt_matrix)\n",
        "dt_matrix = new_dt_matrix\n",
        "\n",
        "# visualisation de la distribution des thèmes sur chaque document ; ici dans le cas où il y a 3 thèmes\n",
        "#features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
        "#features\n",
        "\n",
        "# clustering des documents selon la méthode kmeans \n",
        "# en utilisant la distribution des thèmes et non la distrubution des termes \n",
        "# comme traits discriminants entre documents\n",
        "from sklearn.cluster import KMeans\n",
        "# Warning: ici n_clusters ne doit pas être touché\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "#km.fit_transform(features)\n",
        "km.fit_transform(dt_matrix)\n",
        "\n",
        "# affichage des clusters\n",
        "cluster_labels = km.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY4OMNakilOO"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBKM65tIoK8"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "On retiendra que les vectorisations des documents sous la forme d'un sac de mots d'un vocabulaire permet d'obtenir des résultats intéressants en recherche d'information et en partitionnement mais présente deux défauts :\n",
        "\n",
        "- un modèle \"sac de mots\" ne conserve pas l'ordre des mots et leur contexte d'apparition des mots\n",
        "- les vecteurs sont de grande taille, celle du vocabulaire du corpus. Quitte à être remplis de valeurs nulles.\n",
        "\n",
        "La représentation d'un document à travers des thèmes globalement générés sur le corpus constitue une représentation plus compacte qu'une représentation avec un vocabulaire comme trait. La matrice de documents est au final non creuse.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjvyEFwqcgp1"
      },
      "source": [
        "# Références\n",
        "* https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "* word2vec embeddings building, most similar and visualization (ENSAE 2021) https://colab.research.google.com/drive/1Y9fC04hnTspwmqJkK5yZJWwUpDybzq9m#scrollTo=Uy2u0Ngs1R-w\n",
        "* traditional and Words embeddings avec Word2vec https://github.com/clement-plancq/outils-corpus/blob/master/outils_corpus-7.ipynb\n",
        "* text preprocessing, Bag of Words Model, Bag of N-Grams Model, TF-IDF Model, Document Similarity, Document Clustering with Similarity Features, Topic Models, Document Clustering with Topic Model Features\n",
        "  * https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
        "  *  Traditional strategies (2020)  https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch04%20-%20Feature%20Engineering%20for%20Text%20Representation/Ch04a%20-%20Feature%20Engineering%20Text%20Data%20-%20Traditional%20Strategies.ipynb\n",
        "  * With some explanation https://github.com/dipanjanS/nlp_essentials/blob/master/notebooks/02_Text_Representation_Statistical_Models.ipynb\n",
        "  * Advanced Deep Learning Strategies (2020) https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch04%20-%20Feature%20Engineering%20for%20Text%20Representation/Ch04b%20-%20Feature%20Engineering%20Text%20Data%20-%20Advanced%20Deep%20Learning%20Strategies.ipynb\n",
        "  * Older version 2018  https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch04_Feature_Engineering_and_Selection/Feature%20Engineering%20on%20Text%20Data.ipynb ; https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch04_Feature_Engineering_and_Selection/Feature%20Engineering%20on%20Text%20Data.ipynb\n",
        "* https://github.com/fastai/course-nlp/blob/master/2-svd-nmf-topic-modeling.ipynb\n",
        "* https://github.com/dipanjanS/nlp_essentials/blob/master/notebooks/04_NLP_Applications_Text_Similarity_Content_Recommenders.ipynb\n",
        "* topic modelling with Gensim's LDA and visualization with PyLDAvis (ENSAE 2021)  https://colab.research.google.com/drive/1-4OAfhAZGWNzTB1CAtIEPvK-xJ6SunnP?usp=sharing#scrollTo=F2BCX2wSt9VA\n"
      ]
    }
  ]
}