{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2-ATAL-2021-22_02_NER with BiLSTM-CRF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0GzIyX1JHthIfgMDAtL1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/Copie2_M2_ATAL_2021_22_02_NER_with_BiLSTM_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Recent Advances in Sequence Labeling from Deep Learning Models\n",
        "\n",
        "Les approches pour l'étiquetage de séquence fondées sur les réseaux de neurones profonds compte trois étapes :\n",
        "1. The embedding module is the first stage that maps words into their distributed representations (pretrained word embeddings, character-\n",
        "level representations, hand-crafted features and sentence-level\n",
        "representations). \n",
        "2. The context encoder module extracts contextual features (e.g. RNN/Bi-LSTM, CNN)\n",
        "3. and the inference module predict labels and generate optimal label sequence as output of the model (e.g. SoftMax, CRF, RNN). \n",
        "\n",
        "[Zhiyong He, Zanbo Wang, Sheng Jiang. A Survey on Recent Advances in Sequence Labeling from Deep Learning Models. Published 13 November 2020. Computer Science. ArXiv](https://arxiv.org/pdf/2011.06727.pdf)\n"
      ],
      "metadata": {
        "id": "NWNr2Oc6qJHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Bref historique des systèmes de NER neuronaux\n",
        "\n",
        "(on ne vous demande pas de lire les articles)\n",
        "\n",
        "* L'architecture \"SENNA\", novatrice dans l'idée de la résolution des tâches du TAL avec un modèle de langue neuronal (incluant notamment une méthode de construction de \"pretrained word embeddings\") : R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch, Journal of Machine Learning Research (JMLR), 2011. ; [[article]](http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf) ; [[implémentation]](https://ronan.collobert.com/senna/)\n",
        "* Premier article à appliquer les BiLSTM-CRF au NER : Zhiheng Huang, Wei Xu, Kai Yu, Bidirectional LSTM-CRF Models for Sequence Tagging, Arxiv, Computation and Language, Submitted on 9 Aug 2015 ; [[article]](https://arxiv.org/pdf/1508.01991.pdf) ; [[implémentation1]](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html) (tutoriel avancé de pytorch) ; [[implémentation2]](https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch) (inclut aussi un modèle Bi-LSTM-CNN-CRF) ; [[implémentation3]](https://github.com/jidasheng/bi-lstm-crf)  ; [[implémentation4]](http://www.gabormelli.com/RKB/index.php?title=Bidirectional_LSTM/CRF_(BiLTSM-CRF)_Training_System) ; [[implémentation5]](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html) (avec tensorflow)\n",
        "* BiLSTM-CNN-CRF Implementation for Sequence Tagging (extension with the ELMo representations) : Reimers, Nils, and Gurevych, Iryna, Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), September 2017, Copenhagen, Denmark, 338-348 ; [[article]](http://aclweb.org/anthology/D17-1035) ; [[implémentation]](https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf)\n",
        "* Le 3e modèle le plus performant en 2020 sur la tâche NER sans ressources externes : Ying Luo, Fengshun Xiao, and Hai Zhao. Hierarchical contextualized representation for named entity recognition. In AAAI, pages 8441–8448, 2020 ; [[implémentation]](https://github.com/cslydia/Hire-NER) ; Utilise [NCRF++: An Open-source Neural Sequence Labeling Toolkit](https://github.com/jiesutd/NCRFpp)\n"
      ],
      "metadata": {
        "id": "0boga_k7vMTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Bidirectional LSTM-CRF Implémentation de Huang et al. 2015 \n",
        "\n",
        "Le code dans les cellules suivantes provient de l'[implémentation 2](https://github.com/jidasheng/bi-lstm-crf/) du système de (Huang et al., 2015). Celle-ci s'appuie sur la bibliothèque pytorch.\n"
      ],
      "metadata": {
        "id": "Inik1ZFZ2tOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### VOTRE TRAVAIL \n",
        "* Exécutez les cellules sans passer trop de temps à comprendre les détails de l'implémentation. Répondez aux questions quand vous y êtes invité.\n"
      ],
      "metadata": {
        "id": "hLo59Npr9S5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation des dépendances \n",
        "\n",
        "(éventuellement exécuter plusieurs fois la cellule pour faire disparaître les erreurs)"
      ],
      "metadata": {
        "id": "7ZoON5ru4-Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/54358280/packed-padded-sequence-gives-error-when-used-with-gpu\n",
        "!pip install torch==1.6.0 torchvision==0.7.0"
      ],
      "metadata": {
        "id": "Jr7DMncsp_uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test pour déterminer si le hardware de votre machine dispose d'un gpu. Dans un premier temps, sous gcolab, garder le type d'exécution natif \"cpu\" afin d'avoir une idée des temps d'entraînement de l'architecture."
      ],
      "metadata": {
        "id": "lQK2lrEy5PSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Get cpu or gpu device for training.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "MOVf1iODp5Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition d'une cellule neuronale CRF \n",
        "\n",
        "* Trouve la séquence d'étiquettes la plus probable correspondant à une séquence de mots donnée\n",
        "* Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/crf.py\n",
        "* Pour en savoir plus sur les CRF, vous pouvez lire : http://www.cs.columbia.edu/~mcollins/crf.pdf.\n",
        "\n"
      ],
      "metadata": {
        "id": "fSzPQYDlMtsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def log_sum_exp(x):\n",
        "    \"\"\"calculate log(sum(exp(x))) = max(x) + log(sum(exp(x - max(x))))\n",
        "    \"\"\"\n",
        "    max_score = x.max(-1)[0]\n",
        "    return max_score + (x - max_score.unsqueeze(-1)).exp().sum(-1).log()\n",
        "\n",
        "\n",
        "IMPOSSIBLE = -1e4\n",
        "\n",
        "class CRF(nn.Module):\n",
        "    \"\"\"General CRF module.\n",
        "    The CRF module contain a inner Linear Layer which transform the input from features space to tag space.\n",
        "\n",
        "    :param in_features: number of features for the input\n",
        "    :param num_tag: number of tags. DO NOT include START, STOP tags, they are included internal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, num_tags):\n",
        "        super(CRF, self).__init__()\n",
        "\n",
        "        self.num_tags = num_tags + 2\n",
        "        self.start_idx = self.num_tags - 2\n",
        "        self.stop_idx = self.num_tags - 1\n",
        "\n",
        "        self.fc = nn.Linear(in_features, self.num_tags)\n",
        "\n",
        "        # transition factor, Tij mean transition from j to i\n",
        "        self.transitions = nn.Parameter(torch.randn(self.num_tags, self.num_tags), requires_grad=True)\n",
        "        self.transitions.data[self.start_idx, :] = IMPOSSIBLE\n",
        "        self.transitions.data[:, self.stop_idx] = IMPOSSIBLE\n",
        "\n",
        "    def forward(self, features, masks):\n",
        "        \"\"\"decode tags\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "        return self.__viterbi_decode(features, masks[:, :features.size(1)].float())\n",
        "\n",
        "    def loss(self, features, ys, masks):\n",
        "        \"\"\"negative log likelihood loss\n",
        "        B: batch size, L: sequence length, D: dimension\n",
        "\n",
        "        :param features: [B, L, D]\n",
        "        :param ys: tags, [B, L]\n",
        "        :param masks: masks for padding, [B, L]\n",
        "        :return: loss\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "\n",
        "        L = features.size(1)\n",
        "        masks_ = masks[:, :L].float()\n",
        "\n",
        "        forward_score = self.__forward_algorithm(features, masks_)\n",
        "        gold_score = self.__score_sentence(features, ys[:, :L].long(), masks_)\n",
        "        loss = (forward_score - gold_score).mean()\n",
        "        return loss\n",
        "\n",
        "    def __score_sentence(self, features, tags, masks):\n",
        "        \"\"\"Gives the score of a provided tag sequence\n",
        "\n",
        "        :param features: [B, L, C]\n",
        "        :param tags: [B, L]\n",
        "        :param masks: [B, L]\n",
        "        :return: [B] score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        # emission score\n",
        "        emit_scores = features.gather(dim=2, index=tags.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # transition score\n",
        "        start_tag = torch.full((B, 1), self.start_idx, dtype=torch.long, device=tags.device)\n",
        "        tags = torch.cat([start_tag, tags], dim=1)  # [B, L+1]\n",
        "        trans_scores = self.transitions[tags[:, 1:], tags[:, :-1]]\n",
        "\n",
        "        # last transition score to STOP tag\n",
        "        last_tag = tags.gather(dim=1, index=masks.sum(1).long().unsqueeze(1)).squeeze(1)  # [B]\n",
        "        last_score = self.transitions[self.stop_idx, last_tag]\n",
        "\n",
        "        score = ((trans_scores + emit_scores) * masks).sum(1) + last_score\n",
        "        return score\n",
        "\n",
        "    def __viterbi_decode(self, features, masks):\n",
        "        \"\"\"decode to tags using viterbi algorithm\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        bps = torch.zeros(B, L, C, dtype=torch.long, device=features.device)  # back pointers\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        max_score = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        max_score[:, self.start_idx] = 0\n",
        "\n",
        "        for t in range(L):\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            emit_score_t = features[:, t]  # [B, C]\n",
        "\n",
        "            # [B, 1, C] + [C, C]\n",
        "            acc_score_t = max_score.unsqueeze(1) + self.transitions  # [B, C, C]\n",
        "            acc_score_t, bps[:, t, :] = acc_score_t.max(dim=-1)\n",
        "            acc_score_t += emit_score_t\n",
        "            max_score = acc_score_t * mask_t + max_score * (1 - mask_t)  # max_score or acc_score_t\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        max_score += self.transitions[self.stop_idx]\n",
        "        best_score, best_tag = max_score.max(dim=-1)\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_paths = []\n",
        "        bps = bps.cpu().numpy()\n",
        "        for b in range(B):\n",
        "            best_tag_b = best_tag[b].item()\n",
        "            seq_len = int(masks[b, :].sum().item())\n",
        "\n",
        "            best_path = [best_tag_b]\n",
        "            for bps_t in reversed(bps[b, :seq_len]):\n",
        "                best_tag_b = bps_t[best_tag_b]\n",
        "                best_path.append(best_tag_b)\n",
        "            # drop the last tag and reverse the left\n",
        "            best_paths.append(best_path[-2::-1])\n",
        "\n",
        "        return best_score, best_paths\n",
        "\n",
        "    def __forward_algorithm(self, features, masks):\n",
        "        \"\"\"calculate the partition function with forward algorithm.\n",
        "        TRICK: log_sum_exp([x1, x2, x3, x4, ...]) = log_sum_exp([log_sum_exp([x1, x2]), log_sum_exp([x3, x4]), ...])\n",
        "\n",
        "        :param features: features. [B, L, C]\n",
        "        :param masks: [B, L] masks\n",
        "        :return:    [B], score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        scores = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        scores[:, self.start_idx] = 0.\n",
        "        trans = self.transitions.unsqueeze(0)  # [1, C, C]\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for t in range(L):\n",
        "            emit_score_t = features[:, t].unsqueeze(2)  # [B, C, 1]\n",
        "            score_t = scores.unsqueeze(1) + trans + emit_score_t  # [B, 1, C] + [1, C, C] + [B, C, 1] => [B, C, C]\n",
        "            score_t = log_sum_exp(score_t)  # [B, C]\n",
        "\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            scores = score_t * mask_t + scores * (1 - mask_t)\n",
        "        scores = log_sum_exp(scores + self.transitions[self.stop_idx])\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "kyEIEbBGMp8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Paramètres > Cocher \"affichage de la numérotation des lignes\"\n",
        "\n",
        "* Quel est le nom de la _loss function_ ? A quelle ligne est-ce spécifiée ?\n",
        "* En quelques mots, à quoi sert l'algorithme de Viterbi ? Cherchez sur le web...\n",
        "\n",
        "Eventuellement, en apprendre davantage sur quelques [_loss functions_](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)."
      ],
      "metadata": {
        "id": "Byc18sR5FNSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition d'une architecture neuronale Bi-LSTM CRF\n",
        "\n",
        "La classe suivante implémente un modèle Bi-LSTM CRF\n",
        "- Construction des embeddings de la phrase\n",
        "- Capture du contexte avec une cellule RNN \n",
        "- Prédiction de la séquence d'étiquetage à l'aide de la cellule CRF qui prend comme input la sortie du RNN\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/model.py\n"
      ],
      "metadata": {
        "id": "_LZZnpMvM20E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class BiRnnCrf(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_rnn_layers=1, rnn=\"lstm\"):\n",
        "        super(BiRnnCrf, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tagset_size = tagset_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        RNN = nn.LSTM if rnn == \"lstm\" else nn.GRU\n",
        "        self.rnn = RNN(embedding_dim, hidden_dim // 2, num_layers=num_rnn_layers,\n",
        "                       bidirectional=True, batch_first=True)\n",
        "        self.crf = CRF(hidden_dim, self.tagset_size)\n",
        "\n",
        "    def __build_features(self, sentences):\n",
        "        masks = sentences.gt(0)\n",
        "        embeds = self.embedding(sentences.long())\n",
        "\n",
        "        seq_length = masks.sum(1)\n",
        "        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n",
        "        embeds = embeds[perm_idx, :]\n",
        "\n",
        "        pack_sequence = pack_padded_sequence(embeds, lengths=sorted_seq_length,  batch_first=True)\n",
        "        packed_output, _ = self.rnn(pack_sequence)\n",
        "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        _, unperm_idx = perm_idx.sort()\n",
        "        lstm_out = lstm_out[unperm_idx, :]\n",
        "\n",
        "        return lstm_out, masks\n",
        "\n",
        "    def loss(self, xs, tags):\n",
        "        features, masks = self.__build_features(xs)\n",
        "        loss = self.crf.loss(features, tags, masks=masks)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, xs):\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        features, masks = self.__build_features(xs)\n",
        "        scores, tag_seq = self.crf(features, masks)\n",
        "        return scores, tag_seq"
      ],
      "metadata": {
        "id": "FvjvPnHDM3Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Paramètres > Cocher \"affichage de la numérotation des lignes\"\n",
        "\n",
        "* L'implémentation offre deux types de cellules RNN possibles. Lesquels ? Indiquez le numéro de ligne où ce choix est spécifié.\n",
        "* Après la représentation en embeddings des phrase et avant le passage à la cellule RNN, quel type de traitement est réalisé ? Indiquez le numéro de ligne où ce traitement est spécifié. "
      ],
      "metadata": {
        "id": "ZcA7yrXSDkPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Définition des prétraitement des données \n"
      ],
      "metadata": {
        "id": "gGl1Xe3C9rVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D'abord la définition de méthodes \"utils\" pour la phase de prétraitement à savoir la sauvegarde et le chargement de fichiers de configuration e.g. vocabulaire, jeu d'étiquettes, paramètres du modèle neuronal (dimension des embeddings...), partition des données générées...\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/utils.py"
      ],
      "metadata": {
        "id": "ViLNx_efOyez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "OOV = \"<OOV>\"\n",
        "\n",
        "\n",
        "def save_json_file(obj, file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        return json.load(f)"
      ],
      "metadata": {
        "id": "hUPqw3j6Oyte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis la classe de pré-traitement des données qui sera initialisé à l'aide des chemins des fichiers contenant le vocabulaire, le jeu d'étiquettes et les données annotées (phrases découpées en mots avec étiquettes). Outre charger ces fichiers de configuration et données, la classe partitionne les données en ensemble d'entrainement, de validation et de tests (d'après les paramètres spécifiés par défaut ou à l'appel du système). Les données sont aussi \"vectorisées\". Il s'agit essentiellement d'une substitution des mots des phrases par leur identifiant numérique correspondant à une entrée dans le vocabulaire donné.\n",
        "\n",
        "https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/preprocess.py"
      ],
      "metadata": {
        "id": "DhsxcoePO-4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import join, exists\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "#FILE_VOCAB = \"vocab.json\"\n",
        "#FILE_TAGS = \"tags.json\"\n",
        "#FILE_DATASET = \"dataset.txt\"\n",
        "#FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, config_dir, save_config_dir=None, verbose=True):\n",
        "        self.config_dir = config_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.vocab, self.vocab_dict = self.__load_list_file(FILE_VOCAB, offset=1, verbose=verbose)\n",
        "        self.tags, self.tags_dict = self.__load_list_file(FILE_TAGS, verbose=verbose)\n",
        "        if save_config_dir:\n",
        "            self.__save_config(save_config_dir)\n",
        "\n",
        "        self.PAD_IDX = 0\n",
        "        self.OOV_IDX = len(self.vocab)\n",
        "        self.__adjust_vocab()\n",
        "\n",
        "    def __load_list_file(self, file_name, offset=0, verbose=False):\n",
        "        file_path = join(self.config_dir, file_name)\n",
        "        if not exists(file_path):\n",
        "            raise ValueError('\"{}\" file does not exist.'.format(file_path))\n",
        "        else:\n",
        "            elements = load_json_file(file_path)\n",
        "            elements_dict = {w: idx + offset for idx, w in enumerate(elements)}\n",
        "            if verbose:\n",
        "                print(\"config {} loaded\".format(file_path))\n",
        "            return elements, elements_dict\n",
        "\n",
        "    def __adjust_vocab(self):\n",
        "        self.vocab.insert(0, PAD)\n",
        "        self.vocab_dict[PAD] = 0\n",
        "\n",
        "        self.vocab.append(OOV)\n",
        "        self.vocab_dict[OOV] = len(self.vocab) - 1\n",
        "\n",
        "    def __save_config(self, dst_dir):\n",
        "        char_file = join(dst_dir, FILE_VOCAB)\n",
        "        save_json_file(self.vocab, char_file)\n",
        "\n",
        "        tag_file = join(dst_dir, FILE_TAGS)\n",
        "        save_json_file(self.tags, tag_file)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"tag dict file => {}\".format(tag_file))\n",
        "            print(\"tag dict file => {}\".format(char_file))\n",
        "\n",
        "    @staticmethod\n",
        "    def __cache_file_path(corpus_dir, max_seq_len):\n",
        "        return join(corpus_dir, FILE_DATASET_CACHE.format(max_seq_len))\n",
        "\n",
        "    def load_dataset(self, corpus_dir, val_split, test_split, max_seq_len):\n",
        "        \"\"\"load the train set\n",
        "        :return: (xs, ys)\n",
        "            xs: [B, L]\n",
        "            ys: [B, L, C]\n",
        "        \"\"\"\n",
        "        ds_path = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        if not exists(ds_path):\n",
        "            xs, ys = self.__build_corpus(corpus_dir, max_seq_len)\n",
        "        else:\n",
        "            print(\"loading dataset {} ...\".format(ds_path))\n",
        "            dataset = np.load(ds_path)\n",
        "            xs, ys = dataset[\"xs\"], dataset[\"ys\"]\n",
        "\n",
        "        xs, ys = map(\n",
        "            torch.tensor, (xs, ys)\n",
        "        )\n",
        "\n",
        "        # split the dataset\n",
        "        total_count = len(xs)\n",
        "        assert total_count == len(ys)\n",
        "        val_count = int(total_count * val_split)\n",
        "        test_count = int(total_count * test_split)\n",
        "        train_count = total_count - val_count - test_count\n",
        "        assert train_count > 0 and val_count > 0\n",
        "\n",
        "        indices = np.cumsum([0, train_count, val_count, test_count])\n",
        "        datasets = [(xs[s:e], ys[s:e]) for s, e in zip(indices[:-1], indices[1:])]\n",
        "        print(\"datasets loaded:\")\n",
        "        for (xs_, ys_), name in zip(datasets, [\"train\", \"val\", \"test\"]):\n",
        "            print(\"\\t{}: {}, {}\".format(name, xs_.shape, ys_.shape))\n",
        "        return datasets\n",
        "\n",
        "    def decode_tags(self, batch_tags):\n",
        "        batch_tags = [\n",
        "            [self.tags[t] for t in tags]\n",
        "            for tags in batch_tags\n",
        "        ]\n",
        "        return batch_tags\n",
        "\n",
        "    def sent_to_vector(self, sentence, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(sentence)\n",
        "        vec = [self.vocab_dict.get(c, self.OOV_IDX) for c in sentence[:max_seq_len]]\n",
        "        return vec + [self.PAD_IDX] * (max_seq_len - len(vec))\n",
        "\n",
        "    def tags_to_vector(self, tags, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(tags)\n",
        "        vec = [self.tags_dict[c] for c in tags[:max_seq_len]]\n",
        "        return vec + [0] * (max_seq_len - len(vec))\n",
        "\n",
        "    def __build_corpus(self, corpus_dir, max_seq_len):\n",
        "      # remove cache files !!!\n",
        "        file_path = join(corpus_dir, FILE_DATASET)\n",
        "        xs, ys = [], []\n",
        "        with open(file_path, encoding=\"utf8\") as f:\n",
        "            for idx, line in tqdm(enumerate(f), desc=\"parsing {}\".format(file_path)):\n",
        "                fields = line.strip().split(\"\\t\")\n",
        "                if len(fields) != 2:\n",
        "                    raise ValueError(\"format error in line {}, tabs count: {}\".format(idx + 1, len(fields) - 1))\n",
        "\n",
        "                sentence, tags = fields\n",
        "\n",
        "\n",
        "                try:\n",
        "                    if sentence[0] == \"[\":\n",
        "                        sentence = json.loads(sentence)\n",
        "                    tags = json.loads(tags)\n",
        "\n",
        "                    #print ('Debug: sentence', sentence)\n",
        "                    #print ('Debug: tags', tags)\n",
        "\n",
        "                    xs.append(self.sent_to_vector(sentence, max_seq_len=max_seq_len))\n",
        "                    ys.append(self.tags_to_vector(tags, max_seq_len=max_seq_len))\n",
        "                    if len(sentence) != len(tags):\n",
        "                        raise ValueError('\"sentence length({})\" != \"tags length({})\" in line {}\"'.format(\n",
        "                            len(sentence), len(tags), idx + 1))\n",
        "                except Exception as e:\n",
        "                    raise ValueError(\"exception raised when parsing line {}\\n\\t{}\\n\\t{}\".format(idx + 1, line, e))\n",
        "\n",
        "        xs, ys = np.asarray(xs), np.asarray(ys)\n",
        "\n",
        "        # save train set\n",
        "        cache_file = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        np.savez(cache_file, xs=xs, ys=ys)\n",
        "        print(\"dataset cache({}, {}) => {}\".format(xs.shape, ys.shape, cache_file))\n",
        "        return xs, ys"
      ],
      "metadata": {
        "id": "vyRopi-JO799"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition du modèle et de la méthode d'entraînement \n",
        "\n",
        "D'abord la définition de la méthode qui instancie l'architecture. Les fichiers associés (_model_ et _arguments_) seront sauvés (ou chargés si un précédent entraînement a déjà eu lieu)  depuis _model_dir_.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/utils.py"
      ],
      "metadata": {
        "id": "TS9TOT0hPgd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import exists, join\n",
        "import torch\n",
        "\n",
        "#FILE_ARGUMENTS = \"arguments.json\"\n",
        "#FILE_MODEL = \"model.pth\"\n",
        "\n",
        "def arguments_filepath(model_dir):\n",
        "    return join(model_dir, FILE_ARGUMENTS)\n",
        "\n",
        "\n",
        "def model_filepath(model_dir):\n",
        "    return join(model_dir, FILE_MODEL)\n",
        "\n",
        "\n",
        "def build_model(args, processor, load=True, verbose=False):\n",
        "    model = BiRnnCrf(len(processor.vocab), len(processor.tags),\n",
        "                     embedding_dim=args['embedding_dim'], hidden_dim=args['hidden_dim'], num_rnn_layers=args['num_rnn_layers'])\n",
        "\n",
        "    # weights\n",
        "    model_path = model_filepath(args['model_dir'])\n",
        "    if exists(model_path) and load:\n",
        "        state_dict = torch.load(model_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        if verbose:\n",
        "            print(\"load model weights from {}\".format(model_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "def running_device(device):\n",
        "    if torch.cuda.is_available():\n",
        "      print ('running_device gpu')\n",
        "    else:  print ('running_device cpu')\n",
        "    return device if device else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oQjVXdb8Pdjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis la définition des méthodes dédiées à l'entraînement du modèle\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/train.py"
      ],
      "metadata": {
        "id": "znjmr60SPtZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import mkdir\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def __eval_model(model, device, dataloader, desc):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # eval\n",
        "        losses, nums = zip(*[\n",
        "            (model.loss(xb.to(device), yb.to(device)), len(xb))\n",
        "            for xb, yb in tqdm(dataloader, desc=desc)])\n",
        "        return np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "\n",
        "def __save_loss(losses, file_path):\n",
        "    pd.DataFrame(data=losses, columns=[\"epoch\", \"batch\", \"train_loss\", \"val_loss\"]).to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "def __save_model(model_dir, model):\n",
        "    model_path = model_filepath(model_dir)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(\"save model => {}\".format(model_path))\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    model_dir = args['model_dir']\n",
        "    if not exists(model_dir):\n",
        "        mkdir(model_dir)\n",
        "#    save_json_file(vars(args), arguments_filepath(model_dir))\n",
        "    save_json_file(args, arguments_filepath(model_dir))\n",
        "\n",
        "    preprocessor = Preprocessor(config_dir=args['corpus_dir'], save_config_dir=args['model_dir'], verbose=True)\n",
        "    model = build_model(args, preprocessor, load=args['recovery'], verbose=True)\n",
        "\n",
        "    # loss\n",
        "    loss_path = join(args['model_dir'], \"loss.csv\")\n",
        "    losses = pd.read_csv(loss_path).values.tolist() if args['recovery'] and exists(loss_path) else []\n",
        "\n",
        "    # datasets\n",
        "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = preprocessor.load_dataset(\n",
        "        args['corpus_dir'], args['val_split'], args['test_split'], max_seq_len=args['max_seq_len'])\n",
        "    train_dl = DataLoader(TensorDataset(x_train, y_train), batch_size=args['batch_size'], shuffle=True)\n",
        "    valid_dl = DataLoader(TensorDataset(x_val, y_val), batch_size=args['batch_size'] * 2)\n",
        "    test_dl = DataLoader(TensorDataset(x_test, y_test), batch_size=args['batch_size'] * 2)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "    device = running_device(args['device'])\n",
        "    model.to(device)\n",
        "\n",
        "    val_loss = 0\n",
        "    best_val_loss = 1e4\n",
        "    for epoch in range(args['num_epoch']):\n",
        "        # train\n",
        "        model.train()\n",
        "        bar = tqdm(train_dl)\n",
        "        for bi, (xb, yb) in enumerate(bar):\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss = model.loss(xb.to(device), yb.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            bar.set_description(\"{:2d}/{} loss: {:5.2f}, val_loss: {:5.2f}\".format(\n",
        "                epoch+1, args['num_epoch'], loss, val_loss))\n",
        "            losses.append([epoch, bi, loss.item(), np.nan])\n",
        "\n",
        "        # evaluation\n",
        "        val_loss = __eval_model(model, device, dataloader=valid_dl, desc=\"eval\").item()\n",
        "        # save losses\n",
        "        losses[-1][-1] = val_loss\n",
        "        __save_loss(losses, loss_path)\n",
        "\n",
        "        # save model\n",
        "        if not args['save_best_val_model'] or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            __save_model(args['model_dir'], model)\n",
        "            print(\"save model(epoch: {}) => {}\".format(epoch, loss_path))\n",
        "\n",
        "    # test\n",
        "    test_loss = __eval_model(model, device, dataloader=test_dl, desc=\"test\").item()\n",
        "    last_loss = losses[-1][:]\n",
        "    last_loss[-1] = test_loss\n",
        "    losses.append(last_loss)\n",
        "    __save_loss(losses, loss_path)\n",
        "    print(\"training completed. test loss: {:.2f}\".format(test_loss))\n",
        "\n"
      ],
      "metadata": {
        "id": "3zYXtKrJPssX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition de la méthode de prédiction (utilisation du modèle)\n",
        "\n",
        "La classe WordsTagger effectue l'étiquetage à proprement parler d'une nouvelle séquence de mots. Elle requiert le chemin vers un modèle.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/predict.py"
      ],
      "metadata": {
        "id": "Ssoc_QOnVUVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordsTagger:\n",
        "    def __init__(self, model_dir, device=None):\n",
        "        args = load_json_file(arguments_filepath(model_dir))\n",
        "        #args = dict()\n",
        "        args['model_dir'] = model_dir\n",
        "        self.args = args\n",
        "\n",
        "        self.preprocessor = Preprocessor(config_dir=model_dir, verbose=False)\n",
        "        self.model = build_model(self.args, self.preprocessor, load=True, verbose=False)\n",
        "        self.device = running_device(device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def __call__(self, sentences, begin_tags=\"BS\"):\n",
        "        \"\"\"predict texts\n",
        "        :param sentences: a text or a list of text\n",
        "        :param begin_tags: begin tags for the beginning of a span\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not isinstance(sentences, (list, tuple)):\n",
        "            raise ValueError(\"sentences must be a list of sentence\")\n",
        "\n",
        "        try:\n",
        "            sent_tensor = np.asarray([self.preprocessor.sent_to_vector(s) for s in sentences])\n",
        "            sent_tensor = torch.from_numpy(sent_tensor).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                _, tags = self.model(sent_tensor)\n",
        "            tags = self.preprocessor.decode_tags(tags)\n",
        "        except RuntimeError as e:\n",
        "            print(\"*** runtime error: {}\".format(e))\n",
        "            raise e\n",
        "        return tags, self.tokens_from_tags(sentences, tags, begin_tags=begin_tags)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokens_from_tags(sentences, tags_list, begin_tags):\n",
        "        \"\"\"extract entities from tags\n",
        "        :param sentences: a list of sentence\n",
        "        :param tags_list: a list of tags\n",
        "        :param begin_tags:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not tags_list:\n",
        "            return []\n",
        "\n",
        "        def _tokens(sentence, ts):\n",
        "            # begins: [(idx, label), ...]\n",
        "            all_begin_tags = begin_tags + \"O\"\n",
        "            begins = [(idx, t[2:]) for idx, t in enumerate(ts) if t[0] in all_begin_tags]\n",
        "            begins = [\n",
        "                         (idx, label)\n",
        "                         for idx, label in begins\n",
        "                         if ts[idx] != \"O\" or (idx > 0 and ts[idx - 1] != \"O\")\n",
        "                     ] + [(len(ts), \"\")]\n",
        "\n",
        "            tokens_ = [(sentence[s:e], label) for (s, label), (e, _) in zip(begins[:-1], begins[1:]) if label]\n",
        "            return [((t, tag) if tag else t) for t, tag in tokens_]\n",
        "\n",
        "        tokens_list = [_tokens(sentence, ts) for sentence, ts in zip(sentences, tags_list)]\n",
        "        return tokens_list\n",
        "\n"
      ],
      "metadata": {
        "id": "7aPsc2UMVTSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Entrainement effectif du modèle "
      ],
      "metadata": {
        "id": "Fjl9xpPIIj54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Préparation des données d'entraînement WikiNER et des fichiers de configuration requis\n",
        "\n",
        "Récupération des données d'entraînement Wikiner et prépation des fichiers de configuration : vocabulaire, étiquettes et données au format du code utilisé.\n",
        "\n",
        "Après exécution de la cellule, consulter le répertoire `data` pour y trouver les fichiers tagset, vocab et txt produits pour le système NER précédemment défini."
      ],
      "metadata": {
        "id": "h_WA1wg_JdOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data \n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/wikiner_ud.joblib.bz2 -P data\n",
        "!bzip2 -dk data/wikiner_ud.joblib.bz2\n",
        "\n",
        "# Loading the corpus \n",
        "from joblib import load\n",
        "wikiner_corpus = load('data/wikiner_ud.joblib') \n",
        "\n",
        "# Aperçu du nombre de phrases et d'une phrase annotée (liste de tokens composés de la forme, de la catégorie grammaticale et de l'étiquette BIO correspondant en l'entité nommée.\n",
        "print (len(wikiner_corpus))\n",
        "# 132257\n",
        "print (wikiner_corpus[0]) \n",
        "vocab = set()\n",
        "tagset = set()\n",
        "for s in wikiner_corpus:\n",
        "  for w,p,n in s:\n",
        "    vocab.add(w.lower())\n",
        "    tagset.add(n)\n",
        "print (tagset)\n",
        "# {'B-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-LOC', 'B-PER', 'I-PER', 'O'}\n",
        "\n",
        "# export\n",
        "with open('data/wikiner_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in wikiner_corpus:\n",
        "      sentence = list()\n",
        "      tags = list()    \n",
        "      for w,p,n in line:\n",
        "        sentence.append(w)\n",
        "        tags.append(n)\n",
        "      f.write('{}\\t{}\\n'.format(json.dumps(sentence), json.dumps(tags)))\n",
        "\n",
        "# export tagset au format bi_lstm_crf \n",
        "import json\n",
        "with open('data/wikiner_corpus_tagset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(tagset), f, ensure_ascii=False)\n",
        "\n",
        "# export vocab au format bi_lstm_crf \n",
        "with open('data/wikiner_corpus_vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(vocab), f, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "FXeEbpt4eJ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Déclaration du répertoire de données et des noms des fichiers de vocab, du jeu d'étiquettes et du corpus étiquetés. En fait les noms des repertoires des données et du modèles sont définis un peu plus bas..."
      ],
      "metadata": {
        "id": "QGLGylxgJ8WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_VOCAB = \"wikiner_corpus_vocab.json\"\n",
        "FILE_TAGS = \"wikiner_corpus_tagset.json\"\n",
        "FILE_DATASET = \"wikiner_corpus.txt\""
      ],
      "metadata": {
        "id": "cW6hifzxd4Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Déclaration du répertoire du modèle qui sera généré "
      ],
      "metadata": {
        "id": "B8ismdbZLeRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "FILE_ARGUMENTS = \"arguments.json\"\n",
        "FILE_MODEL = \"model.pth\""
      ],
      "metadata": {
        "id": "T177RCDzLXSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exécution de l'entraînement à partir de paramètres du modèle donnés\n",
        "\n"
      ],
      "metadata": {
        "id": "HeiBr7WGL3Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VOTRE TRAVAIL\n",
        "\n",
        "* Avec le type d'exécution \"cpu\", l'entraînement peut prendre quelques minutes. Regardez le temps approximatif annoncé pour 1 époque. Passez en type \"gpu\" et comparez le temps.\n",
        "* Avec les paramètres par défaut, quelle score de loss obtenez-vous pour les données de validation suite à la dernière époque d'entraînement ? Et sur les données de test ?\n",
        "\n",
        "!!! Attention, l'implémentation cherchera à charger une configuration existante dans le répertoire du modèle spécifié. Si vous changez le paramétrage alors supprimer les fichiers spécifiques au modèle ou bien spécifier un nouveau répertoire pour le nouveau modèle."
      ],
      "metadata": {
        "id": "rKMQ1SrmTTnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = dict()\n",
        "args['corpus_dir'] = \"data\"  # the corpus directory\n",
        "args['model_dir'] = \"model_wikiner_vanilla2\"       # the output directory for model files\n",
        "args['num_epoch'] = 5 # 5                # number of epoch to train\n",
        "args['lr'] = 1e-3                     # learning rate\n",
        "args['weight_decay'] = 0.             # the L2 normalization parameter\n",
        "args['batch_size'] = 1000             # batch size for training\n",
        "args['device'] = None                 # the training device: \"cuda:0\", \"cpu:0\". It will be auto-detected by default\n",
        "args['max_seq_len'] = 100 # 100              # max sequence length within training\n",
        "args['val_split'] = 0.2                  # the split for the validation dataset\n",
        "args['test_split'] = 0.2                 # the split for the testing dataset\n",
        "args['recovery'] = \"store_true\"       # continue to train from the saved model in model_dir\n",
        "args['save_best_val_model'] = \"store_true\" # save the model whose validation score is smallest\n",
        "args['embedding_dim'] = 100 # 100           # the dimension of the embedding layer\n",
        "args['hidden_dim'] = 128              # the dimension of the RNN hidden state\n",
        "args['num_rnn_layers'] = 21 # 1            # the number of RNN layers\n",
        "args['rnn_type'] =\"lstm\"              # RNN type, choice: \"lstm\", \"gru\"\n",
        "# print(args)\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train(args)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 162.1955807209015 seconds --- gpu 5 epochs max_seq_len 100 embedding_dim 100 num_rnn_layers 1 val_loss:  4.47 test_loss: 4.27"
      ],
      "metadata": {
        "id": "mZG7z_Y7YgG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prédiction effective du modèle"
      ],
      "metadata": {
        "id": "w08NdCkzR8mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Préparation des données de tests WiNER\n",
        "\n",
        "D'abord la définition de quelques méthodes utiles pour la préparation des données de tests"
      ],
      "metadata": {
        "id": "8CLRzOY3QZOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities \n",
        "def flatten(t):\n",
        "  # applatie une liste de listes en une unique liste... \n",
        "  # [[a, b], [c], [d, e, f]] -> [a, b, c, d, e, f]\n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "import re \n",
        "def normalise_labels(sentences):\n",
        "  # normalise les sorties des étiquettes NER utilisées par les différents \n",
        "  # systèmes afin de les rendre comparable\n",
        "  new_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    new_sentence = list()\n",
        "    for label in sentence:\n",
        "      if label != 'O':\n",
        "        label = re.sub('^[A-Z]-','', label)\n",
        "      new_sentence.append(label)\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n"
      ],
      "metadata": {
        "id": "WmrbXjoIQrh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Préparation des données"
      ],
      "metadata": {
        "id": "UmZ4UIFORdVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the test corpus\n",
        "!mkdir -p data\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/winer_dev.joblib -P data\n",
        "from joblib import load\n",
        "winer_corpus = load('data/winer_dev.joblib')\n",
        "\n",
        "# get the tokens of each text\n",
        "# liste chaque forme de surface de chaque mot de chaque phrase\n",
        "winer_tokens = [[token for token, pos, label in text] for text in winer_corpus]\n",
        "# liste chaque étiquette (label) de chaque mot de chaque phrase\n",
        "winer_ref = [[label for token, pos, label in text] for text in winer_corpus]\n",
        "labels = list(set(flatten(winer_ref)))\n",
        "\n",
        "#\n",
        "print ('#texts:', len(winer_corpus))\n",
        "print ('labels:', labels)\n",
        "\n",
        "print ('sample of annotated texts:', winer_corpus[0])   \n",
        "print ('sample of tokenized text:', winer_tokens[0])   "
      ],
      "metadata": {
        "id": "h8aIpI_0sIqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Définition de la méthode d'évaluation"
      ],
      "metadata": {
        "id": "ONcPIZxFQ8HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measures definition\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def results_per_class(labels, y_ref, y_hyp):\n",
        "  # Inspect per-class results in more detail:\n",
        "  sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        "  )\n",
        "  # print ('y_ref', len(y_ref), 'y_hyp', len(y_hyp), 'sorted_labels', len(sorted_labels))\n",
        "  return classification_report(flatten(y_ref), flatten(y_hyp), labels=sorted_labels, digits=3)\n",
        "\n",
        "# Il y a beaucoup plus d'entités 'O' que les autres dans le corpus, \n",
        "# mais nous sommes davantage intéressés par les autres entités. \n",
        "# Pour ne pas biaiser les scores de moyenne, on retire les étiquettes qui ne nous intéressent pas.\n",
        "print (\"before removing:\", labels)\n",
        "labels_to_remove = ['O', 'Event', 'Date', 'Hour']\n",
        "for l in labels_to_remove:\n",
        "  if l in labels: labels.remove(l)\n",
        "print (\"after removing:\", labels)"
      ],
      "metadata": {
        "id": "Bq_F2cQAQ8m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prédiction sur une phrase exemple"
      ],
      "metadata": {
        "id": "SLwtv_AAUuDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = WordsTagger(model_dir=\"model_wikiner_vanilla\")\n",
        "tags, sequences = model([['George', 'W.', 'Bush', 'fut', 'président', 'des', 'États-Unis', \"d'\", 'Amérique', '.']])  # CHAR-based model\n",
        "print(tags)  "
      ],
      "metadata": {
        "id": "ZDbE4MwVvtTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exécution de la prédiction sur les données de test"
      ],
      "metadata": {
        "id": "RW3LdyPBUymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "#from bi_lstm_crf.app import WordsTagger\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "bilstmcrf_model = WordsTagger(model_dir=\"model_wikiner_vanilla\") #_vanilla\n",
        "\n",
        "bilstmcrf_hyp = []\n",
        "# pour chaque phrase de wikiner\n",
        "for text in winer_tokens:\n",
        "    tags, sequences = bilstmcrf_model([text])    \n",
        "    bilstmcrf_hyp.append(tags[0])\n",
        "    #print (tags)\n",
        "    #break\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 40.24239158630371 seconds ---\n",
        "# --- 144.3440752029419 seconds ---\n",
        "# --- 33.92570495605469 seconds ---\n",
        "\n",
        "# normalize the hyp labels\n",
        "print (bilstmcrf_hyp[0])\n",
        "normalized_bilstmcrf_hyp = normalise_labels(bilstmcrf_hyp)\n",
        "print (normalized_bilstmcrf_hyp[0])\n",
        "print (winer_ref[0])\n",
        "\n",
        "# Evaluate on data \n",
        "print (results_per_class(labels, winer_ref, normalized_bilstmcrf_hyp))"
      ],
      "metadata": {
        "id": "Z3aL5ctwsXTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "avec les valeurs par défaut\n",
        "\n",
        "```\n",
        "running_device gpu\n",
        "--- 49.18917155265808 seconds ---\n",
        "['O', 'O', 'O', 'O', 'I-PER', 'I-MISC', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O']\n",
        "['O', 'O', 'O', 'O', 'PER', 'MISC', 'O', 'O', 'O', 'O', 'MISC', 'MISC', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'MISC', 'MISC', 'MISC', 'O', 'MISC', 'MISC', 'MISC', 'O', 'O', 'O', 'O', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'LOC', 'O', 'O', 'MISC', 'MISC', 'MISC', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'LOC', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'O', 'LOC', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O']\n",
        "['O', 'O', 'O', 'O', 'PER', 'PER', 'Date', 'Date', 'Date', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'PER', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O']\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.287     0.659     0.400      4483\n",
        "        MISC      0.012     0.625     0.024       443\n",
        "         LOC      0.341     0.502     0.406      4724\n",
        "         ORG      0.331     0.232     0.273      3816\n",
        "\n",
        "   micro avg      0.154     0.482     0.233     13466\n",
        "   macro avg      0.243     0.504     0.276     13466\n",
        "weighted avg      0.309     0.482     0.354     13466\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VWbW17grVzQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# VOTRE TRAVAIL\n",
        "\n",
        "* Jouez avec le paramétrage d'entraînement du modèle (par exemple en doublant les valeurs par défaut): nombre d'époque, taille des phrases considérées (max_seq_len), dimension des embeddings (embedding_dim), nombre de couches RNN (num_rnn_layers), type de cellule RNN (rnn_type), nombre de dimension du RNN (hidden state), le taux d'apprentissage (lr)... Déterminer l'apport de chaque paramètre. Discuter les performances en termes de précision, rappel et micro/macro-F1.\n",
        "* Dans vos expériences, rencontrez-vous des limites avec le hardware mis à disposition par gcolab ? \n",
        "* Expérimenter une modification en profondeur (au choix, d'autres sont possibles)\n",
        "  * Modifiez le code pour utiliser des _pre-trained word embeddings_. Mesurez leur apport. Expérimentez à minima le modèle [word2vec de 200 dimensions construit avec skipgram sur le corpus FrWac, et mis à disposition par Jean-Philippe Fauconnier](https://fauconnier.github.io/#data)  \n",
        "  * Ajouter les traits sur la surface des mots\n",
        "* Faire un retour sur les différents modèles que vous avez implémentés (y compris à base de CRF pur).\n",
        "* Suivant votre avancement, d'autres word embeddings peuvent être testés (e.g. glove), une architecture bi LSTM CNN CRF, voire un BERT-CRF...\n",
        "\n",
        "Ci dessous quelques pointeurs sur comment utiliser des modèles pré-entraînés avec pytorch\n",
        "* https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings\n",
        "* https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76 \n",
        "* https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f\n"
      ],
      "metadata": {
        "id": "_rEMG4w79NWt"
      }
    }
  ]
}