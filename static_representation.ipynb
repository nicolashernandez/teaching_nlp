{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/static_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEzct6qpjorn"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "L'objectif de ce notebook est de\n",
        "- construire des représentations statiques clairsemées et denses\n",
        "- observer leur utilisation sur une tâche de classification\n",
        "\n",
        "Plusieurs modélisations de données seront explorées\n",
        "1. term frequency model\n",
        "2. TF-IDF\n",
        "3. pretrained word2vec\n",
        "4. trained word2vec\n",
        "\n",
        "Dans le cadre de ce travail, vous ne travaillerez que sur un échantillon (sample) de données pour ne pas attendre trop longtemps la réalisation des traitements.\n",
        "\n",
        "\n",
        "- jouer avec les prétraitements\n",
        "- jouer avec les dimensions\n",
        "- jouer avec les vectorisations\n",
        "- obtener la meilleur performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXBCWTTCjr_o"
      },
      "source": [
        "# A bit of theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmNr9PDicuOn"
      },
      "source": [
        "## Modèle \"sac de mots\" clairsemé\n",
        "\n",
        "Le [**sac de mots** ou *BOW* pour *Bag Of Words* en anglais](https://fr.wikipedia.org/wiki/Sac_de_mots) est un modèle classique utilisé en Recherche d'Information (RI) pour représenter le contenu d'un document. Chaque document est décrit vis-à-vis d'un vocabulaire donné commun.\n",
        "\n",
        "Différentes vues sont possibles :\n",
        "- compter binairement si les mots du vocabulaire sont présents dans le document,\n",
        "- compter les occurrences des mots du vocabulaire dans le document,\n",
        "- pondérer les mots en tenant compte de leurs spécificités dans le document vis-à-vis des autres documents (on parle de pondération _tf-idf_).\n",
        "\n",
        "La **vectorisation** est le processus qui désigne transformation des textes en vecteurs (de mots selon une modélisation *bow*).\n",
        "\n",
        "On doit à [Karen Spärck Jones](https://fr.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones) la proposition de la pondération _tf-idf_ des termes.\n",
        "> « La spécificité d'un terme peut être quantifiée comme une fonction inverse du nombre de documents dans lesquels il apparaît. »\n",
        "\n",
        "[Gérard Salton](https://fr.wikipedia.org/wiki/Gerard_Salton), quant à lui, est reconnu comme étant le père de la recherche d'information en ayant proposé une modélisation des documents dans un espace vectoriel.\n",
        "\n",
        "\n",
        "* Karen Spärck Jones, « A statistical interpretation of term specificity and its application in retrieval », Journal of Documentation, vol. 28, no 1,‎ 1972, p. 11–21 (DOI 10.1108/eb026526)\n",
        "* G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing, Communications of the ACM, v.18 n.11, p. 613-620, novembre 1975\n",
        "\n",
        "Par la suite nous utiliserons le module python sklearn qui offre des facilités pour pré-traiter (normaliser) et vectoriser aisément les textes.\n",
        "\n",
        "On utilisera néanmoins un pré-traitement extérieur pour comparer différentes vectorisations concurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKxD0JXTjZvM"
      },
      "source": [
        "## Modèle dense statique\n",
        "\n",
        "Les [**plongements de mots** (_word embeddings_ en anglais)](https://fr.wikipedia.org/wiki/Word_embedding) désignent le résultat de techniques récentes de vectorisation qui produisent des vecteurs denses de dimensions réduites, prédéfinies, et non corrélées avec la taille du vocabulaire (e.g. 100, 300, 500...).\n",
        "Ces techniques reposent sur l'hypothèse distributionnelle de Harris qui veut que les mots apparaissant dans des contextes similaires ont des significations apparentées.\n",
        "\n",
        "La méthode de référence est connue sous le nom [**Word2Vec** est attribuée à Mikolov et ses collègues (Google) en 2013](https://github.com/tmikolov/word2vec).\n",
        "> Tomas Mikolov, Kai Chen, Greg Corrado et Jeffrey Dean, « Efficient Estimation of Word Representations in Vector Space », arXiv:1301.3781 [cs],‎ 16 janvier 2013\n",
        "\n",
        "Les auteurs proposent deux architectures neuronales à 2 couches [CBOW (_continuous bag of words_) et SkipGram](https://fr.wikipedia.org/wiki/Word2vec#/media/Fichier:CBOW_eta_Skipgram.png).\n",
        "\n",
        "> Le CBOW vise à prédire un mot étant donné son contexte. Skip-gram a une architecture symétrique visant à prédire les mots du contexte étant donné un mot en entrée. En pratique, le modèle CBOW est plus rapide à apprendre, mais le modèle skip-gram donne généralement de meilleurs résultats.\n",
        "\n",
        "> La couche cachée contient quelques centraines de neurones et constitue à l'issue de l'entraînement le plongement représentant un mot. La couche de sortie permet d'implémenter une tâche de classification au moyen d'une softmax.\n",
        "\n",
        "> L'apprentissage ne nécessite néanmoins aucun label, la vérité terrain étant directement déduite des données et plus particulièrement de la proximité des mots au sein du corpus d'entraînement. En ce sens, l'apprentissage de Word2vec constitue un apprentissage auto-supervisé.\n",
        "\n",
        "Les vecteurs obtenus sont dits statiques (ou non contextuels) car ils restent tel quel quelle que soit l'occurrence du mot en contexte.\n",
        "\n",
        "Un second modèle bien connu est celui de [**FastText** (Facebook)](https://github.com/facebookresearch/fastText) qui propose de traiter la variabilité morphologique des mots en construisant des vecteurs non pas pour des mots mais pour des sous-mots (séquence de caractères). Le lecteur d'un mot est la somme de tous les vecteurs des sous-mots le composant.\n",
        "\n",
        "Cette approche est indépendante de la langue, et montre de meilleurs résultats que word2vec sur des tâches syntaxiques, surtout quand le corpus d'entraînement est petit. Word2vec est légèrement meilleur pour des tâches sémantiques. Un des avantage de FastText est de pouvoir fournir des vecteurs mêmes pour les mots hors vocabulaires.\n",
        "\n",
        "Plusieurs **librairies permettent de créer, charger, sauver et manipuler des modèles de plongements de mots**. Nous allons utiliser _gensim_ qui permet aussi bien de travailler avec des modèles [word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) que [fasttext](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py).  \n",
        "\n",
        "Ici une [Comparison of FastText and Word2Vec](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NL-4rAEsmOz"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-MoKnfAdsoq2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200\n",
        "import numpy as np\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiThtk3ieMI"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Prefer to load preprocessed data, it would be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7wjXSENuddb"
      },
      "source": [
        "## Load and preprocess the IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMokGsZUhJrF"
      },
      "source": [
        "For our experiment, we use the [Large Movie Review Dataset](https://huggingface.co/datasets/stanfordnlp/imdb) from [Learning Word Vectors for Sentiment Analysis](https://aclanthology.org/P11-1015) (Maas et al., ACL 2011).\n",
        "\n",
        "The dataset was developed for binary sentiment classification task.\n",
        "\n",
        "It contains\n",
        "* a set of 25,000 highly polar movie reviews for training,\n",
        "* 25,000 for testing\n",
        "* and additional 50,000 unlabeled data for use as well.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yWRx2VViHCM",
        "outputId": "7d416c02-810c-4c37-beb7-b4cb7964adf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "6RCDJaQzf0Kx",
        "outputId": "75e19406-fe24-43d6-98e7-8dd7224e686d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1828f623d74a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimdb_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stanfordnlp/imdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimdb_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
        "imdb_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "I2kH4e5Hp4Ku",
        "outputId": "c0b40c8c-06d1-402e-a001-9b70a064e621"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"raw_train_df\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24904,\n        \"samples\": [\n          \"Although I didn't like Stanley & Iris tremendously as a film, I did admire the acting. Jane Fonda and Robert De Niro are great in this movie. I haven't always been a fan of Fonda's work but here she is delicate and strong at the same time. De Niro has the ability to make every role he portrays into acting gold. He gives a great performance in this film and there is a great scene where he has to take his father to a home for elderly people because he can't care for him anymore that will break your heart. I wouldn't really recommend this film as a great cinematic entertainment, but I will say you won't see much bette acting anywhere.\",\n          \"Yet again one of the most misunderstood Goddesses of my country has been twisted by \\\"Westerners\\\" who cannot understand the esoteric symbolism of the Mother Goddess in her dark forms. The Mother takes on the frightening form of Kali Mata to destroy our inner demons, and to terrify our egos. And though blood sacrifice is given to Kali and Durga, the events depicted in this film are just absurd. The Mother takes on a wrathful form to be wrathful to our inner demons, limitations, and ego when no other form will suffice. It's also in her wrathful form that she burns away all your Karmas in the \\\"Smashan\\\" fires that you cultivate in your heart for her to dance on if you love her, and she will bring you to reality and truth. Reality and truth has a dark side as well as light, which serves a purpose. The Mother is the embodiment of the physical Universe as well, she is Nature. Nature can be cruel and destructive to maintain balance. You cannot have growth and life without death and destruction. Kali represents the force of destruction for the purpose of new growth and life both mundane and spiritual in the universe. It's very outrageous to me that people who know nothing of India or it's divinity can just take one of our beloved Goddesses and use her like a cheap prostitute to make some low-budget, talentless horror film. How dare they take our beloved Mother and portray her as a horror that makes people chop their eyelids off!? She is only horrific to those who are attached to their ego and who live in delusion , greed, anger, and other inner-demons. It's very clear to me that the person who wrote this movie must have a very serious self-deluding ego, and serious inner-demons to see Kali as so horrible and terrible. When the ego drops away she becomes a form that is enchanting, beautiful, and young, a beauty that is so enchanting to behold that she enchants the entire Universe with it. Kali Maa is an ancient Mother, not to be trifled with for the sake of entertainment, let's just hope that in her endless compassion and mercy that she does not take on wrathful form to those involved with this movie.<br /><br />The audacity that Westerners have in using religions like my own, or the religions of the Caribbean Islands such as Santeria, and Vodou which are actually very positive, and other such religions to twist and exaggerate misunderstood elements that the Western mind cannot comprehend, is totally ridiculous. It's clear that there is no respect for what people live, breathe and believe in when it comes to these kind of flicks.<br /><br />Kali Maa in reality is a caring and compassionate mother, whom we shed tears at her beautiful feet in devotion and love for. And I am happy that my Mother takes on wrathful form sometimes to protect her devotees from themselves and from outside forces.<br /><br />Many Praises to the REAL Kali Maa, who has shown MANY the path of God and realization.\",\n          \"Paris is the place to be to enjoy beautiful art and music, and to fall madly in love - as is the case in this film. Boy meets girl, they fall in love, but something stands in their way of eternal happiness, the classic story.<br /><br />The wonderful music of George Gerschwin complements the great dancing by Gene Kelly and Leslie Caron. \\\"An American in Paris\\\" is a humorous, light-hearted, loving film well worth watching.<br /><br />8/10<br /><br />\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "raw_train_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-46ecc097-3898-4da5-b886-6f6d2ba9bf58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.&lt;br /&gt;&lt;br /&gt;One might feel virtuous for sitting thru it because it touc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.&lt;br /&gt;&lt;br /&gt;The film has two strong elements and those are, (1) the realistic acting (2) the i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..&lt;br /&gt;&lt;br /&gt;\"Is that all there is??\" ...I was just an early teen when this s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>A hit at the time but now better categorised as an Australian cult film. The humour is broad, unsubtle and, in the final scene where a BBC studio fire is extinguished by urinating on it, crude. Co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>I love this movie like no other. Another time I will try to explain its virtues to the uninitiated, but for the moment let me quote a few of pieces the remarkable dialogue, which, please remember,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>This film and it's sequel Barry Mckenzie holds his own, are the two greatest comedies to ever be produced. A great story a young Aussie bloke travels to england to claim his inheritance and meets ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>'The Adventures Of Barry McKenzie' started life as a satirical comic strip in 'Private Eye', written by Barry Humphries and based on an idea by Peter Cook. McKenzie ( 'Bazza' to his friends ) is a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46ecc097-3898-4da5-b886-6f6d2ba9bf58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-46ecc097-3898-4da5-b886-6f6d2ba9bf58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-46ecc097-3898-4da5-b886-6f6d2ba9bf58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cc942ede-6639-459b-b54d-5a419590e0ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc942ede-6639-459b-b54d-5a419590e0ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cc942ede-6639-459b-b54d-5a419590e0ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2248e61d-6f84-409d-8b7e-5f5754b3e67c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('raw_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2248e61d-6f84-409d-8b7e-5f5754b3e67c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('raw_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                          text  \\\n",
              "0      I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if i...   \n",
              "1      \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim t...   \n",
              "2      If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touc...   \n",
              "3      This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the i...   \n",
              "4      Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this s...   \n",
              "...                                                                                                                                                                                                        ...   \n",
              "24995  A hit at the time but now better categorised as an Australian cult film. The humour is broad, unsubtle and, in the final scene where a BBC studio fire is extinguished by urinating on it, crude. Co...   \n",
              "24996  I love this movie like no other. Another time I will try to explain its virtues to the uninitiated, but for the moment let me quote a few of pieces the remarkable dialogue, which, please remember,...   \n",
              "24997  This film and it's sequel Barry Mckenzie holds his own, are the two greatest comedies to ever be produced. A great story a young Aussie bloke travels to england to claim his inheritance and meets ...   \n",
              "24998  'The Adventures Of Barry McKenzie' started life as a satirical comic strip in 'Private Eye', written by Barry Humphries and based on an idea by Peter Cook. McKenzie ( 'Bazza' to his friends ) is a...   \n",
              "24999  The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours the...   \n",
              "\n",
              "       label  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "...      ...  \n",
              "24995      1  \n",
              "24996      1  \n",
              "24997      1  \n",
              "24998      1  \n",
              "24999      1  \n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_train_df = imdb_dataset['train'].to_pandas()\n",
        "raw_test_df = imdb_dataset['test'].to_pandas()\n",
        "raw_unsupervised_df = imdb_dataset['unsupervised'].to_pandas()\n",
        "\n",
        "raw_train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snGBplFjvs_S"
      },
      "source": [
        "define data cleaning method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84C9guJ0ipD3",
        "outputId": "1cfe0e1f-493e-4778-d14a-e765fa81a4ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# optional\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "non_English_character_compiled_pattern = re.compile('[^a-zA-Z]')\n",
        "\n",
        "def preprocess (raw_text):\n",
        "  # remove non English character\n",
        "  preprocessed_text = re.sub(non_English_character_compiled_pattern,' ',raw_text)\n",
        "  # lower casing\n",
        "  preprocessed_text = preprocessed_text.lower()\n",
        "  # split as a list of words\n",
        "  preprocessed_text = preprocessed_text.split()\n",
        "  # remove stopwords\n",
        "  preprocessed_text = [word for word in preprocessed_text if not word in stopwords.words('english')]\n",
        "  # stemming\n",
        "  # preprocessed_text = [ps.stem(word) for word in preprocessed_text]\n",
        "  # join words again to form the text\n",
        "  preprocessed_text = ' '.join(preprocessed_text)\n",
        "  return preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH4uET8t2ibG"
      },
      "source": [
        "preprocess training data\n",
        "\n",
        "```\n",
        "100%|██████████| 25000/25000 [12:31<00:00, 33.27it/s]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPjV0wsDtIXm"
      },
      "outputs": [],
      "source": [
        "# preprocess X\n",
        "X_train = []\n",
        "for raw_text in tqdm(raw_train_df['text']):\n",
        "  X_train.append(preprocess(raw_text))\n",
        "X_train_np = np.array(X_train)\n",
        "\n",
        "# Y\n",
        "Y_train = list(raw_train_df['label'])\n",
        "Y_train_np = np.array(Y_train)\n",
        "\n",
        "\n",
        "# to backup and visualize\n",
        "train_df = pd.DataFrame({'text': X_train, 'label': Y_train})\n",
        "train_df.to_csv('imdb_train.csv', index=False)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71n5cU6X2b6r"
      },
      "source": [
        "preprocess test data\n",
        "\n",
        "```\n",
        "100%|██████████| 25000/25000 [14:27<00:00, 28.83it/s]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ol2ponLx3H6",
        "outputId": "7e39bb60-777e-4408-b6f6-ec1cd35426b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [14:27<00:00, 28.83it/s]\n"
          ]
        }
      ],
      "source": [
        "# preprocess X\n",
        "X_test = []\n",
        "for raw_text in tqdm(raw_test_df['text']):\n",
        "  X_test.append(preprocess(raw_text))\n",
        "X_test_np = np.array(X_test)\n",
        "\n",
        "# Y\n",
        "Y_test = list(raw_test_df['label'])\n",
        "Y_test_np = np.array(Y_test)\n",
        "\n",
        "# to backup and visualize\n",
        "test_df = pd.DataFrame({'text': X_test, 'label': Y_test})\n",
        "test_df.to_csv('imdb_test.csv', index=False)\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTDOECzW2YLQ"
      },
      "source": [
        "preprocess unsupervised data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxVqqc84ZrKG"
      },
      "source": [
        "\n",
        "```\n",
        "100%|██████████| 50000/50000 [23:22<00:00, 35.64it/s]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkJiJMDcSL42",
        "outputId": "7f18132e-2cf7-4326-d93e-d4026fdde83c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_dataset['unsupervised']['label'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tn0ViYu2XPu"
      },
      "outputs": [],
      "source": [
        "#\n",
        "X_unsupervised = []\n",
        "for raw_text in tqdm(raw_unsupervised_df['text']):\n",
        "  X_unsupervised.append(preprocess(raw_text))\n",
        "\n",
        "#\n",
        "X_unsupervised_np = np.array(X_unsupervised)\n",
        "\n",
        "#\n",
        "Y_unsupervised = list(raw_unsupervised_df['label'])\n",
        "\n",
        "#\n",
        "test_df = pd.DataFrame({'text': X_unsupervised, 'label': Y_unsupervised})\n",
        "test_df.to_csv('imdb_unsupervised.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUzwK7rY7PNq"
      },
      "source": [
        "## Load preprocessed dataset\n",
        "\n",
        "Import manually the files in the local file sytem, then run the following code.\n",
        "\n",
        "The data has been preprocessed with the `preprocess` method without stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "HiMYiO9H7PVE",
        "outputId": "627f55f0-5533-4d2a-8c00-d7e4ac4f1928"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                          text  \\\n",
              "0      rented curious yellow video store controversy surrounded first released also heard first seized u customs ever tried enter country therefore fan films considered controversial really see br br plo...   \n",
              "1      curious yellow risible pretentious steaming pile matter one political views film hardly taken seriously level claim frontal male nudity automatic nc true seen r rated films male nudity granted off...   \n",
              "2      avoid making type film future film interesting experiment tells cogent story br br one might feel virtuous sitting thru touches many important issues without discernable motive viewer comes away n...   \n",
              "3      film probably inspired godard masculin f minin urge see film instead br br film two strong elements realistic acting impressive undeservedly good photo apart strikes endless stream silliness lena ...   \n",
              "4      oh brother hearing ridiculous film umpteen years think old peggy lee song br br early teen smoked fish hit u young get theater although manage sneak goodbye columbus screening local film museum be...   \n",
              "...                                                                                                                                                                                                        ...   \n",
              "24995  hit time better categorised australian cult film humour broad unsubtle final scene bbc studio fire extinguished urinating crude contains every cliche traditional australian pilgrimage old country ...   \n",
              "24996  love movie like another time try explain virtues uninitiated moment let quote pieces remarkable dialogue please remember tongue cheek aussies poms understand everyone else well br br title song ly...   \n",
              "24997  film sequel barry mckenzie holds two greatest comedies ever produced great story young aussie bloke travels england claim inheritance meets mates loveable innocent br br chock block full great say...   \n",
              "24998  adventures barry mckenzie started life satirical comic strip private eye written barry humphries based idea peter cook mckenzie bazza friends lanky loud hat wearing australian whose two main inter...   \n",
              "24999  story centers around barry mckenzie must go england wishes claim inheritance grossest aussie shearer ever set foot outside great nation something culture clash much fun games ensue songs barry mck...   \n",
              "\n",
              "       label  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "...      ...  \n",
              "24995      1  \n",
              "24996      1  \n",
              "24997      1  \n",
              "24998      1  \n",
              "24999      1  \n",
              "\n",
              "[25000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-714785d0-b13a-4f06-aecf-a317b3e46742\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rented curious yellow video store controversy surrounded first released also heard first seized u customs ever tried enter country therefore fan films considered controversial really see br br plo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>curious yellow risible pretentious steaming pile matter one political views film hardly taken seriously level claim frontal male nudity automatic nc true seen r rated films male nudity granted off...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>avoid making type film future film interesting experiment tells cogent story br br one might feel virtuous sitting thru touches many important issues without discernable motive viewer comes away n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>film probably inspired godard masculin f minin urge see film instead br br film two strong elements realistic acting impressive undeservedly good photo apart strikes endless stream silliness lena ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>oh brother hearing ridiculous film umpteen years think old peggy lee song br br early teen smoked fish hit u young get theater although manage sneak goodbye columbus screening local film museum be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>hit time better categorised australian cult film humour broad unsubtle final scene bbc studio fire extinguished urinating crude contains every cliche traditional australian pilgrimage old country ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>love movie like another time try explain virtues uninitiated moment let quote pieces remarkable dialogue please remember tongue cheek aussies poms understand everyone else well br br title song ly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>film sequel barry mckenzie holds two greatest comedies ever produced great story young aussie bloke travels england claim inheritance meets mates loveable innocent br br chock block full great say...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>adventures barry mckenzie started life satirical comic strip private eye written barry humphries based idea peter cook mckenzie bazza friends lanky loud hat wearing australian whose two main inter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>story centers around barry mckenzie must go england wishes claim inheritance grossest aussie shearer ever set foot outside great nation something culture clash much fun games ensue songs barry mck...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-714785d0-b13a-4f06-aecf-a317b3e46742')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-714785d0-b13a-4f06-aecf-a317b3e46742 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-714785d0-b13a-4f06-aecf-a317b3e46742');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53a8d655-eb32-41f3-bec0-7c14849ab057\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53a8d655-eb32-41f3-bec0-7c14849ab057')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53a8d655-eb32-41f3-bec0-7c14849ab057 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0c19abd1-e2f4-47e1-96ee-464d161b5f39\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0c19abd1-e2f4-47e1-96ee-464d161b5f39 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24901,\n        \"samples\": [\n          \"film yawn titles credits boring point tedium acting wooden stilted admittedly director richard jobson directing debut earth green lit script poorly developed one looks like another money drain government project scottish screen credited surprise surprise nearly fell asleep three times review unfortunately restrained one please please mister jobson ever prior directing sedative film go back\",\n          \"delectable fusion new age babble luridly bad film making may open borrow one film favorite verbs might leave jaw slack belly sore laughter retching based best selling book james redfield first self published cornucopia kitsch tracks spiritual awakening american history teacher matthew settle traveling deepest darkest phoniest peru sniffing either air something else illegal namely discovers schlock shangri la populated smiling zombies may nuts heavily medicated perhaps often accompanied panpipe flourish occasional shout celestial choir although lot talk energy quality decidedly missing motley cast whose numbers include thomas kretschmann annabeth gish hector elizondo jurgen prochnow firmly ensconced camp pantheon care plot involves military terrorists roman catholic church armand mastroianni provided inept direction mr redfield barnet bain dan gordon wrote hoot script short easily worst film seen years viewing movies\",\n          \"movie portrayed trailer comedy extreme tragedy left sick stomach hated think want make movie like man enough reflect true intentions movie trailer would seen movie would known think trailer reflect theme intentions movie tired really wanted fun comedy extremely disappointed several days still bad taste mouth movie never disappointed movie ever written comment bad movie really think true deception involved trailer showed true intention movie one would seen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#\n",
        "train_df = pd.read_csv('imdb_train.csv')\n",
        "train_df = train_df.dropna(axis=0)\n",
        "train_df['label'] = train_df['label'].astype(int)\n",
        "X_train = list(train_df['text'])\n",
        "Y_train = list(train_df['label'])\n",
        "\n",
        "#\n",
        "test_df = pd.read_csv('imdb_test.csv')\n",
        "#print(test_df.shape)\n",
        "#print(test_df.isnull().sum())\n",
        "test_df = test_df.dropna(axis=0)\n",
        "test_df['label'] = test_df['label'].astype(int)\n",
        "#print(test_df.shape)\n",
        "X_test = list(test_df['text'])\n",
        "Y_test = list(test_df['label'])\n",
        "\n",
        "#\n",
        "X_train_np = np.array(X_train)\n",
        "Y_train_np = np.array(Y_train)\n",
        "X_test_np = np.array(X_test)\n",
        "Y_test_np = np.array(Y_test)\n",
        "#X_unsupervised_np = np.array(X_unsupervised)\n",
        "\n",
        "#\n",
        "unsupervised_df = pd.read_csv('imdb_unsupervised.csv')\n",
        "X_unsupervised = list(unsupervised_df['text'])\n",
        "#Y_unsupervised = list(unsupervised_df['label'])\n",
        "\n",
        "#\n",
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample\n",
        "\n",
        "In order to make the process faster, work on a sample"
      ],
      "metadata": {
        "id": "1pQfuq2gF8F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a sample then split it into training and testing sets\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sample_data = resample(train_df, n_samples = 1000, random_state = 42)\n",
        "print (sample_data.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(sample_data['text'], sample_data['label'], test_size=0.2, random_state=45 )\n",
        "X_train_np = np.array(X_train)\n",
        "Y_train_np = np.array(Y_train)\n",
        "X_test_np = np.array(X_test)\n",
        "Y_test_np = np.array(Y_test)\n",
        "\n",
        "# FURTHER  perform your own preprocess on the raw dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIWlx6fvF8SJ",
        "outputId": "065cf3c9-ac5e-40fc-dad6-912f34ca63e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print ('Y_train distribution', list(Counter(Y_train).items()))\n",
        "print ('Y_test distribution', list(Counter(Y_test).items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZhT5AlAKMGo",
        "outputId": "1deae6a1-bf47-4dd8-d2be-fbb9fb88a36d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_train distribution [(0, 388), (1, 412)]\n",
            "Y_test distribution [(1, 92), (0, 108)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkr1RKkDipqo"
      },
      "source": [
        "# Approach 1: term frequency model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBUAsxjCKEe9"
      },
      "source": [
        "Le code suivant réalise une vectorisation du corpus en comptant les occurrences des mots. La vectorisation prend en charge la construction d'un vocabulaire sur le corpus ainsi que quelques pré-traitements de normalisation linguistiques. Enfin le résultat de la vectorisation est affiché sous la forme d'une matrice **document-terme** (les documents sont en ligne et les mots en colonne).\n",
        "Nous aurons une matrice de dimension _nb de documents * taille du vocabulaire_.\n",
        "\n",
        "Here we limit the number of dimensions to ```max_features=1000```.\n",
        "\n",
        "Do not hesite to have a look at all the parameters of [CountVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "rJlCIP-3r_Vi",
        "outputId": "590fc9ef-ab86-4ed9-9444-c00f7f0c7f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(max_features=1000)\n",
            "Matrix dimensions: (800, 1000)\n",
            "Matrix dimensions: (200, 1000)\n",
            "Vocabulary size: 1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     ability  able  absolutely  accent  across  act  acting  action  actor  \\\n",
              "0          0     0           1       0       1    0       0       0      0   \n",
              "1          0     0           0       0       0    0       0       0      0   \n",
              "2          0     0           0       0       0    0       0       1      0   \n",
              "3          0     0           0       0       0    0       0       2      0   \n",
              "4          0     0           0       0       0    0       0       0      0   \n",
              "..       ...   ...         ...     ...     ...  ...     ...     ...    ...   \n",
              "795        0     0           0       0       0    0       0       0      0   \n",
              "796        0     0           1       0       2    0       1       0      0   \n",
              "797        0     0           0       0       0    0       0       0      0   \n",
              "798        0     0           0       0       0    0       0       4      0   \n",
              "799        0     0           0       0       0    0       1       0      0   \n",
              "\n",
              "     actors  ...  writing  written  wrong  year  years  yes  yet  york  young  \\\n",
              "0         0  ...        0        0      0     0      0    0    0     0      2   \n",
              "1         0  ...        0        0      1     0      0    0    0     0      0   \n",
              "2         0  ...        0        0      0     0      0    0    1     0      0   \n",
              "3         2  ...        0        0      0     0      0    0    0     0      0   \n",
              "4         0  ...        0        0      0     0      0    0    0     0      0   \n",
              "..      ...  ...      ...      ...    ...   ...    ...  ...  ...   ...    ...   \n",
              "795       0  ...        0        0      0     0      0    0    0     0      0   \n",
              "796       0  ...        0        0      0     0      0    0    0     0      0   \n",
              "797       0  ...        0        0      0     0      0    0    0     0      0   \n",
              "798       0  ...        0        0      0     0      0    0    0     0      0   \n",
              "799       0  ...        0        0      0     0      0    0    0     0      0   \n",
              "\n",
              "     zombie  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  \n",
              "..      ...  \n",
              "795       0  \n",
              "796       0  \n",
              "797       0  \n",
              "798       0  \n",
              "799       0  \n",
              "\n",
              "[800 rows x 1000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13efe8b0-fb28-49c0-b08c-69a4f1e487d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accent</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actors</th>\n",
              "      <th>...</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>zombie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 1000 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13efe8b0-fb28-49c0-b08c-69a4f1e487d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13efe8b0-fb28-49c0-b08c-69a4f1e487d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13efe8b0-fb28-49c0-b08c-69a4f1e487d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5dd8d28c-01bd-4049-a317-2913c92b4efe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5dd8d28c-01bd-4049-a317-2913c92b4efe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5dd8d28c-01bd-4049-a317-2913c92b4efe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# define the vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words= None, max_features=1000) #stop_words='english')\n",
        "\n",
        "# display the configuration of the vectorizer\n",
        "print (count_vectorizer)\n",
        "\n",
        "# Learn the vocabulary (fit) and transform documents to document-term matrix (i.e. perform the vectorization)\n",
        "X_train_counter_matrix = count_vectorizer.fit_transform(X_train_np)\n",
        "print ('Matrix dimensions:', X_train_counter_matrix.get_shape())\n",
        "count_vectorizer.transform\n",
        "\n",
        "# Transform documents to document-term matrix (i.e. perform the vectorization) by using Vocabulary learned\n",
        "X_test_counter_matrix = count_vectorizer.transform(X_test_np)\n",
        "print ('Matrix dimensions:', X_test_counter_matrix.get_shape())\n",
        "\n",
        "# get all unique words in the corpus (the vocabulary and also the names of the matrix columns/features)\n",
        "vocab = count_vectorizer.get_feature_names_out() # .get_feature_names() # for sklearn_version >= 1.0\n",
        "print ('Vocabulary size:', len(vocab))\n",
        "\n",
        "# show document-term matrix\n",
        "X_train_counter_matrix_array = X_train_counter_matrix.toarray()\n",
        "pd.DataFrame(X_train_counter_matrix_array, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsTaiOQZvytd"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kqbtU5O9vy25"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB_counter_model = MultinomialNB().fit(X_train_counter_matrix, Y_train_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8FxZeANv2wk"
      },
      "source": [
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Kjy7onedv24Y"
      },
      "outputs": [],
      "source": [
        "Y_test_prediction = MNB_counter_model.predict(X_test_counter_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUGEht3v7OF"
      },
      "source": [
        "evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwt7SFzQv7XD",
        "outputId": "4c02fe3a-d1aa-41c4-b007-ec1f7b471f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.79\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.80      0.80       108\n",
            "           1       0.77      0.78      0.77        92\n",
            "\n",
            "    accuracy                           0.79       200\n",
            "   macro avg       0.79      0.79      0.79       200\n",
            "weighted avg       0.79      0.79      0.79       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(accuracy_score(Y_test,Y_test_prediction))\n",
        "print(classification_report(Y_test,Y_test_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIuWsNULPULG"
      },
      "source": [
        "# Approach 2: TF-IDF model\n",
        "\n",
        "La vectorisation avec occurrences présente des limites lorsqu'elle est utilisée sur de large corpus. En effet, le modèle présume que l'importance des mots est fonction de sa fréquence et qu'un mot plus fréquent qu'un autre dans un document est plus discriminant que l'autre. Le problème intervient quand un mot fréquent, supposé important, apparaît dans plusieurs documents. Le fait qu'il apparaisse dans plusieurs documents peut au final le rendre moins discriminant que d'autres pourtant moins fréquents. Le problème vient du fait que l'on prenne des valeurs absolues.\n",
        "\n",
        "Le modèle **TF-IDF** vise à solutionner ce problème en normalisant le compte des occurrences. TF-IDF correspond à _Term Frequency-Inverse Document Frequency_.\n",
        "\n",
        "On définit le TF-IDF comme suit: `tfidf = tf x idf`\n",
        "* `tfidf(w, D)` est le score TF-IDF du mot `w` dans le document `D`\n",
        "* `tf(w, D)` représente le nombre d'occurrence du terme `w` dans le document `D`\n",
        "* `idf(w, D)` représente la fréquence inverse documentaire du terme `w`, qui peut être calculée comme le log du nombre total de documents dans le corpus `C` divisé par la fréquence documentaire du terme `w` (i.e. le nombre de documents du corpus `C` dans lequel le terme `w` se produit).\n",
        "\n",
        "\n",
        "Here the definition of the [TfidfVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFOpYvgwywo",
        "outputId": "1998ff15-fa62-4b5e-a7aa-bccbe3436617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.78      0.82       108\n",
            "           1       0.76      0.85      0.80        92\n",
            "\n",
            "    accuracy                           0.81       200\n",
            "   macro avg       0.81      0.81      0.81       200\n",
            "weighted avg       0.81      0.81      0.81       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# define the vectorizer\n",
        "tfidf_vectorizer = # TODO\n",
        "\n",
        "# Learn the vocabulary (fit) and transform documents to document-term matrix (i.e. perform the vectorization)\n",
        "X_train_tfidf_matrix = # TODO\n",
        "X_test_tfidf_matrix = # TODO\n",
        "\n",
        "# train\n",
        "# TODO\n",
        "\n",
        "# predict\n",
        "# TODO\n",
        "\n",
        "# evaluate\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuUDZHuYjSy6"
      },
      "source": [
        "# Approach 3: existing static dense models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aW9RPQRD32D"
      },
      "source": [
        "## Load existing pretrain models\n",
        "\n",
        "Le dépôt [gensim-data](https://github.com/RaRe-Technologies/gensim-data) contient quelques corpus et modèles pré-entraînés librement disponibles.\n",
        "\n",
        "Jean-Philippe Fauconnier met des [modèles word2vec à disposition pour le français ](https://fauconnier.github.io/#data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L23OV9ZgblQp",
        "outputId": "9b7c68c5-096a-45c7-c124-d991aa4470e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"corpora\": {\n",
            "        \"semeval-2016-2017-task3-subtaskBC\": {\n",
            "            \"num_records\": -1,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 6344358,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py\",\n",
            "            \"license\": \"All files released for the task are free for general research use\",\n",
            "            \"fields\": {\n",
            "                \"2016-train\": [\n",
            "                    \"...\"\n",
            "                ],\n",
            "                \"2016-dev\": [\n",
            "                    \"...\"\n",
            "                ],\n",
            "                \"2017-test\": [\n",
            "                    \"...\"\n",
            "                ],\n",
            "                \"2016-test\": [\n",
            "                    \"...\"\n",
            "                ]\n",
            "            },\n",
            "            \"description\": \"SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section \\u201cPapers\\u201d of https://github.com/RaRe-Technologies/gensim-data/issues/18.\",\n",
            "            \"checksum\": \"701ea67acd82e75f95e1d8e62fb0ad29\",\n",
            "            \"file_name\": \"semeval-2016-2017-task3-subtaskBC.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://alt.qcri.org/semeval2017/task3/\",\n",
            "                \"http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf\",\n",
            "                \"https://github.com/RaRe-Technologies/gensim-data/issues/18\",\n",
            "                \"https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"semeval-2016-2017-task3-subtaskA-unannotated\": {\n",
            "            \"num_records\": 189941,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 234373151,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py\",\n",
            "            \"license\": \"These datasets are free for general research use.\",\n",
            "            \"fields\": {\n",
            "                \"THREAD_SEQUENCE\": \"\",\n",
            "                \"RelQuestion\": {\n",
            "                    \"RELQ_CATEGORY\": \"question category, according to the Qatar Living taxonomy\",\n",
            "                    \"RELQ_DATE\": \"date of posting\",\n",
            "                    \"RELQ_ID\": \"question indentifier\",\n",
            "                    \"RELQ_USERID\": \"identifier of the user asking the question\",\n",
            "                    \"RELQ_USERNAME\": \"name of the user asking the question\",\n",
            "                    \"RelQBody\": \"body of question\",\n",
            "                    \"RelQSubject\": \"subject of question\"\n",
            "                },\n",
            "                \"RelComments\": [\n",
            "                    {\n",
            "                        \"RelCText\": \"text of answer\",\n",
            "                        \"RELC_USERID\": \"identifier of the user posting the comment\",\n",
            "                        \"RELC_ID\": \"comment identifier\",\n",
            "                        \"RELC_USERNAME\": \"name of the user posting the comment\",\n",
            "                        \"RELC_DATE\": \"date of posting\"\n",
            "                    }\n",
            "                ]\n",
            "            },\n",
            "            \"description\": \"SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.\",\n",
            "            \"checksum\": \"2de0e2f2c4f91c66ae4fcf58d50ba816\",\n",
            "            \"file_name\": \"semeval-2016-2017-task3-subtaskA-unannotated.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://alt.qcri.org/semeval2016/task3/\",\n",
            "                \"http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf\",\n",
            "                \"https://github.com/RaRe-Technologies/gensim-data/issues/18\",\n",
            "                \"https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"patent-2017\": {\n",
            "            \"num_records\": 353197,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 3087262469,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py\",\n",
            "            \"license\": \"not found\",\n",
            "            \"description\": \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
            "            \"checksum-0\": \"818501f0b9af62d3b88294d86d509f8f\",\n",
            "            \"checksum-1\": \"66c05635c1d3c7a19b4a335829d09ffa\",\n",
            "            \"file_name\": \"patent-2017.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://patents.reedtech.com/pgrbft.php\"\n",
            "            ],\n",
            "            \"parts\": 2\n",
            "        },\n",
            "        \"quora-duplicate-questions\": {\n",
            "            \"num_records\": 404290,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 21684784,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py\",\n",
            "            \"license\": \"probably https://www.quora.com/about/tos\",\n",
            "            \"fields\": {\n",
            "                \"question1\": \"the full text of each question\",\n",
            "                \"question2\": \"the full text of each question\",\n",
            "                \"qid1\": \"unique ids of each question\",\n",
            "                \"qid2\": \"unique ids of each question\",\n",
            "                \"id\": \"the id of a training set question pair\",\n",
            "                \"is_duplicate\": \"the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise\"\n",
            "            },\n",
            "            \"description\": \"Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.\",\n",
            "            \"checksum\": \"d7cfa7fbc6e2ec71ab74c495586c6365\",\n",
            "            \"file_name\": \"quora-duplicate-questions.gz\",\n",
            "            \"read_more\": [\n",
            "                \"https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"wiki-english-20171001\": {\n",
            "            \"num_records\": 4924894,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 6516051717,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py\",\n",
            "            \"license\": \"https://dumps.wikimedia.org/legal.html\",\n",
            "            \"fields\": {\n",
            "                \"section_texts\": \"list of body of sections\",\n",
            "                \"section_titles\": \"list of titles of sections\",\n",
            "                \"title\": \"Title of wiki article\"\n",
            "            },\n",
            "            \"description\": \"Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`\",\n",
            "            \"checksum-0\": \"a7d7d7fd41ea7e2d7fa32ec1bb640d71\",\n",
            "            \"checksum-1\": \"b2683e3356ffbca3b6c2dca6e9801f9f\",\n",
            "            \"checksum-2\": \"c5cde2a9ae77b3c4ebce804f6df542c2\",\n",
            "            \"checksum-3\": \"00b71144ed5e3aeeb885de84f7452b81\",\n",
            "            \"file_name\": \"wiki-english-20171001.gz\",\n",
            "            \"read_more\": [\n",
            "                \"https://dumps.wikimedia.org/enwiki/20171001/\"\n",
            "            ],\n",
            "            \"parts\": 4\n",
            "        },\n",
            "        \"text8\": {\n",
            "            \"num_records\": 1701,\n",
            "            \"record_format\": \"list of str (tokens)\",\n",
            "            \"file_size\": 33182058,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py\",\n",
            "            \"license\": \"not found\",\n",
            "            \"description\": \"First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.\",\n",
            "            \"checksum\": \"68799af40b6bda07dfa47a32612e5364\",\n",
            "            \"file_name\": \"text8.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://mattmahoney.net/dc/textdata.html\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"fake-news\": {\n",
            "            \"num_records\": 12999,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 20102776,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py\",\n",
            "            \"license\": \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
            "            \"fields\": {\n",
            "                \"crawled\": \"date the story was archived\",\n",
            "                \"ord_in_thread\": \"\",\n",
            "                \"published\": \"date published\",\n",
            "                \"participants_count\": \"number of participants\",\n",
            "                \"shares\": \"number of Facebook shares\",\n",
            "                \"replies_count\": \"number of replies\",\n",
            "                \"main_img_url\": \"image from story\",\n",
            "                \"spam_score\": \"data from webhose.io\",\n",
            "                \"uuid\": \"unique identifier\",\n",
            "                \"language\": \"data from webhose.io\",\n",
            "                \"title\": \"title of story\",\n",
            "                \"country\": \"data from webhose.io\",\n",
            "                \"domain_rank\": \"data from webhose.io\",\n",
            "                \"author\": \"author of story\",\n",
            "                \"comments\": \"number of Facebook comments\",\n",
            "                \"site_url\": \"site URL from BS detector\",\n",
            "                \"text\": \"text of story\",\n",
            "                \"thread_title\": \"\",\n",
            "                \"type\": \"type of website (label from BS detector)\",\n",
            "                \"likes\": \"number of Facebook likes\"\n",
            "            },\n",
            "            \"description\": \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
            "            \"checksum\": \"5e64e942df13219465927f92dcefd5fe\",\n",
            "            \"file_name\": \"fake-news.gz\",\n",
            "            \"read_more\": [\n",
            "                \"https://www.kaggle.com/mrisdal/fake-news\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"20-newsgroups\": {\n",
            "            \"num_records\": 18846,\n",
            "            \"record_format\": \"dict\",\n",
            "            \"file_size\": 14483581,\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py\",\n",
            "            \"license\": \"not found\",\n",
            "            \"fields\": {\n",
            "                \"topic\": \"name of topic (20 variant of possible values)\",\n",
            "                \"set\": \"marker of original split (possible values 'train' and 'test')\",\n",
            "                \"data\": \"\",\n",
            "                \"id\": \"original id inferred from folder name\"\n",
            "            },\n",
            "            \"description\": \"The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.\",\n",
            "            \"checksum\": \"c92fd4f6640a86d5ba89eaad818a9891\",\n",
            "            \"file_name\": \"20-newsgroups.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://qwone.com/~jason/20Newsgroups/\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"__testing_matrix-synopsis\": {\n",
            "            \"description\": \"[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.\",\n",
            "            \"checksum\": \"1767ac93a089b43899d54944b07d9dc5\",\n",
            "            \"file_name\": \"__testing_matrix-synopsis.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis\"\n",
            "            ],\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"__testing_multipart-matrix-synopsis\": {\n",
            "            \"description\": \"[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.\",\n",
            "            \"checksum-0\": \"c8b0c7d8cf562b1b632c262a173ac338\",\n",
            "            \"checksum-1\": \"5ff7fc6818e9a5d9bc1cf12c35ed8b96\",\n",
            "            \"checksum-2\": \"966db9d274d125beaac7987202076cba\",\n",
            "            \"file_name\": \"__testing_multipart-matrix-synopsis.gz\",\n",
            "            \"read_more\": [\n",
            "                \"http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis\"\n",
            "            ],\n",
            "            \"parts\": 3\n",
            "        }\n",
            "    },\n",
            "    \"models\": {\n",
            "        \"fasttext-wiki-news-subwords-300\": {\n",
            "            \"num_records\": 999999,\n",
            "            \"file_size\": 1005007116,\n",
            "            \"base_dataset\": \"Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py\",\n",
            "            \"license\": \"https://creativecommons.org/licenses/by-sa/3.0/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 300\n",
            "            },\n",
            "            \"description\": \"1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\",\n",
            "            \"read_more\": [\n",
            "                \"https://fasttext.cc/docs/en/english-vectors.html\",\n",
            "                \"https://arxiv.org/abs/1712.09405\",\n",
            "                \"https://arxiv.org/abs/1607.01759\"\n",
            "            ],\n",
            "            \"checksum\": \"de2bb3a20c46ce65c9c131e1ad9a77af\",\n",
            "            \"file_name\": \"fasttext-wiki-news-subwords-300.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"conceptnet-numberbatch-17-06-300\": {\n",
            "            \"num_records\": 1917247,\n",
            "            \"file_size\": 1225497562,\n",
            "            \"base_dataset\": \"ConceptNet, word2vec, GloVe, and OpenSubtitles 2016\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py\",\n",
            "            \"license\": \"https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 300\n",
            "            },\n",
            "            \"description\": \"ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\",\n",
            "            \"read_more\": [\n",
            "                \"http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972\",\n",
            "                \"https://github.com/commonsense/conceptnet-numberbatch\",\n",
            "                \"http://conceptnet.io/\"\n",
            "            ],\n",
            "            \"checksum\": \"fd642d457adcd0ea94da0cd21b150847\",\n",
            "            \"file_name\": \"conceptnet-numberbatch-17-06-300.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"word2vec-ruscorpora-300\": {\n",
            "            \"num_records\": 184973,\n",
            "            \"file_size\": 208427381,\n",
            "            \"base_dataset\": \"Russian National Corpus (about 250M words)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py\",\n",
            "            \"license\": \"https://creativecommons.org/licenses/by/4.0/deed.en\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 300,\n",
            "                \"window_size\": 10\n",
            "            },\n",
            "            \"description\": \"Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\",\n",
            "            \"preprocessing\": \"The corpus was lemmatized and tagged with Universal PoS\",\n",
            "            \"read_more\": [\n",
            "                \"https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models\",\n",
            "                \"http://rusvectores.org/en/\",\n",
            "                \"https://github.com/RaRe-Technologies/gensim-data/issues/3\"\n",
            "            ],\n",
            "            \"checksum\": \"9bdebdc8ae6d17d20839dd9b5af10bc4\",\n",
            "            \"file_name\": \"word2vec-ruscorpora-300.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"word2vec-google-news-300\": {\n",
            "            \"num_records\": 3000000,\n",
            "            \"file_size\": 1743563840,\n",
            "            \"base_dataset\": \"Google News (about 100 billion words)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py\",\n",
            "            \"license\": \"not found\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 300\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
            "            \"read_more\": [\n",
            "                \"https://code.google.com/archive/p/word2vec/\",\n",
            "                \"https://arxiv.org/abs/1301.3781\",\n",
            "                \"https://arxiv.org/abs/1310.4546\",\n",
            "                \"https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"a5e5354d40acb95f9ec66d5977d140ef\",\n",
            "            \"file_name\": \"word2vec-google-news-300.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-wiki-gigaword-50\": {\n",
            "            \"num_records\": 400000,\n",
            "            \"file_size\": 69182535,\n",
            "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 50\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"c289bc5d7f2f02c6dc9f2f9b67641813\",\n",
            "            \"file_name\": \"glove-wiki-gigaword-50.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-wiki-gigaword-100\": {\n",
            "            \"num_records\": 400000,\n",
            "            \"file_size\": 134300434,\n",
            "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 100\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"40ec481866001177b8cd4cb0df92924f\",\n",
            "            \"file_name\": \"glove-wiki-gigaword-100.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-wiki-gigaword-200\": {\n",
            "            \"num_records\": 400000,\n",
            "            \"file_size\": 264336934,\n",
            "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 200\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"59652db361b7a87ee73834a6c391dfc1\",\n",
            "            \"file_name\": \"glove-wiki-gigaword-200.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-wiki-gigaword-300\": {\n",
            "            \"num_records\": 400000,\n",
            "            \"file_size\": 394362229,\n",
            "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 300\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"29e9329ac2241937d55b852e8284e89b\",\n",
            "            \"file_name\": \"glove-wiki-gigaword-300.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-twitter-25\": {\n",
            "            \"num_records\": 1193514,\n",
            "            \"file_size\": 109885004,\n",
            "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 25\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"50db0211d7e7a2dcd362c6b774762793\",\n",
            "            \"file_name\": \"glove-twitter-25.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-twitter-50\": {\n",
            "            \"num_records\": 1193514,\n",
            "            \"file_size\": 209216938,\n",
            "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 50\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"c168f18641f8c8a00fe30984c4799b2b\",\n",
            "            \"file_name\": \"glove-twitter-50.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-twitter-100\": {\n",
            "            \"num_records\": 1193514,\n",
            "            \"file_size\": 405932991,\n",
            "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 100\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"b04f7bed38756d64cf55b58ce7e97b15\",\n",
            "            \"file_name\": \"glove-twitter-100.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"glove-twitter-200\": {\n",
            "            \"num_records\": 1193514,\n",
            "            \"file_size\": 795373100,\n",
            "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
            "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py\",\n",
            "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
            "            \"parameters\": {\n",
            "                \"dimension\": 200\n",
            "            },\n",
            "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
            "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.\",\n",
            "            \"read_more\": [\n",
            "                \"https://nlp.stanford.edu/projects/glove/\",\n",
            "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
            "            ],\n",
            "            \"checksum\": \"e52e8392d1860b95d5308a525817d8f9\",\n",
            "            \"file_name\": \"glove-twitter-200.gz\",\n",
            "            \"parts\": 1\n",
            "        },\n",
            "        \"__testing_word2vec-matrix-synopsis\": {\n",
            "            \"description\": \"[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\",\n",
            "            \"parameters\": {\n",
            "                \"dimensions\": 50\n",
            "            },\n",
            "            \"preprocessing\": \"Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.\",\n",
            "            \"read_more\": [],\n",
            "            \"checksum\": \"534dcb8b56a360977a269b7bfc62d124\",\n",
            "            \"file_name\": \"__testing_word2vec-matrix-synopsis.gz\",\n",
            "            \"parts\": 1\n",
            "        }\n",
            "    }\n",
            "}\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as gensim_api\n",
        "\n",
        "# show info about available models/datasets\n",
        "print(json.dumps(gensim_api.info(), indent=4))\n",
        "\n",
        "# load a model\n",
        "#w2v_model = gensim_api.load(\"glove-twitter-25\")\n",
        "w2v_model = gensim_api.load(\"glove-twitter-50\")\n",
        "#w2v_model = gensim_api.load(\"glove-twitter-100\")\n",
        "#w2v_model = gensim_api.load(\"glove-wiki-gigaword-100\")\n",
        "#w2v_model = gensim_api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNTxexplsc0p"
      },
      "source": [
        "## Obtenir les mots similaires\n",
        "\n",
        "Pour chaque question ci-dessous, jouez le jeu et prenez le temps faire des propositions de réponses avant d'exécuter le code qui permettra de consulter la connaissance du modèle et connaître ce qu'il répondrait."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NneaJ99LwgB0"
      },
      "source": [
        "Si je vous dis 'roi', vous pensez à quoi ? Faire quelques propositions de synonymes ou de mots substituables sémantiquement proches. La méthode `most_similar` affichera les 10 mots les plus proches d'un mot donné, du plus similaire au moins similaire, avec pour chacun un score de similarité avec le mot donné (scores décroissants donc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTQ2GmzhsdaT"
      },
      "outputs": [],
      "source": [
        "# obtenir les mots similaires à 1 mot\n",
        "w2v_model.most_similar(\"king\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Kz9RfpxT3R"
      },
      "source": [
        "Si je vous demande de me donner des mots relatifs à des 'palais' et 'paris', à quoi pensez-vous ? Pour information, la méthode accepte une liste de mots en paramètres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uxDAKRdtUQ6"
      },
      "outputs": [],
      "source": [
        "# obtenir les mots similaires relatifs à une liste\n",
        "w2v_model.most_similar(['berlin', 'paris'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfifn3_1vhCf"
      },
      "source": [
        "Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ? Répondez avant d'exécuter le code ci-dessous.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDV5EriBtOAV"
      },
      "outputs": [],
      "source": [
        "# Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ?\n",
        "w2v_model.most_similar(positive = ['king', 'woman'], negative = ['man'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1S5RZkrv1Sh"
      },
      "source": [
        "Même question mais si j'ajoute les vecteurs de 'paris' et de 'japon' et que je retire le vecteur de 'france'.\n",
        "\n",
        "Trouver des exemples qui montrent les connaissances de genre, de nombre du modèles\n",
        "* tree + grape - apple =\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3tDW4NMwG5b"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trouver des exemples d'opérations (e.g. `computer programmer - man + worman =  housekeeper ?`) et de mots similaires  qui montrent des biais dans le modèles (e.g. quels sont les mots similaires de `gay` de `awful`?).\n",
        "\n",
        "TODO\n"
      ],
      "metadata": {
        "id": "OHwhBn4limEa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY9Io_UjzESH"
      },
      "source": [
        "## Visualiser les plongements lexicaux dans un graph en 2D\n",
        "\n",
        "Pour ce faire, il faut transformer les vecteurs de n dimensions à des vecteurs à 2 dimensions. La réduction des dimensions est effectuée à l'aide d'une [analyse en composantes principales (ACP ou PCA pour _Principal Component Analysis_ en anglais)](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KogxHwaszDVm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('ggplot')\n",
        "#words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "#words = ['roi', 'reine']\n",
        "# soit une liste de mots à projeter\n",
        "words = ['palais', 'église', 'cathédrale', 'monastère', 'route', 'train', 'bateau', 'calèche', 'voiture', 'armée', 'soldat', 'bataille']\n",
        "wvs = model[words]\n",
        "\n",
        "# Application de la transformation PCA qui réduit les vecteurs à 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "#np.set_printoptions(suppress=True)\n",
        "P = pca.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "# Affichage\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.scatter(P[:, 0], P[:, 1], c='lightgreen', edgecolors='g')\n",
        "for label, x, y in zip(labels, P[:, 0], P[:, 1]):\n",
        "    plt.annotate(label, xy=(x*1.05, y*1.05), xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-K7O0aq0U_4"
      },
      "source": [
        "* Est-ce que les synonymes se retrouvent bien dans les mêmes zones spatiales ? Vous pouvez tester avec d'autres listes de mots. TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzUeK8RlV4Ud"
      },
      "source": [
        "## Visualiser les plongements lexicaux en 3D dynamique à l'aide du _projector de tensorflow_\n",
        "\n",
        "Sur https://radimrehurek.com/gensim/scripts/word2vec2tensor.html, on peut lire comment convertir un modèle w2v (éventuellement produit par gensim) en modèle tsv, puis comment le visualiser avec le projector de tensorflow :\n",
        "\n",
        "1. Convert your word-vector with word2vec2tensor method ou le script gensim.scripts.word2vec2tensor\n",
        "2. Open http://projector.tensorflow.org/\n",
        "3. Click “Load Data” button from the left menu.\n",
        "4. Select “Choose file” in “Load a TSV file of vectors.” and choose “/tmp/my_model_prefix_tensor.tsv” file.\n",
        "5.  Select “Choose file” in “Load a TSV file of metadata.” and choose “/tmp/my_model_prefix_metadata.tsv” file.\n",
        "6. ???\n",
        "7. PROFIT!\n",
        "\n",
        "Le code ci-dessous définit des fonctions de conversion au format de tensorflow soit depuis le format gensim-w2v soit le format w2v binaire natif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCIzCPsHl8RJ"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.scripts.word2vec2tensor import word2vec2tensor\n",
        "\n",
        "def convert_gensim_w2v_to_w2v (gensim_w2v_in_path, w2v_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from gensim_w2v format to w2v (orginal) format\n",
        "  \"\"\"\n",
        "  w2v_model = KeyedVectors.load(gensim_w2v_in_path)\n",
        "  vectors = w2v_model.wv\n",
        "  # save memory\n",
        "  # del model\n",
        "\n",
        "  # The trained word vectors can also be stored/loaded from a format compatible\n",
        "  # with the original word2vec implementation via Word2Vec.wv.save_word2vec_format\n",
        "  # and gensim.models.keyedvectors.KeyedVectors.load_word2vec_format().\n",
        "  vectors.save_word2vec_format(w2v_out_path, binary = True)\n",
        "\n",
        "def convert_w2v_to_tsv (w2v_in_path, tsv_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from w2v original format to tsv format\n",
        "  \"\"\"\n",
        "  # When running word2vec2tensor with a file resulting from\n",
        "  # save_word2vec_format, we obtain the following error:\n",
        "  # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 0: invalid start byte\n",
        "  # To solve the issue, I have to load with load_word2vec_format the saved file\n",
        "  # and save it again with save_word2vec_format\n",
        "  w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_in_path, binary=True, unicode_errors='ignore')\n",
        "  w2v_model.wv.save_word2vec_format(w2v_in_path+\".tmp\", binary = True)\n",
        "  word2vec2tensor(w2v_in_path+\".tmp\", tsv_out_path,  binary = True)\n",
        "\n",
        "def convert_gensim_w2v_to_tsv (gensim_w2v_in_path, tsv_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from gensim w2v format to tsv format\n",
        "  \"\"\"\n",
        "  convert_gensim_w2v_to_w2v (gensim_w2v_in_path, gensim_w2v_in_path+\".tmp\")\n",
        "  convert_w2v_to_tsv (gensim_w2v_in_path+\".tmp\", tsv_out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFb7NDjszD-f"
      },
      "source": [
        "Convertissons le modèle public récupéré"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNHxotHYzENQ",
        "outputId": "b322f8f1-b09d-4c96-8bd5-a844ac86b270"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ],
      "source": [
        "convert_w2v_to_tsv(w2v_model_path, w2v_model_path+\".tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AsqSRYCrDgq"
      },
      "source": [
        "\n",
        "Télécharger les 3? fichiers produits et chargez les dans projector tensorflow. Si c'est trop compliqué, le projector vient avec des modèles préchargés.\n",
        "\n",
        "* Observez-vous des zones plus denses que d'autres ? Qu'est ce que cela peut vouloir signifier ?\n",
        "* Testez les labels 3D, de cliquer sur un point/mot (fixer le voisinage à la valeur minimale) pour observer l'illumination d'une zone, chercher un mot, visualiser 'isolate 6 points'.\n",
        "* Testez aussi un des tensors found disponible comme Word2Vec 10K ou all.\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fo1FcwYfeCt"
      },
      "source": [
        "## Comparer et évaluer deux modèles\n",
        "\n",
        "[`gensim` implémente la comparaison de modèles](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb) selon la tâche de **raisonnement analogique** telle que décrite à la [section 4.1 du papier de 2013 de Mikolov et al.](https://arxiv.org/pdf/1301.3781v3.pdf).\n",
        "\n",
        "```\n",
        ":capital-common-countries\n",
        "Athens Greece Baghdad Iraq\n",
        "Athens Greece Bangkok Thailand\n",
        "...\n",
        ":capital-world\n",
        "Algiers Algeria Baghdad Iraq\n",
        "Ankara Turkey Dublin Ireland\n",
        "...\n",
        ": city-in-state\n",
        "Chicago Illinois Houston Texas\n",
        "Chicago Illinois Philadelphia Pennsylvania\n",
        "...\n",
        ": gram1-adjective-to-adverb\n",
        "amazing amazingly apparent apparently\n",
        "amazing amazingly calm calmly\n",
        "...\n",
        "```\n",
        "\n",
        "D'autres évaluations intrinsèques sont possibles comme le [calcul d'un coefficient de corrélation entre un taux de similarité calculée sur la base d'une appréciation humaine et un score de similarité cosinus entre des représentations Word2Vec](https://nlp-ensae.github.io/materials/course2/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI_ZHPkjiday"
      },
      "source": [
        "Ci-dessous nous mettons en oeuvre la tâche de raisonnement analogique de Mikolov et al."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7WhKE0_fglD",
        "outputId": "4e2f5e4d-c90e-4169-cf7a-96fa64ae589b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-04 07:45:22--  https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "questions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-04 07:45:23 (3.85 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the file questions-words.txt to be used for comparing word embeddings\n",
        "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC2EoBqyiwbr"
      },
      "outputs": [],
      "source": [
        "# un oeil sur les n premières lignes du fichier\n",
        "!head questions-words.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcqh-0Ni8Ms"
      },
      "source": [
        "Définition de la méthode de calcul de la performance de resolution de la tâche d'analogie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbiByLHzi8y1"
      },
      "outputs": [],
      "source": [
        "def print_accuracy(model, questions_file):\n",
        "    print('Evaluating...\\n')\n",
        "    acc = model.accuracy(questions_file)\n",
        "    #acc = model.wv.evaluate_word_analogies(questions_file)\n",
        "\n",
        "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
        "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
        "    sem_acc = 100*float(sem_correct)/sem_total\n",
        "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
        "\n",
        "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
        "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
        "    syn_acc = 100*float(syn_correct)/syn_total\n",
        "    print('Morphologic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
        "    return (sem_acc, syn_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fULOXjn8jOK_"
      },
      "source": [
        "Exécution de l'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw8fHge1jIw7"
      },
      "outputs": [],
      "source": [
        "#\n",
        "word_analogies_file = 'questions-words.txt'\n",
        "\n",
        "print('\\nLoading Word2Vec embeddings')\n",
        "w2v_model = KeyedVectors.load(w2v_model_path)\n",
        "print('Accuracy for Word2Vec:')\n",
        "print_accuracy(w2v_model, word_analogies_file)\n",
        "\n",
        "print('\\nLoading FastText embeddings')\n",
        "ft_model = KeyedVectors.load(ft_model_path)\n",
        "print('Accuracy for FastText (with n-grams):')\n",
        "print_accuracy(ft_model, word_analogies_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11hzoM9koW2"
      },
      "source": [
        "\n",
        "TODO\n",
        "\n",
        "* Lequel des deux modèles donnent les meilleurs résultats sur l'analyse morphologique ? Sur l'analyse sémantique ? Est-ce cohérent de ce que vous connaissez des modèles ?\n",
        "* Relancez la construction des modèles puis leur comparaison. Obtenez-vous les mêmes scores de performance ? Pourquoi ?\n",
        "* Les données d'entraînement sont des romans classiques issus de la collection Gutenberg. Si les données avaient été issues de la Wikipedia, quels résultats auraient pu changer ? Si vous souhaitez tester, ci-dessous je vous donne un snippet de code qui récupère une version normalisée de la wikipédia et qui lance la contruction des modèles w2v et ft. Cela prendra qqs minutes...\n",
        "* Selon vous, dans une perspective de comparaison de modèles, est-ce important de construire ceux-ci sur les mêmes données ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wfDsIPk8oiq"
      },
      "outputs": [],
      "source": [
        "# WARNING: ce qui suit est optionnel !\n",
        "\n",
        "# récupération d'un corpus normalisé de la wikipédia-en\n",
        "# une string tokenisée avec le caractère espace de mots pleins\n",
        "!mkdir data\n",
        "!wget -nc http://mattmahoney.net/dc/text8.zip -P data\n",
        "!unzip data/text8.zip -d data\n",
        "\n",
        "text8_path = 'data/text8'\n",
        "\n",
        "# Text8Corpus class for reading space-separated words file\n",
        "from gensim.models.word2vec import Text8Corpus\n",
        "\n",
        "# Construction des modèles w2v et ft avec text8\n",
        "%time w2v_model = Word2Vec(Text8Corpus(text8_path), **params)\n",
        "%time ft_model = FastText(Text8Corpus(text8_path), **params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDd3/POCnrzKzC6YGWpt1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}