{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_représentation vectorielle continue.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcwhwDLZX4MJ75l0GrH5fL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/04_repr%C3%A9sentation_vectorielle_continue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKxD0JXTjZvM"
      },
      "source": [
        "# Représentation vectorielle continue \n",
        "\n",
        "Les [**plongements de mots** (_word embeddings_ en anglais)](https://fr.wikipedia.org/wiki/Word_embedding) désignent le résultat de techniques récentes de vectorisation qui produisent des vecteurs denses de dimensions réduites, prédéfinies, et non corrélées avec la taille du vocabulaire (e.g. 100, 300, 500...). \n",
        "Ces techniques reposent sur l'hypothèse distributionnelle de Harris qui veut que les mots apparaissant dans des contextes similaires ont des significations apparentées. \n",
        "\n",
        "La méthode de référence est connue sous le nom [**Word2Vec** est attribuée à Mikolov et ses collègues (Google) en 2013](https://github.com/tmikolov/word2vec).\n",
        "> Tomas Mikolov, Kai Chen, Greg Corrado et Jeffrey Dean, « Efficient Estimation of Word Representations in Vector Space », arXiv:1301.3781 [cs],‎ 16 janvier 2013\n",
        "\n",
        "Les auteurs proposent deux architectures neuronales à 2 couches [CBOW (_continuous bag of words_) et SkipGram](https://fr.wikipedia.org/wiki/Word2vec#/media/Fichier:CBOW_eta_Skipgram.png).\n",
        "\n",
        "> Le CBOW vise à prédire un mot étant donné son contexte. Skip-gram a une architecture symétrique visant à prédire les mots du contexte étant donné un mot en entrée. En pratique, le modèle CBOW est plus rapide à apprendre, mais le modèle skip-gram donne généralement de meilleurs résultats.\n",
        "\n",
        "> La couche cachée contient quelques centraines de neurones et constitue à l'issue de l'entraînement le plongement représentant un mot. La couche de sortie permet d'implémenter une tâche de classification au moyen d'une softmax.\n",
        "\n",
        "> L'apprentissage ne nécessite néanmoins aucun label, la vérité terrain étant directement déduite des données et plus particulièrement de la proximité des mots au sein du corpus d'entraînement. En ce sens, l'apprentissage de Word2vec constitue un apprentissage auto-supervisé. \n",
        "\n",
        "Les vecteurs obtenus sont dits statiques (ou non contextuels) car ils restent tel quel quelle que soit l'occurrence du mot en contexte.\n",
        "\n",
        "Un second modèle bien connu est celui de [**FastText** (Facebook)](https://github.com/facebookresearch/fastText) qui propose de traiter la variabilité morphologique des mots en construisant des vecteurs non pas pour des mots mais pour des sous-mots (séquence de caractères). Le lecteur d'un mot est la somme de tous les vecteurs des sous-mots le composant. \n",
        "\n",
        "Cette approche est indépendante de la langue, et montre de meilleurs résultats que word2vec sur des tâches syntaxiques, surtout quand le corpus d'entraînement est petit. Word2vec est légèrement meilleur pour des tâches sémantiques. Un des avantage de FastText est de pouvoir fournir des vecteurs mêmes pour les mots hors vocabulaires.\n",
        "\n",
        "Plusieurs **librairies permettent de créer, charger, sauver et manipuler des modèles de plongements de mots**. Nous allons utiliser _gensim_ qui permet aussi bien de travailler avec des modèles [word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) que [fasttext](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py).  \n",
        "\n",
        "Ici une [Comparison of FastText and Word2Vec](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1UAG8aqs52c"
      },
      "source": [
        "## Utiliser un modèle word2vec existant avec gensim \n",
        "\n",
        "Le dépôt [gensim-data](https://github.com/RaRe-Technologies/gensim-data) contient quelques corpus et modèles pré-entraînés librement disponibles. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niTFE-e0q6xz"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Exécutez le code ci-dessous ou consulter le lien gensim-data et indiquez le nom d'un modèle construit sur la base de tweets si cela existe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YToqA7TYmZWz"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "api.info()  # show info about available models/datasets\n",
        "# model = api.load(\"glove-twitter-25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR0U0UhfnCiC"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehusd1BGmaVk"
      },
      "source": [
        "Nous allons testé un modèle construit sur le **français**. Il ne fait pas partie de ceux de la gensim-data mais il fait parti des [modèles mis à disposition par Jean-Philippe Fauconnier](https://fauconnier.github.io/#data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Urd1Uc6ohlx"
      },
      "source": [
        "### Récupération et chargement d'un modèle pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DeEOGwrsRg"
      },
      "source": [
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin -P model\n",
        "!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin -P model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFyhPoUIsQk5"
      },
      "source": [
        "# Chargement du modèle\n",
        "from gensim.models import KeyedVectors\n",
        "#model = KeyedVectors.load_word2vec_format(\"model/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\", binary=True, unicode_errors=\"ignore\")\n",
        "model = KeyedVectors.load_word2vec_format(\"model/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\", binary=True, unicode_errors=\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNTxexplsc0p"
      },
      "source": [
        "### Obtenir les mots similaires \n",
        "\n",
        "Pour chaque question ci-dessous, jouez le jeu et prenez le temps faire des propositions de réponses avant d'exécuter le code qui permettra de consulter la connaissance du modèle et connaître ce qu'il répondrait."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NneaJ99LwgB0"
      },
      "source": [
        "Si je vous dis 'roi', vous pensez à quoi ? Faire quelques propositions de synonymes ou de mots substituables sémantiquement proches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQ2GmzhsdaT"
      },
      "source": [
        "# obtenir les mots similaires à 1 mot\n",
        "model.most_similar(\"roi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Kz9RfpxT3R"
      },
      "source": [
        "Si je vous demande de me donner des mots relatifs à des 'palais' et 'paris', à quoi pensez-vous ? Pour information, la méthode accepte une liste de mots en paramètres. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uxDAKRdtUQ6"
      },
      "source": [
        "# obtenir les mots similaires relatifs à une liste\n",
        "model.most_similar(['palais', 'paris'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfifn3_1vhCf"
      },
      "source": [
        "Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ? Répondez avant d'exécuter le code ci-dessous.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDV5EriBtOAV"
      },
      "source": [
        "# Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ?\n",
        "model.most_similar(positive = ['roi', 'femme'], negative = ['homme'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1S5RZkrv1Sh"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Même question mais si j'ajoute les vecteurs de 'paris' et de 'japon' et que je retire le vecteur de 'france'. Faites une proposition et écrivez le code pour vérifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAv_BML1Ji-w"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3tDW4NMwG5b"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGnMqJr0w23I"
      },
      "source": [
        "#### QUESTION\n",
        "* Que pensez-vous de la \"puissance\" de ce type de modèle ? Vos propositions se trouvaient-elles parmis les suggestions du modèle ?\n",
        "* Reprenez ces cas d'usage et faites quelques expériences pour essayer de piéger le modèle. Quelles erreurs ou limites trouvez-vous ? Donnez les codes qui vous conduisent à vos conclusions.\n",
        "\n",
        "Au passage, si vous êtes \"joueurs\" de jeux de société, je vous recommande [Just one word](https://www.letempledujeu.fr/zoom61293.html). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UkU7YsaxLbT"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urxNEHJUbzkc"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY9Io_UjzESH"
      },
      "source": [
        "### Visualiser les plongements lexicaux dans un graph en 2D\n",
        "\n",
        "Pour ce faire, il faut transformer les vecteurs de n dimensions à des vecteurs à 2 dimensions. La réduction des dimensions est effectuée à l'aide d'une [analyse en composantes principales (ACP ou PCA pour _Principal Component Analysis_ en anglais)](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KogxHwaszDVm"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('ggplot')\n",
        "#words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "#words = ['roi', 'reine']\n",
        "# soit une liste de mots à projeter\n",
        "words = ['palais', 'église', 'cathédrale', 'monastère', 'route', 'train', 'bateau', 'calèche', 'voiture', 'armée', 'soldat', 'bataille']\n",
        "wvs = model[words]\n",
        "\n",
        "# Application de la transformation PCA qui réduit les vecteurs à 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "#np.set_printoptions(suppress=True)\n",
        "P = pca.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "# Affichage \n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.scatter(P[:, 0], P[:, 1], c='lightgreen', edgecolors='g')\n",
        "for label, x, y in zip(labels, P[:, 0], P[:, 1]):\n",
        "    plt.annotate(label, xy=(x*1.05, y*1.05), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-K7O0aq0U_4"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Est-ce que les synonymes se retrouvent bien dans les mêmes zones spatiales ? Vous pouvez tester avec d'autres listes de mots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "507NKhVH0Taq"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvZq8LXc6Mj"
      },
      "source": [
        "## Construire un modèle word2vec et fasttext avec gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjuUMhvdXhq"
      },
      "source": [
        "Word2Vec et FastText prennent tous deux un corpus normalisé segmenté en phrases et tokenisés en mots. \n",
        "\n",
        "On pourrait très bien utiliser spaCy ou nltk pour ce faire, mais ce type de pré-traitement prend \"un peu de temps\". On va directement utilisé un corpus de la base nltk disponible avec la segmentation en phrases et la tokenization mots.\n",
        "\n",
        "Le code ci-dessous utilise le sélection du corpus gutenberg segmentée en phrases et en tokens par nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ4817rGdX-w"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk_gutenberg_corpus = list()\n",
        "word_counter = 0\n",
        "\n",
        "for fileid in nltk.corpus.gutenberg.fileids():\n",
        "  segmented_and_tokenized_doc = nltk.corpus.gutenberg.sents(fileid)\n",
        "  for sent in segmented_and_tokenized_doc:\n",
        "    words = [re.sub(r'[^a-zA-Z\\s]', '', word, re.I|re.A).lower() for word in sent]\n",
        "    # 98552 2621785\n",
        "    words = [word for word in words if len(word) > 3]\n",
        "    # 95804 1154977\n",
        "    if len(words)>0: \n",
        "      nltk_gutenberg_corpus.append(words)\n",
        "      word_counter += len(words)\n",
        "      \n",
        "print ('sentences_len:', len(nltk_gutenberg_corpus), 'words_len:', word_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI2LwobJfVqO"
      },
      "source": [
        "Hyper-paramètres les plus communs du constructeur de la [classe Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) :\n",
        "\n",
        "* `corpus`: List of tokenized sentences \n",
        "* `size` : Dimensionality of the word vectors (default: 100)\n",
        "* `window` : Maximum distance between the current and predicted word within a sentence\n",
        "* `sg` : Training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "* `iter` :  Number of iterations (epochs) over the corpus\n",
        "* `workers` Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUUD2EMae1y9"
      },
      "source": [
        "# Set values for various parameters\n",
        "lr = 0.05   # Learning rate\n",
        "dim = 100   # Word vector dimensionality  \n",
        "ws = 5      # Context window size    \n",
        "epoch = 5\n",
        "minCount = 5 # Minimum word count \n",
        "neg = 5\n",
        "loss = 'ns'\n",
        "t = 1e-4\n",
        "#sample = 1e-3   # Downsample setting for frequent words\n",
        "sg=1 \n",
        "\n",
        "params = {\n",
        "    'alpha': lr,\n",
        "    'size': dim,\n",
        "    'window': ws,\n",
        "    'iter': epoch,\n",
        "    'min_count': minCount,\n",
        "    'sample': t,\n",
        "    'sg': 1,\n",
        "    'hs': 0,\n",
        "    'negative': neg\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1R9Ee81fRw7"
      },
      "source": [
        "Construction des modèles. Observez la rapidité compte tenu du nombre de mots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvJPnIjme657"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "# \n",
        "%time w2v_model = Word2Vec(nltk_gutenberg_corpus, **params) \n",
        "\n",
        "# save the model\n",
        "!mkdir -p models\n",
        "w2v_model_path = 'models/w2v_nltk-gutenberg_100_5_5_sg.bin'\n",
        "w2v_model.save(w2v_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_SvnXRufQk5"
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "#\n",
        "%time ft_model = FastText(nltk_gutenberg_corpus, **params)\n",
        "\n",
        "# save the model\n",
        "!mkdir -p models\n",
        "ft_model_path = 'models/ft_nltk-gutenberg_100_5_5_sg.bin'\n",
        "ft_model.save(ft_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzUeK8RlV4Ud"
      },
      "source": [
        "### Visualiser es plongements lexicaux en 3D dynamique à l'aide du _projector de tensorflow_\n",
        "\n",
        "Sur https://radimrehurek.com/gensim/scripts/word2vec2tensor.html, on peut lire comment convertir un modèle w2v (éventuellement produit par gensim) en modèle tsv, puis comment le visualiser avec le projector de tensorflow : \n",
        "\n",
        "1. Convert your word-vector with word2vec2tensor method ou le script gensim.scripts.word2vec2tensor \n",
        "2. Open http://projector.tensorflow.org/\n",
        "3. Click “Load Data” button from the left menu.\n",
        "4. Select “Choose file” in “Load a TSV file of vectors.” and choose “/tmp/my_model_prefix_tensor.tsv” file.\n",
        "5.  Select “Choose file” in “Load a TSV file of metadata.” and choose “/tmp/my_model_prefix_metadata.tsv” file.\n",
        "6. ???\n",
        "7. PROFIT!\n",
        "\n",
        "Le code ci-dessous définit une fonction de conversion au format de tensorflow puis la met en application pour le modèle w2v construit sur la collection gutenberg. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCIzCPsHl8RJ"
      },
      "source": [
        "import gensim\n",
        "from gensim.scripts.word2vec2tensor import word2vec2tensor\n",
        "\n",
        "def convert_w2v_to_tsv (w2v_model, w2v_out_path, tsv_out_path):\n",
        "  vectors = w2v_model.wv\n",
        "  # save memory\n",
        "  #del model\n",
        "\n",
        "  # The trained word vectors can also be stored/loaded from a format compatible\n",
        "  # with the original word2vec implementation via Word2Vec.wv.save_word2vec_format \n",
        "  # and gensim.models.keyedvectors.KeyedVectors.load_word2vec_format().\n",
        "  vectors.save_word2vec_format(w2v_out_path, binary = True)\n",
        "\n",
        "  # When running word2vec2tensor with a file resulting from save_word2vec_format,\n",
        "  # we obtain the following error:\n",
        "  # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 0: invalid start byte\n",
        "  # To solve the issue, I have to load with load_word2vec_format the saved file \n",
        "  # and save it again with save_word2vec_format\n",
        "  w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_out_path, binary=True, unicode_errors='ignore')   \n",
        "  w2v_model.wv.save_word2vec_format(w2v_out_path, binary = True)\n",
        "  word2vec2tensor(w2v_out_path, tsv_out_path,  binary = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqJSbyzWpefJ"
      },
      "source": [
        "w2v_model_path = 'models/w2v_nltk-gutenberg_100_5_5_sg.bin'\n",
        "\n",
        "tensor_filename = 'model/tensor_nltk-gutenberg_100_5_5_sg.tsv'\n",
        "\n",
        "convert_w2v_to_tsv(w2v_model, w2v_model_path, tensor_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AsqSRYCrDgq"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Observez-vous des zones plus denses que d'autres ? Qu'est ce que cela peut vouloir signifier ? \n",
        "* Testez les labels 3D, de cliquer sur un point/mot (fixer le voisinage à la valeur minimale) pour observer l'illumination d'une zone, chercher un mot, visualiser 'isolate 6 points'. \n",
        "* Testez aussi un des tensors found disponible comme Word2Vec 10K ou all.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7ghTpGvAP2"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fo1FcwYfeCt"
      },
      "source": [
        "## Comparer et évaluer deux modèles\n",
        "\n",
        "[`gensim` implémente la comparaison de modèles](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb) selon la tâche de **raisonnement analogique** telle que décrite à la [section 4.1 du papier de 2013 de Mikolov et al.](https://arxiv.org/pdf/1301.3781v3.pdf).\n",
        "\n",
        "```\n",
        ":capital-common-countries\n",
        "Athens Greece Baghdad Iraq\n",
        "Athens Greece Bangkok Thailand\n",
        "...\n",
        ":capital-world\n",
        "Algiers Algeria Baghdad Iraq\n",
        "Ankara Turkey Dublin Ireland\n",
        "...\n",
        ": city-in-state\n",
        "Chicago Illinois Houston Texas\n",
        "Chicago Illinois Philadelphia Pennsylvania\n",
        "...\n",
        ": gram1-adjective-to-adverb\n",
        "amazing amazingly apparent apparently\n",
        "amazing amazingly calm calmly\n",
        "...\n",
        "```\n",
        "\n",
        "D'autres évaluations intrinsèques sont possibles comme le [calcul d'un coefficient de corrélation entre un taux de similarité calculée sur la base d'une appréciation humaine et un score de similarité cosinus entre des représentations Word2Vec](https://nlp-ensae.github.io/materials/course2/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI_ZHPkjiday"
      },
      "source": [
        "Ci-dessous nous mettons en oeuvre la tâche de raisonnement analogique de Mikolov et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7WhKE0_fglD"
      },
      "source": [
        "# download the file questions-words.txt to be used for comparing word embeddings\n",
        "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC2EoBqyiwbr"
      },
      "source": [
        "# un oeil sur les n premières lignes du fichier\n",
        "!head questions-words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcqh-0Ni8Ms"
      },
      "source": [
        "Définition de la méthode de calcul de la performance de resolution de la tâche d'analogie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbiByLHzi8y1"
      },
      "source": [
        "def print_accuracy(model, questions_file):\n",
        "    print('Evaluating...\\n')\n",
        "    acc = model.accuracy(questions_file)\n",
        "    #acc = model.wv.evaluate_word_analogies(questions_file)\n",
        "\n",
        "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
        "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
        "    sem_acc = 100*float(sem_correct)/sem_total\n",
        "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
        "    \n",
        "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
        "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
        "    syn_acc = 100*float(syn_correct)/syn_total\n",
        "    print('Morphologic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
        "    return (sem_acc, syn_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fULOXjn8jOK_"
      },
      "source": [
        "Exécution de l'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw8fHge1jIw7"
      },
      "source": [
        "#\n",
        "word_analogies_file = 'questions-words.txt'\n",
        "\n",
        "print('\\nLoading Word2Vec embeddings')\n",
        "w2v_model = KeyedVectors.load(w2v_model_path)\n",
        "print('Accuracy for Word2Vec:')\n",
        "print_accuracy(w2v_model, word_analogies_file)\n",
        "\n",
        "print('\\nLoading FastText embeddings')\n",
        "ft_model = KeyedVectors.load(ft_model_path)\n",
        "print('Accuracy for FastText (with n-grams):')\n",
        "print_accuracy(ft_model, word_analogies_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11hzoM9koW2"
      },
      "source": [
        "#### QUESTION\n",
        "* Lequel des deux modèles donnent les meilleurs résultats sur l'analyse morphologique ? Sur l'analyse sémantique ? Est-ce cohérent de ce que vous connaissez des modèles ?   \n",
        "* Selon vous, dans une perspective de comparaison de modèles, est-ce important de construire ceux-ci sur les mêmes données ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42nxk1zldic"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzrX5PKl62WQ"
      },
      "source": [
        "## Construire une représentation continue de document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x1z2XpF9Vim"
      },
      "source": [
        "> Une approche naïve de la _représentation du sens d'un document revient à faire la moyenne des vecteurs des mots qui le composent_. [Le and Mikolov in 2014](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) \n",
        "\n",
        "L'API de spaCY permet aisément d'accéder aux embeddings des mots d'un document. Elle propose aussi une représentation vectorielle des documents sous la forme de la moyenne des vecteurs de mots qui les composent. L'API permet aussi de calculer des similarités entre ces vecteurs (mot/mot, document/document, document/mot).\n",
        "\n",
        "Les modèles de spacy embarquent généralement des  vecteurs de mots de type Word2Vec (éviter un modèle _sm/small_). L'import des formats gensim, fastext ou Word2Vec est possible. \n",
        "\n",
        "Pour ces raisons nous utiliserons ici [spaCy](https://spacy.io/usage/linguistic-features#vectors-similarity). Remarquez que gensim offre les mêmes fonctionnalités sous le nom de [doc2vec et de paragraph vector](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H7DDOkl4-Hl"
      },
      "source": [
        "# sélectionner le modèle à installer\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "# redémarrer l'environnement d'exécution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbx-UPc2ylv"
      },
      "source": [
        "# charger le modèle\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G-J2HGZFFsI"
      },
      "source": [
        "Après traitement (via la méthode `nlp`), on accède au vecteur d'un mot ou du document via la propriété `vector`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V86z9Wk43Eww"
      },
      "source": [
        "# trois documents traités avec spaCy\n",
        "doc1 = nlp('The dog is lazy but the brown fox is quick!')\n",
        "doc2 = nlp('The quick brown fox jumps over the lazy dog.')\n",
        "doc3 = nlp('The sky is blue and beautiful.')\n",
        "\n",
        "# trois mots du 1er document\n",
        "dog = doc1[1]\n",
        "lazy = doc1[3]\n",
        "fox = doc1[7]\n",
        "\n",
        "# la représentation vectorielle continue d'un mot\n",
        "print(dog.text, dog.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdv8c0a_G7UK"
      },
      "source": [
        "SpaCy permet aussi d'accéder facilement à l'embedding d'un document (résultant de la moyenne des embeddings des mots qui le compose). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPubw8HiHx1h"
      },
      "source": [
        "# la représentation vectorielle continue d'un document\n",
        "print (doc1.text, doc1.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOK9dK_i83mP"
      },
      "source": [
        "L'API offre une fonction de calcul de similarité [`similarity`](https://spacy.io/usage/vectors-similarity), ci-dessous illustrée sur la comparaison de mots. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns33bJBM9i9f"
      },
      "source": [
        "print(dog.similarity(lazy))\n",
        "print(dog.similarity(fox))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Bvump3IOsh"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* En écrivant le code qui va bien, indiquez quelle est la dimension des vecteurs embarqués par le modèle de spaCy chargé ? Vérifiez que les documents font la même taille. Est-ce d'ailleurs normal ?\n",
        "\n",
        "* Expérimentez la fonction de calcul de similarité sur les documents courts doc1/doc2 et doc1/doc3. Donnez le code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifnqjF86Jfjf"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzYB6IiGKoE8"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65vzdOuKzb4"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "nltk embarque des corpus de textes libres notamment des textes de la [collection gutenberg](https://www.gutenberg.org/about/).\n",
        "\n",
        "* Le code ci-dessous permet de charger trois textes dans un format brut. Deux sont de Shakespeare à savoir _Hamlet_ et _Macbeth_ et un est de Lewis Carroll à savoir _Alice aux pays des merveilles_. Ajouter le code permettant de calculer la similarité des textes hamlet/macbeth et hamlet/alice.\n",
        "* En observant les résultats des calculs de similarité entre petits documents et entre documents, trouvez-vous que la représentation des documents calculée comme une moyenne des vecteurs des mots qui les composent est fiable ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF7LOHcu_hcB"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# lister les textes présents dans la collection gutenberg\n",
        "print(nltk.corpus.gutenberg.fileids())\n",
        "\n",
        "# récupère le texte de trois oeuvres\n",
        "hamlet = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt') # .words() pour le texte tokenisé\n",
        "macbeth = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
        "alice = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "\n",
        "print ('hamlet', len(hamlet), hamlet[:100], '----')\n",
        "print ('macbeth', len(macbeth), macbeth[:100], '----')\n",
        "print ('alice', len(alice), alice[:100], '----')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCjvRijAMpi9"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbeBgRqXAEiY"
      },
      "source": [
        "# TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XyFa9V0O7sG"
      },
      "source": [
        "## Partitionnement sur la base d'une représentation document-embeddings\n",
        "\n",
        "Cette section fait écho aux sections de partionnements sur la base de représentation des documents de la forme document-terms et document-thèmes d'un notebook antérieul. La méthode de partitionnement kmeans est utilisée.\n",
        "\n",
        "Les lignes de code ci-dessous déclare un corpus, le pré-traite (i.e. calcul les vecteurs continus des documents) et affiche le corpus sous la forme d'une dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrMBBMkCWy1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# déclaration de la donnée\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "# pré-traitement\n",
        "dt_matrix = [nlp(doc).vector for doc in corpus]\n",
        "\n",
        "# affichage visuel\n",
        "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyl9aamcQlkc"
      },
      "source": [
        "Le code suivant opère le clustering et l'affichage des classes prédites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0npcaqrZB4eT"
      },
      "source": [
        "# clustering des documents selon la méthode kmeans \n",
        "# en utilisant la distribution des thèmes et non la distrubution des termes \n",
        "# comme traits discriminants entre documents\n",
        "from sklearn.cluster import KMeans\n",
        "# Warning: ici n_clusters ne doit pas être touché\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "#km.fit_transform(features)\n",
        "km.fit_transform(dt_matrix)\n",
        "\n",
        "# affichage des clusters\n",
        "cluster_labels = km.labels_\n",
        "import pandas as pd\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T21SodpxQx__"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* De prime abord, est-ce que ce type de représentation vous semble exploitable avec des textes courts ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bno3jlT4QywJ"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTautWC03PT9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdXVnDprkjH"
      },
      "source": [
        "# Références\n",
        "* représentation continue de document https://github.com/clement-plancq/outils-corpus/blob/master/outils_corpus-7.ipynb\n",
        "* https://nlp-ensae.github.io/materials/course2/"
      ]
    }
  ]
}
