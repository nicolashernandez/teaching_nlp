{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_LinguisticAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5jh78yjel68T"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJ4YQ4lI9VLGG80OlmUTNq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2-CN-2021-22_01_LinguisticAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0B9daf8oqMQ"
      },
      "source": [
        "# Généralités\n",
        "\n",
        "### Objectif  \n",
        "* analyser et interpréter les sorties et la qualité d'analyseurs linguistiques \n",
        "* découvrir les caractéristiques (en particulier linguistiques) d'une donnée langagière qui peuvent influer sur les traitements automatiques et la qualité de ceux-ci \n",
        "\n",
        "### NLP (python) Libraries\n",
        "* spaCy: state-of-the-art natural language processing with industrial motivations and tools, multilingue, statistical/neuronal models https://spacy.io/usage/linguistic-features\n",
        "* NLTK : pédagogique, des approches à base de règles ou statistiques, multilingues\n",
        "* Stanza : python, nouveau framework de Stanford, modèles neuronaux entraînés sur données UD\n",
        "* flair, fondé sur PyTorch, multilingue, support spécial pour le biomedical https://github.com/flairNLP/flair\n",
        "* gensim: topic modeling and similarity detection\n",
        "\n",
        "Mais aussi en java    \n",
        "* Stanford Core NLP : multilingue, statistique, résolution de la coréference\n",
        "* DKPro\n",
        "\n",
        "### Thèmes abordés\n",
        "\n",
        "* (Pré-)traitements linguistiques\n",
        "  * Tokenization, sentence segmentation FIXME\n",
        "  * POS tagging, morphology, lemmatization, dependency parsing, \n",
        "  * Named entities\n",
        "  * Language detection FIXME\n",
        "* Mise en application \n",
        "  * Langue : Français \n",
        "  * Types de texte : Dépèches journalistiques, Twitter, romantic novels and legal text\n",
        "  * Dimension multilingue\n",
        "\n",
        "### Consignes de travail\n",
        "\n",
        "Réponse aux questions dans la section \"votre réponse\". Des réponses brèves et simples sont attendues. \n",
        "\n",
        "Vous avez le droit de modifier le code pour vous permettre de plus facilement répondre aux questions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKO-7ZT7x4y"
      },
      "source": [
        "# Installation de l'environnement : chargement des modèles et des données\n",
        "\n",
        "Executer le code suivant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhTHluRv9RIb"
      },
      "source": [
        "# Téléchargement d'un modèle pour le traitement du français\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z199LIHqPoc"
      },
      "source": [
        "# Importation de la bibliothèque spaCy \n",
        "import spacy\n",
        "\n",
        "# Chargement du modèle pour le français\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Importation d'une liste d'exemple de phrases en français\n",
        "from spacy.lang.fr.examples import sentences "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZwjmG6GrRiA"
      },
      "source": [
        "Si vous obtenez l'erreur suivante \n",
        "```\n",
        "# OSError: [E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
        "```\n",
        "Alors \n",
        "- télécharger le modèle `!python -m spacy download fr_core_news_sm `\n",
        "- puis Exécution > Redémarrer l'environnement d'Exécution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwWQQfB-kDf4"
      },
      "source": [
        "Executer aussi :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGZh5n0kDM7"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Z4qHGSYKCV"
      },
      "source": [
        "# Analyses linguistiques du français \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQQ4CIckfKUR"
      },
      "source": [
        "## Tokénisation\n",
        "\n",
        "Un **token** est une instance d'une séquence de caractères dans un document donné qui constitue une unité pour une quelconque raison (e.g. délimitée par des espaces). Les tokens qui ont une réalité grammaticale (e.g. Nom, Verbe, Adjectif...) peuvent être considérés comme des instances de **mots**. Le mot a une réalité linguistique (ses caractères sont des lettres de l'alphabet plus l'espace et le tiret). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jh78yjel68T"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code suivant commence par déclarer deux tokenizers utilisant des technologies différentes : l'un est proposé par la bibliothèque _nltk_ et l'autre _spaCy_. Le code se poursuit en calculant le nombre de tokens obtenus par chacun des tokenizers. Puis applique les deux tokenizers et affiche les différences de tokenization pour les 5 1ères phrases du corpus de phrases exemples en français de spaCy.\n",
        "\n",
        "* Exécuter le code. Quelles erreurs/différences de tokenization observez-vous ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZYVlMjPfOAw"
      },
      "source": [
        "# import spaCy and define a method to tokenize via spacy\n",
        "# en fait, la méthode récupère seulement la tokenization, mais la méthode 'nlp' \n",
        "# produit plusieurs analyses (on le verra plus tard)\n",
        "# machine learning based model\n",
        "import spacy\n",
        "spacy_tokenize = lambda text: [token.text for token in nlp(text)]\n",
        "\n",
        "# import un tokenizer de nltk \n",
        "# uses regular expressions (to fit the penn treebank corpus tokenization) \n",
        "# and an unsupervised sentence segmentation model (aka punkt) \n",
        "# to distinguish abreviations from sentence endings.  \n",
        "from nltk import word_tokenize\n",
        "nltk_tokenize = lambda text: [token for token in word_tokenize(text, language='french')]\n",
        "\n",
        "# compte le nombre de phrases exemples et le nombre de tokens total obtenu \n",
        "# par chaque tokenizer nltk et spacy\n",
        "len_nltk_tokens = 0\n",
        "len_spacy_tokens = 0\n",
        "for i in range (0, len(sentences)):\n",
        "  len_nltk_tokens += len(word_tokenize(sentences[i]))\n",
        "  len_spacy_tokens += len(spacy_tokenize(sentences[i]))\n",
        "print('len_sentences=',len(sentences),' ; len_nltk_tokenize=', len_nltk_tokens, ' ; len_spacy_tokens=', len_spacy_tokens)\n",
        "print()\n",
        "\n",
        "# pour chaque phrase, chacun des tokenizers produisent une liste de tokens\n",
        "# la phrase est affichée ainsi que les contextes des tokens où les tokenisations \n",
        "# n'ont pas produit la même analyse   \n",
        "from difflib import context_diff, ndiff\n",
        "for i in range (0, 5):\n",
        "  print (i, sentences[i])\n",
        "  print('\\n'.join(context_diff(word_tokenize(sentences[i]), spacy_tokenize(sentences[i]), fromfile='nltk_tokenize', tofile='spacy_tokenize')))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwiphA6oq5d"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIQMphkuiNS"
      },
      "source": [
        "\n",
        "## Analyse lexicale\n",
        "\n",
        "\n",
        "### Token vs Mot vs Formes morphologiques vs Lemme\n",
        "\n",
        "Le **mot** peut avoir plusieurs **formes (morphologiques)** lesquelles renseignent sur le genre, le nombre, le mode/temps (pour les verbes). Le mot est généralement utilisée comme entrée lexicale (dans un disctionnaire). On appelle **lemme** la forme référente d'un mot. \"renseigner\", \"pomme de terre\" sont des mots. Ces formes servent aussi de lemmes. \"renseignent\" est une des formes du mot \"renseigner\".  \n",
        "\n",
        "\n",
        "### Propriétés associées à un token mot dans Spacy\n",
        "\n",
        "Les modèles de Spacy produisent de base une analyse grammaticale, morphologique et syntaxique des mots pour plusieurs langues.\n",
        "Les propriétés suivantes informent de différents attributs (en particulier linguistiques associées) à chaque token.\n",
        "* `text`: The original word text\n",
        "* `lemma_`: The base form of the word.\n",
        "* `pos_`: The simple UPOS part-of-speech tag.\n",
        "* `tag_`: The detailed part-of-speech tag with morphological information.\n",
        "* `dep_`: Syntactic dependency, i.e. the relation between tokens.\n",
        "* `shape_`: The word shape – capitalization, punctuation, digits.\n",
        "* `is_alpha`: Is the token an alpha character?\n",
        "* `is_stop`: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "Au sujet de la propriété `is_stop`, en français on parle de _mot outil_ ou de _mot vide_, vide de sens, qui ne permettent pas d'analyser le contenu \"thématique\" de la phrase. Ces mots sont en général des listes fermées et comptent les déterminants, les prépositions, et quelques adverbes. Statistiquement ils sont plus fréquents que d'autres (cf. la loi de zipf dans un prochain cours). Selon les usages, les mots vides peuvent êtres utiles. Par exemple, en analyse d'opinion, les adverbes de négation ou d'emphases jouent un rôle important. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZDBpp36KQT"
      },
      "source": [
        "### QUESTIONS \n",
        "Le code suivant permet d'appliquer un modèle Spacy offrant des traitements TAL à un document (ici une phrase) donné.\n",
        "* Ajouter les propriétés non spécifiées permettant d'observer les résultats de la lemmatisation, de l'étiquetage grammatical, de l'analyse morphologique et de l'analyse en dépendance syntaxique. \n",
        "* Consulter l'analyse des 5 premières phrases exemples de spacy (0 à 4). Trouvez-vous des erreurs de tokenization ? de lemmatisation ? d'étiquetage grammatical ? d'analyse morphologique ? Eventuellement, donner quelques exemples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl2wxiPM5y8S"
      },
      "source": [
        "# Ici le document est la 1ère phrase (sentence 0) des exemples de Spacy \n",
        "# de phrases écrites en français\n",
        "doc = sentences[2]\n",
        "\n",
        "# exécution des traitements en une seule commande\n",
        "spacy_doc = nlp(doc)\n",
        "\n",
        "# affichage de la phrase\n",
        "print(spacy_doc.text)\n",
        "\n",
        "# affichage de quelques résultats d'analyse \n",
        "# et ce, dans une pandas dataframe pour améliorer le visuel\n",
        "import pandas as pd\n",
        "spacy_tokens_as_list = [(token.text, token.is_stop) for token in spacy_doc]\n",
        "pd.DataFrame(spacy_tokens_as_list, columns=['Token', 'is_stop'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQAdZqp5Hir"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slk2dK1aq-Jo"
      },
      "source": [
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3UeYPftYmIg"
      },
      "source": [
        "## Analyse syntaxique\n",
        "En syntaxe, il existe deux modèles d'analyse de la structure syntaxique:\n",
        "- L'**analyse en constituants** (_chunk_ en anglais) ou **syntagmes** qui met en avant des catégories d'objets syntaxiques : groupe/syntagme nominal, syntagme verbal... avec une structure récursive (e.g. un syntagme verbal peut contenir un syntagme nominal...);\n",
        "- L'**analyse en dépendances**  qui met en avant les fonctions jouées par des têtes lexicales : sujet, objet, modifieur...\n",
        "\n",
        "Les constituants peuvent jouer le rôle de termes candidats pour indexer de l'information. On verra que d'autres techniques existent. \n",
        "L'arbre syntaxique peut être exploitée en simplification de phrases ou bien dans des systèmes de question-réponse pour identifier la relation entre des entités ou objets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-x8Ysc7baMq"
      },
      "source": [
        "### QUESTIONS : Analyse en constituants\n",
        "Le code suivant permet d'observer des constituants nominaux pour les 5 premières phrases des phrases exemples en français de Spacy.\n",
        "\n",
        "* Les syntagmes identifiés vous semble-t-ils correctes ? Sont ils raccord avec l'analyse grammaticale produite des tokens mots ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHFhADebWV8"
      },
      "source": [
        "#spacy_noun_chunks_as_list = [(chunk.text,  chunk.root.text, chunk.root.dep_,\n",
        "#            chunk.root.head.text) for chunk in spacy_doc.noun_chunks]\n",
        "#pd.DataFrame(spacy_noun_chunks_as_list, columns=['text', 'root', 'dep', 'head'])\n",
        "for i in range(0,5):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "  print(i, spacy_doc.text)\n",
        "  for chunk in spacy_doc.noun_chunks:\n",
        "    print(chunk.text) #, chunk.root.text, chunk.root.dep_, chunk.root.head.text)   \n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADl-NVXSdQN"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrP0M_8dTqp_"
      },
      "source": [
        "### QUESTIONS : Analyse en dépendance\n",
        "\n",
        "Le code suivant permet de visualiser la structure syntaxique en dépendance pour un document (une phrase) donnée.\n",
        "\n",
        "* Consulter l'analyse des 5 premières phrases exemples de spacy (0 à 4). Si l'on traduit la dépendance comme une forme d'_importance_, les flèches partent du mot le plus important et vont jusqu'aux mots les moins importants dans la phrase. Est-ce bien le cas pour toutes les phrases ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkgQPtnBs5uC"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "for i in range(0,5):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "  displacy.render(spacy_doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hevn0R8Aahvs"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfwLaFdP07oQ"
      },
      "source": [
        "## Reconnaissance d'entités nommées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlb0dRU21zsw"
      },
      "source": [
        "### QUESTIONS\n",
        "\n",
        "Le code suivant permet de visualiser entités nommées présentes dans le document analysé.\n",
        "* Lister les types d'entités (_labels_) présentes dans les exemples\n",
        "* Consulter l'analyse des 10 premières phrases exemples de spacy (0 à 9). Trouvez-vous des erreurs de délimitation d'entités nommées ? Dans l'étiquetage du type des entités ? Eventuellement, donner quelques exemples.\n",
        "* Jeter un oeil sur les performances (section *accuracy evaluation*) des modèles pour le français https://spacy.io/models/fr pour avoir une idée de la performance supposée de ceux-ci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jkS1aoU2Brx"
      },
      "source": [
        "# le code suivant permet de visualiser entités nommées présentes dans le document analysé\n",
        "# les offsets et le type d'entité sont aussi fournis \n",
        "for ent in spacy_doc.ents:\n",
        "   print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw6iMfw16KF"
      },
      "source": [
        "### VOS RÉPONSES\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm3_nGMo1B0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoBU3isucVpQ"
      },
      "source": [
        "## Expérience personnelle vs performances attestées\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFF9cpUczG2"
      },
      "source": [
        "### QUESTIONS\n",
        "* Jeter un oeil sur les performances (section *accuracy evaluation*) du modèle utilisé pour le français https://spacy.io/models/fr pour avoir une idée de la performance supposée de celui-ci sur les tâches d'analyse linguistique (tokenization, étiquetage grammatique, lemmatisation, analyse morphologique, analyse en dépendance et reconnaissance d'entités nommées). Est-ce raccord avec ce que vous avez observé ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUhWwwVGc5hA"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GNFxUwaNaM"
      },
      "source": [
        "# Analyse de textes de genres différents\n",
        "\n",
        "Le code suivant télécharge dans un répertoire `data` un corpus de phrases issus de 4 genres différents : textes parlementaires européens (_legal europarl_), dépèches journalistiques (_news_wikinews_), littératique romanesque (_roman verne_), et des tweets de twitter (_tweets twitter_).\n",
        "\n",
        "Exécuter le.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2MikY8trjs-"
      },
      "source": [
        "!mkdir data\n",
        "!wget -nc https://raw.githubusercontent.com/nicolashernandez/teaching_nlp/main/data/fr_raw_25000sentences_4genres.zip -P data\n",
        "!unzip data/fr_raw_25000sentences_4genres.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtcfjtV01OSi"
      },
      "source": [
        "\n",
        "## QUESTIONS\n",
        "\n",
        "Le code suivant charge tour à tour chacun des corpus et traite les 5 premières phrases de chaque corpus. Deux résultats de traitement sont observés : la tokenization et la reconnaissance d'entités nommées à l'aide de spaCy et du modèle pour le français précédemment utilisé.\n",
        "\n",
        "* Quels problèmes de tokenization relevez-vous suivant les genres de texte ? Y-a-til des genres pour lesquels la tokenization fonctionne mieux que d'autres ?\n",
        "* Mêmes questions pour la reconnaissance des entités nommées.\n",
        "* Quels genres de texte ont servi de données d'entraînement pour construire le modèle français utilisé ici avec spaCy (cf. https://spacy.io/models/fr) ? Est-ce que cela peut expliquer les performances observées ?  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ETeuhrev-7z"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "filenames = ['legal_europarl', 'news_wikinews', 'roman_verne', 'tweets_twitter']\n",
        "for filename in filenames:\n",
        "  with open(\"data/\"+filename+\".txt\", 'r', encoding='UTF-8') as file:\n",
        "    i = 0\n",
        "    print('-->', filename.upper())\n",
        "    print()\n",
        "    for line in file:\n",
        "        doc = line.rstrip()\n",
        "        spacy_doc = nlp(doc)\n",
        "        print('Sentence',i,':', doc)\n",
        "        print ('spacy_tokenize:', [token.text for token in spacy_doc])\n",
        "        if spacy_doc.ents: displacy.render(spacy_doc, style='ent', jupyter=True)\n",
        "        #spacy_doc_as_list = [(token.text, token.lemma_, token.pos_) for token in spacy_doc]\n",
        "#            , token.tag_,token.shape_, token.is_alpha, token.is_stop) for token in doc]\n",
        "        #print (pd.DataFrame(spacy_doc_as_list, columns=['Token', 'Lemma', 'POS']))\n",
        "        print ()\n",
        "        i += 1\n",
        "        if i==5: break\n",
        "    print ('------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQz5q0C9_PD"
      },
      "source": [
        "\n",
        "## VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2NDYD5Es0Ic"
      },
      "source": [
        "# Biais des modèles construits sur des données\n",
        "\n",
        "Soit le texte suivant\n",
        "> *Elle est médecin. Il est infirmier.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZ-kRIWui7t"
      },
      "source": [
        "\n",
        "### QUESTIONS\n",
        "* Ouvrir [Google Translate dans votre navigateur](https://translate.google.fr/?hl=fr&sl=fr&tl=en&text=Elle%20est%20m%C3%A9decin.%20Il%20est%20infirmier.&op=translate)\n",
        "* Traduire du français (langue source) vers l'anglais (langue cible). Cliquer deux fois sur \"Intervertir les langues\" (pour traduire une fois vers l'anglais puis pour retraduire l'anglais vers le français). Observez-vous quelque chose ?\n",
        "* Faire la même chose en prenant comme langue cible du Hongrois. Observez vous quelque chose ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scPvHf9wumB1"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXOzv3zo_qB"
      },
      "source": [
        "---\n",
        "# Multilinguisme\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Donnés les éléments suivants :\n",
        "* https://spacy.io/models\n",
        "* https://fr.wikipedia.org/wiki/Liste_de_langues_par_nombre_total_de_locuteurs\n",
        "* Le score F-score est une moyenne des scores de Précision et de Rappel. \n",
        "* un [tableau des performances de spaCy telles que présentées en oct 2021 pour le 1er modèle de chaque langue](https://github.com/nicolashernandez/teaching_nlp/raw/main/performances%20spacy%20oct%202021.ods)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7lMYqa4_UpQ"
      },
      "source": [
        "\n",
        "## QUESTIONS \n",
        "* Peut-on considérer que la _tokenization_ est un problème réglé ? Quelle est la langue pour laquelle la performance est la plus basse ? Selon vous, est-ce un problème de ressources ou bien de spécificité de la langue ? \n",
        "* Peut-on considérer que la _segmentation en phrase_ est un problème réglé ? Quelles sont les deux langues pour lesquelles les performances sont les plus basses ? Selon vous, est-ce un problème de ressources ou bien de spécificité de la langue ?  \n",
        "* Pourquoi des performances d'analyse morphologique et grammatical ne sont pas rapportées pour toutes les langues ?\n",
        "* Peut-on considérer que l' _étiquetage grammatical_ est un problème réglé ? \n",
        "* Peut-on considérer que l' _analyse morphologique comprenant la lemmatisation_ est un problème réglé ?\n",
        "* Peut-on considérer que la _reconnaissance des entités nommées_ est un problème réglé ? \n",
        "* Peut-on considérer que l'_analyse syntaxique en dépendance_ est un problème réglé ? \n",
        "* Citer deux langues parmi les langues les plus parlées dans le monde (en nombre total de locuteur) qui ne sont pas prises en charge par spacy.\n",
        "* Trouvez-vous sur le web des bibliothèques (python) qui offrent (partiellement ou totalement) les mêmes traitements que Spacy sur ces langues ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyDnW8vV_WGV"
      },
      "source": [
        "## VOS REPONSES\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRxwNo-7K5E"
      },
      "source": [
        "# Benchmark\n",
        "* https://spacy.io/usage/facts-figures\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Hehbr6_Pjt"
      },
      "source": [
        "## QUESTIONS\n",
        "* En observant les performances (qualité et temps de traitement) de solutions alternatives (Stanza, Flair) sur des tâches reconnaissance d'entités nommées et de parsing en anglais, diriez-vous que Spacy est 1) la meilleure solution, 2) une solution état de l'art, 3) de performance moindre que les solutions existantes ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW8cF6u9_Q52"
      },
      "source": [
        "\n",
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    }
  ]
}