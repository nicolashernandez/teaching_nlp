{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2-ATAL-2021-22_01_LinguisticAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRSCohrMTLDM9w066ANUXJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2_ATAL_2021_22_01_LinguisticAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0B9daf8oqMQ"
      },
      "source": [
        "# Généralités\n",
        "\n",
        "### Objectif  \n",
        "* analyser et interpréter les sorties et la qualité d'analyseurs linguistiques \n",
        "* découvrir les caractéristiques (en particulier linguistiques) d'une donnée langagière qui peuvent influer sur les traitements automatiques et la qualité de ceux-ci \n",
        "\n",
        "### NLP (python) Libraries\n",
        "* [spaCy](https://spacy.io/): \"_Support for 64+ languages ; 64 trained pipelines for 19 languages; Multi-task learning with pretrained transformers like BERT; Pretrained word vectors; State-of-the-art speed\n",
        "Production-ready training system; \n",
        "Linguistically-motivated tokenization; \n",
        "Components for named entity recognition, part-of-speech tagging, dependency parsing, sentence segmentation, text classification, lemmatization, morphological analysis, entity linking and more; \n",
        "Easily extensible with custom components and attributes; \n",
        "Support for custom models in PyTorch, TensorFlow and other frameworks;  \n",
        "Built in visualizers for syntax and NER_\" (state-of-the-art natural language processing with industrial motivations and tools, 'Cy' for 'Cython', multilingue, statistical/neuronal models https://spacy.io/usage/linguistic-features)\n",
        "* [NLTK](https://www.nltk.org) : \"_NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum_\" (pédagogique, des approches à base de règles ou statistiques, multilingues)\n",
        "* [Stanza](https://stanfordnlp.github.io/stanza/index.html) : \"_Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing_\" (python, nouveau framework de Stanford, modèles neuronaux entraînés sur données UD, 66 langues)\n",
        "* [flair](https://github.com/flairNLP/flair), \"_A powerful NLP library. Flair allows you to apply our state-of-the-art natural language processing (NLP) models to your text, such as named entity recognition (NER), part-of-speech tagging (PoS), special support for biomedical data, sense disambiguation and classification, with support for a rapidly growing number of languages._\" ; \"_A text embedding library. Flair has simple interfaces that allow you to use and combine different word and document embeddings, including our proposed Flair embeddings, BERT embeddings and ELMo embeddings._\", \"_A PyTorch NLP framework. Our framework builds directly on PyTorch, making it easy to train your own models and experiment with new approaches using Flair embeddings and classes._\"\n",
        " (fondé sur PyTorch, multilingue, support spécial pour le biomedical, COLING18, EACL19)\n",
        "* [Trankit](https://trankit.readthedocs.io/) \"_is a light-weight Transformer-based Python Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies v2.5 treebanks. Our pipeline also obtains competitive or better named entity recognition (NER) performance compared to existing popular toolkits on 11 public NER datasets over 8 languages._\" (EACL'2021)\n",
        "* [gluon](https://github.com/dmlc/gluon-nlp/) MXNet, Amazon\n",
        "* [TextBlob](https://textblob.readthedocs.io) \"_is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both._\"\n",
        "* [Gensim](https://radimrehurek.com/gensim/): \"_Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible.\n",
        "Gensim is designed to process raw, unstructured digital texts (”plain text”) using unsupervised machine learning algorithms.\n",
        "The algorithms in Gensim, such as Word2Vec, FastText, Latent Semantic Indexing (LSI, LSA, LsiModel), Latent Dirichlet Allocation (LDA, LdaModel) etc, automatically discover the semantic structure of documents by examining statistical co-occurrence patterns within a corpus of training documents. These algorithms are unsupervised, which means no human input is necessary – you only need a corpus of plain text documents. Once these statistical patterns are found, any plain text documents (sentence, phrase, word…) can be succinctly expressed in the new, semantic representation and queried for topical similarity against other documents (words, phrases…)._\" (topic modeling and similarity detection \n",
        "* [udpipe](https://ufal.mff.cuni.cz/udpipe/)\n",
        "\n",
        "Mais aussi en java    \n",
        "* [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/) : \"_CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish._\" (multilingue, statistique, résolution de la coréference)\n",
        "* [DKPro](https://dkpro.github.io/) \"_A collection of software components for natural language processing (NLP) based on the Apache UIMA framework._\"\n",
        "* [Apache OpenNLP](https://opennlp.apache.org/) \"_OpenNLP supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, language detection and coreference resolution._\"\n",
        "\n",
        "### Thèmes abordés\n",
        "\n",
        "* (Pré-)traitements linguistiques\n",
        "  * Tokenization, Sentence segmentation \n",
        "  * POS tagging, Morphology, Lemmatization, Dependency parsing, \n",
        "  * Named Entities Recognition (NER)\n",
        "  * Language Detection \n",
        "* Mise en application \n",
        "  * Langue : Français \n",
        "  * Types de texte : Dépèches journalistiques, Tweets, romans et textes juridiques\n",
        "  * Dimension multilingue\n",
        "\n",
        "### Consignes de travail\n",
        "\n",
        "Réponse aux questions dans la section \"votre réponse\". Des réponses brèves et simples sont attendues. \n",
        "\n",
        "Vous avez le droit de modifier le code pour vous permettre de plus facilement répondre aux questions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKO-7ZT7x4y"
      },
      "source": [
        "---\n",
        "# Installation de l'environnement : chargement des modèles et des données\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Executer le code suivant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhTHluRv9RIb"
      },
      "source": [
        "# Téléchargement d'un modèle pour le traitement du français\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z199LIHqPoc"
      },
      "source": [
        "# Importation de la bibliothèque spaCy \n",
        "import spacy\n",
        "\n",
        "# Chargement du modèle pour le français\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Importation d'une liste d'exemple de phrases en français\n",
        "from spacy.lang.fr.examples import sentences "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZwjmG6GrRiA"
      },
      "source": [
        "Si vous obtenez l'erreur suivante \n",
        "```\n",
        "# OSError: [E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
        "```\n",
        "Alors \n",
        "- Faire `Exécution > Redémarrer l'environnement d'Exécution`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwWQQfB-kDf4"
      },
      "source": [
        "Executer aussi :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGZh5n0kDM7"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Z4qHGSYKCV"
      },
      "source": [
        "# Analyses linguistiques du français \n",
        "\n",
        "Par la suite nous utiliserons principalement la bibliothèque spaCy et sa méthode `nlp` que l'on applique à du texte brut `doc`. Avec cette méthode, spaCy réalise un certain nombre de traitements par défaut disponible dans le modèle chargé tel que la segmentation d'un texte en phrases, la tokenization, l'analyse grammaticale et morphologique des mots (dont lemmatisation), l'analyse en constituants syntaxiques, l'analyse en dépendance syntaxique, la reconnaissance des entités nommées. On désignera par `spacy_doc` l'objet qui correspond au résultat d'analyse (méthode `nlp`) de spaCy sur le texte `doc`.\n",
        "\n",
        "Parcourir l'objet spacy_doc comme une liste python fournira les informations rattachées à chaque token du texte. `spacy_doc.noun_chunks` donnera des informations sur les constituants syntaxiques. `spacy_doc.ents` donnera des informations sur les entités nommées détectées dans le texte.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQQ4CIckfKUR"
      },
      "source": [
        "## Tokénisation\n",
        "\n",
        "Un **token** est une instance d'une séquence de caractères dans un document donné qui constitue une unité pour une quelconque raison (e.g. délimitée par des espaces). Les tokens qui ont une réalité grammaticale (e.g. Nom, Verbe, Adjectif...) peuvent être considérés comme des instances de **mots**. Le mot a une réalité linguistique (ses caractères sont des lettres de l'alphabet plus l'espace et le tiret). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jh78yjel68T"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "Le code suivant commence par déclarer deux tokenizers utilisant des technologies différentes : l'un est proposé par la bibliothèque _nltk_ et l'autre _spaCy_. Le code se poursuit en calculant le nombre de tokens obtenus par chacun des tokenizers. Puis applique les deux tokenizers et affiche les différences de tokenization pour les 5 1ères phrases du corpus de phrases exemples en français de spaCy. Seules les différentes sont affichées en donnant quelques mots en contexte (et non toute la phrase). La bibliothèque utilisée donne aussi les offsets de début/fin de la différence observée. \n",
        "\n",
        "* Exécuter le code. Quelles erreurs/différences de tokenization observez-vous ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZYVlMjPfOAw"
      },
      "source": [
        "# import spaCy and define a method to tokenize via spacy\n",
        "# en fait, la méthode récupère seulement la tokenization, mais la méthode 'nlp' \n",
        "# produit plusieurs analyses (on le verra plus tard)\n",
        "# machine learning based model\n",
        "import spacy\n",
        "spacy_tokenize = lambda text: [token.text for token in nlp(text)]\n",
        "\n",
        "# import un tokenizer de nltk \n",
        "# uses regular expressions (to fit the penn treebank corpus tokenization) \n",
        "# and an unsupervised sentence segmentation model (aka punkt) \n",
        "# to distinguish abreviations from sentence endings.  \n",
        "from nltk import word_tokenize\n",
        "nltk_tokenize = lambda text: [token for token in word_tokenize(text, language='french')]\n",
        "\n",
        "\n",
        "# compte le nombre de phrases exemples et le nombre de tokens total obtenu \n",
        "# par chaque tokenizer nltk et spacy\n",
        "nltk_tokens = list()\n",
        "spacy_tokens = list()\n",
        "for i in range (0, len(sentences)):\n",
        "  nltk_tokens.extend(word_tokenize(sentences[i]))\n",
        "  spacy_tokens.extend(spacy_tokenize(sentences[i]))\n",
        "print('len_sentences=',len(sentences),' ; len_nltk_tokenize=', len(nltk_tokens), ' ; len_spacy_tokens=', len(spacy_tokens))\n",
        "print()\n",
        "\n",
        "# https://twitter.com/_inesmontani/status/1151447435195113472?lang=en\n",
        "#from spacy.gold import align\n",
        "#cost, a2b, b2a, a2b_multi, b2a_multi = align(nltk_tokens, spacy_tokens)\n",
        "#print ('cost:', cost)\n",
        "#print()\n",
        "\n",
        "# pour chaque phrase, chacun des tokenizers produit une liste de tokens\n",
        "# la phrase est affichée ainsi que les contextes des tokens où les tokenisations \n",
        "# n'ont pas produit la même analyse   \n",
        "from difflib import context_diff, ndiff\n",
        "\n",
        "for i in range (0, 5):\n",
        "  print (i, sentences[i])\n",
        "  print('\\n'.join(context_diff(word_tokenize(sentences[i]), spacy_tokenize(sentences[i]), fromfile='nltk_tokenize', tofile='spacy_tokenize')))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwiphA6oq5d"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIQMphkuiNS"
      },
      "source": [
        "\n",
        "## Analyse lexicale\n",
        "\n",
        "\n",
        "### Token vs Mot vs Formes morphologiques vs Lemme\n",
        "\n",
        "Le **mot** peut avoir plusieurs **formes (morphologiques)** lesquelles renseignent sur le genre, le nombre, le mode/temps (pour les verbes). Le mot est généralement utilisée comme entrée lexicale (dans un disctionnaire). On appelle **lemme** la forme référente d'un mot. \"renseigner\", \"pomme de terre\" sont des mots. Ces formes servent aussi de lemmes. \"renseignent\" est une des formes du mot \"renseigner\".  \n",
        "\n",
        "\n",
        "### Propriétés associées à un token mot dans Spacy\n",
        "\n",
        "Les modèles de Spacy produisent de base une analyse grammaticale, morphologique et syntaxique des mots pour plusieurs langues.\n",
        "Les propriétés suivantes informent de différents attributs (en particulier linguistiques associées) à chaque token.\n",
        "* `text`: The original word text\n",
        "* `lemma_`: The base form of the word.\n",
        "* `pos_`: The simple UPOS part-of-speech tag.\n",
        "* `tag_`: The detailed part-of-speech tag with morphological information.\n",
        "* `dep_`: Syntactic dependency, i.e. the relation between tokens.\n",
        "* `shape_`: The word shape – capitalization, punctuation, digits.\n",
        "* `is_alpha`: Is the token an alpha character?\n",
        "* `is_stop`: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "Au sujet de la propriété `is_stop`, en français on parle de _mot outil_ ou de _mot vide_, vide de sens, qui ne permettent pas d'analyser le contenu \"thématique\" de la phrase. Ces mots sont en général des listes fermées et comptent les déterminants, les prépositions, et quelques adverbes. Statistiquement ils sont plus fréquents que d'autres (cf. la loi de zipf dans un prochain cours). Selon les usages, les mots vides peuvent être utiles. Par exemple, en analyse d'opinion, les adverbes de négation ou d'emphases jouent un rôle important. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZDBpp36KQT"
      },
      "source": [
        "### QUESTIONS \n",
        "Le code suivant permet d'appliquer un modèle Spacy offrant des traitements TAL à un document (ici une phrase) donné.\n",
        "* Ajouter les propriétés non spécifiées permettant d'observer les résultats de la lemmatisation, de l'étiquetage grammatical, de l'analyse morphologique et de l'analyse en dépendance syntaxique. \n",
        "* Consulter l'analyse des 5 premières phrases exemples de spacy (0 à 4). Donnez un exemple d'erreur de lemmatisation, d'erreur d'étiquetage grammatical, d'erreur d'analyse morphologique (idéalement dans des phrases différentes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl2wxiPM5y8S"
      },
      "source": [
        "# Ici le document est la 1ère phrase (sentence 0) des exemples de Spacy \n",
        "# de phrases écrites en français\n",
        "doc = sentences[2]\n",
        "\n",
        "# exécution des traitements en une seule commande\n",
        "spacy_doc = nlp(doc)\n",
        "\n",
        "# affichage de la phrase\n",
        "print(spacy_doc.text)\n",
        "\n",
        "# affichage de quelques résultats d'analyse \n",
        "# et ce, dans une pandas dataframe pour améliorer le visuel\n",
        "# importation de la bibliothèque pandas\n",
        "import pandas as pd\n",
        "# spécifie qu'au niveau de l'affichage il n'y a pas de limites, cad affiche toutes les colonnes et toutes les lignes\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "# pandas travaillant avec des listes python, on transforme le résultat d'analyse de spacy en liste\n",
        "spacy_tokens_as_list = [(token.text, token.is_stop) for token in spacy_doc]\n",
        "pd.DataFrame(spacy_tokens_as_list, columns=['Token', 'is_stop'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQAdZqp5Hir"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slk2dK1aq-Jo"
      },
      "source": [
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3UeYPftYmIg"
      },
      "source": [
        "## Analyse syntaxique\n",
        "\n",
        "Si l'analyse lexicale vise à décrire le mot, la syntaxe a pour objet de décrire les groupes de mots et les relations entre les mots ou groupes de mots.\n",
        "\n",
        "En syntaxe, il existe deux modèles d'analyse de la structure syntaxique:\n",
        "- L'**analyse en constituants** qui met en avant des groupes de mots correspondants à des catégories d'objets syntaxiques : groupe nominal, groupe verbal, groupe prépositionnel... La catégorie vient de la nature grammaticale d'un mot directeur dans le groupe de mots. Ces groupes ont une structure récursive (e.g. un groupe nominal peut contenir un groupe prépositionnel... \"L'histoire de ma vie\" contient \"de ma vie\"...). Suivant les écoles, on parle d'**analyse syntagmatique** (qui produit des  **syntagmes**) (et en anglais on parle de _chunking_ qui produit des _chunks_).\n",
        "- L'**analyse en dépendances**  (_dependency parsing_ en anglais) qui met en avant les fonctions jouées par des têtes lexicales (e.g. sujet, objet, modifieur...) vis-à-vis d'autres mots avec lesquels ils entretiennent des relations directes. Les relations entre les mots dessinent un arbre orienté. Le verbe de la proposition principale joue le rôle de la racine. Par exemple, dans la phrase \"L'histoire de ma vie se résume simplement\". La racine est le verbe \"résume\" à partir duquel au moins deux relations directes pourront partir : l'une vers la tête du sujet à savoir le nom \"histoire\" et l'autre vers la tête du complément de manière \"simplement\". \n",
        "\n",
        "Les constituants peuvent jouer le rôle de termes candidats pour indexer de l'information. \n",
        "\n",
        "L'arbre syntaxique peut être exploitée en simplification de phrases (les mots les plus proches de la racine sont les plus importants) ou bien dans des systèmes de question-réponse pour apparier un verbe d'une question avec des arguments possibles dans des phrases contenant une réponse possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-x8Ysc7baMq"
      },
      "source": [
        "### QUESTIONS : Analyse en constituants\n",
        "Le code suivant permet d'observer les 5 premières phrases exemples. \n",
        "\n",
        "* A la main, identifier pour chacune des phrases les syntagmes nominaux maximums qui les composent. Le terme 'nominal' dans 'syntagme nominal' signifie que la tête sémantique (le mot le plus important du syntagme) est un nom. 'Maximum' signifie qu'il n'existe pas de constituants nominaux qui les englobent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n5acAV2aBYn"
      },
      "source": [
        "# Observation des 5 premières phrases exemples\n",
        "for i in range(0,5):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "  print(i, spacy_doc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT7E2wvnZ_5v"
      },
      "source": [
        "* Le code suivant réalise l'identification automatique des syntagmes nominaux. Quels types d'erreurs rencontrez-vous ? Les syntagmes nominaux sont-ils raccords avec l'analyse grammaticale produite sur les tokens mots ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHFhADebWV8"
      },
      "source": [
        "#spacy_noun_chunks_as_list = [(chunk.text,  chunk.root.text, chunk.root.dep_,\n",
        "#            chunk.root.head.text) for chunk in spacy_doc.noun_chunks]\n",
        "#pd.DataFrame(spacy_noun_chunks_as_list, columns=['text', 'root', 'dep', 'head'])\n",
        "for i in range(0,5):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "  print('sentence',i, spacy_doc.text)\n",
        "  for chunk in spacy_doc.noun_chunks:\n",
        "    print('chunk:', chunk.text) #, chunk.root.text, chunk.root.dep_, chunk.root.head.text)   \n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADl-NVXSdQN"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrP0M_8dTqp_"
      },
      "source": [
        "### QUESTIONS : Analyse en dépendance\n",
        "\n",
        "\n",
        "* Avant d'exécuter le code ci-dessous, identifiez à la main les 3 mots que vous estimez le plus important dans les  5 premières phrases exemples de spacy (0 à 4). Vous pouvez appliquer la stratégie de rechercher les têtes lexicales en priorisant d'abord le verbe, puis le sujet, puis l'objet, puis les compléments.\n",
        "\n",
        "* Le code suivant permet de visualiser la structure syntaxique en dépendance pour chacune des 5 premières phrases exemples. Les analyses vous semblent-elles correctes ? Donnez deux exemples d'erreurs distinctes. Malgré les erreurs, retrouvez-vous vos mots dans les 3 premiers niveaux de l'arbre (le 1er niveau est la racine) ?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkgQPtnBs5uC"
      },
      "source": [
        "# import d'une bibliothèque qui permet de visualiser les résultats de spaCy\n",
        "# ici les liens de dépendances entre les mots\n",
        "from spacy import displacy\n",
        "\n",
        "for i in range(0,5):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "  displacy.render(spacy_doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hevn0R8Aahvs"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfwLaFdP07oQ"
      },
      "source": [
        "## Reconnaissance d'entités nommées\n",
        "\n",
        "Les entités nomées sont des expressions qui désignent des noms de lieux (label `LOC`), de personnes (label `PERS`), d'organisation (label `ORG`), ou d'évènement (label `MISC`). Selon les systèmes, les dates/heures et les mesures peuvent aussi être considérées comme des entités nommées.\n",
        "\n",
        "\n",
        "\n",
        "D'un point de vue applicatif, le besoin est parfois d'identifier quelles entités sont en présence. D'autres fois, il peut importer de déterminer les positions/offsets (début/fin en termes de numéro de caractère) de l'entité nommée dans un texte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlb0dRU21zsw"
      },
      "source": [
        "### QUESTIONS\n",
        "\n",
        "Le code suivant permet de visualiser entités nommées présentes dans le document analysé.\n",
        "* Listez les types d'entités (_labels_) présentes dans les exemples\n",
        "* Consultez l'analyse des 10 premières phrases exemples de spacy (0 à 9). Trouvez-vous des erreurs de délimitation d'entités nommées ? Dans l'étiquetage du type des entités ? Eventuellement, donner quelques exemples.\n",
        "* Jetez un oeil sur les performances (section *accuracy evaluation*) des modèles pour le français https://spacy.io/models/fr pour avoir une idée de la performance supposée de ceux-ci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jkS1aoU2Brx"
      },
      "source": [
        "# le code suivant permet de visualiser entités nommées présentes dans le document analysé\n",
        "# les offsets et le type d'entité sont aussi fournis \n",
        "for i in range(0,9):\n",
        "  spacy_doc = nlp(sentences[i])\n",
        "\n",
        "  # pour chaque entité nommé détectée dans la phrase courante\n",
        "  for ent in spacy_doc.ents:\n",
        "\n",
        "    # affiche le texte de l'entité nommée, ses offsets, et son \"type\"\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "  # finalement affiche la phrase en marquant visuellement les zones de textes \n",
        "  # où une entité nommée a été repérée\n",
        "  displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw6iMfw16KF"
      },
      "source": [
        "### VOS RÉPONSES\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm3_nGMo1B0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoBU3isucVpQ"
      },
      "source": [
        "## Expérience personnelle vs performances attestées\n",
        "\n",
        "L'étiquetage grammaticale consiste à donner une étiquette à chacun des mots. Tous les mots auront une étiquette. La performance correspondra alors à combien d'étiquettes de mots sont correctement trouvés sur le nombre total de mots. On parlera de mesure d'**exactitude** (_accuracy_ en anglais, souvent abrégé en _acc_) dans ce cas.\n",
        "\n",
        "La reconnaissance des entités nomées est une tâche un peu différente. A l'instar de l'étiquetage grammatical, elle peut être vue comme mettre une étiquette \"entité nonmée\" à certains des mots. Mais à la différence de l'étiquetage grammatical, il s'agit de déterminer les mots qui doivent recevoir une étiquette \"entité nommée\". Dans cette tâche, on évalue d'une part la capacité à retrouver TOUS LES MOTS qui portent une étiquette \"entité nommée\" ; on parlera de mesure de **rappel** (_recall_). Et d'autre part, on évalue la qualité de la prédiction (sur les mots que l'on dit porter une étiquette \"entité nommée\", combien sont correctes) ; on parlera de mesure de **précision** (_precision_) qui est similaire à la notion d'exactitude. \n",
        "\n",
        "Comme c'est souvent plus simple d'avoir un seul score plutôt que deux, on utilise la mesure de **F-score** qui correspond à une moyenne (harmonique) des scores de précision et de rappel.\n",
        "\n",
        "Si ce n'est pas clair... https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFF9cpUczG2"
      },
      "source": [
        "### QUESTIONS\n",
        "* Jeter un oeil sur les performances (section *accuracy evaluation*) du modèle utilisé pour le français https://spacy.io/models/fr pour avoir une idée de la performance supposée de celui-ci sur les tâches d'analyse linguistique (tokenization, étiquetage grammatique, lemmatisation, analyse morphologique, analyse en dépendance et reconnaissance d'entités nommées). Est-ce raccord avec ce que vous avez observé ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUhWwwVGc5hA"
      },
      "source": [
        "### VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GNFxUwaNaM"
      },
      "source": [
        "# Analyse de textes de genres différents\n",
        "\n",
        "Le code suivant télécharge dans un répertoire `data` un corpus de phrases issus de 4 genres différents : textes parlementaires européens (_legal europarl_), dépèches journalistiques (_news_wikinews_), littératique romanesque (_roman verne_), et des tweets de twitter (_tweets twitter_).\n",
        "\n",
        "Exécuter le.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2MikY8trjs-"
      },
      "source": [
        "!mkdir data\n",
        "!wget -nc https://raw.githubusercontent.com/nicolashernandez/teaching_nlp/main/data/fr_raw_25000sentences_4genres.zip -P data\n",
        "!unzip data/fr_raw_25000sentences_4genres.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtcfjtV01OSi"
      },
      "source": [
        "\n",
        "## QUESTIONS\n",
        "\n",
        "Le code suivant charge tour à tour chacun des corpus et traite les 5 premières phrases de chaque corpus. Deux résultats de traitement sont observés : la tokenization et la reconnaissance d'entités nommées à l'aide de spaCy et du modèle pour le français précédemment utilisé.\n",
        "\n",
        "* Quels problèmes de tokenization relevez-vous suivant les genres de texte ? Y-a-t'il des genres pour lesquels la tokenization fonctionne mieux que d'autres ?\n",
        "* Mêmes questions pour la reconnaissance des entités nommées.\n",
        "* Quels corpus (et donc quels genres de texte) ont servi de données d'entraînement pour construire le modèle français utilisé ici avec spaCy (cf. https://spacy.io/models/fr) ? Est-ce que cela peut expliquer les performances observées ?  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ETeuhrev-7z"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "filenames = ['legal_europarl', 'news_wikinews', 'roman_verne', 'tweets_twitter']\n",
        "for filename in filenames:\n",
        "  with open(\"data/\"+filename+\".txt\", 'r', encoding='UTF-8') as file:\n",
        "    i = 0\n",
        "    print('-->', filename.upper())\n",
        "    print()\n",
        "    for line in file:\n",
        "        doc = line.rstrip()\n",
        "        spacy_doc = nlp(doc)\n",
        "        print('Sentence',i,':', doc)\n",
        "        print ('spacy_tokenize:', [token.text for token in spacy_doc])\n",
        "        if spacy_doc.ents: displacy.render(spacy_doc, style='ent', jupyter=True)\n",
        "        #spacy_doc_as_list = [(token.text, token.lemma_, token.pos_) for token in spacy_doc]\n",
        "#            , token.tag_,token.shape_, token.is_alpha, token.is_stop) for token in doc]\n",
        "        #print (pd.DataFrame(spacy_doc_as_list, columns=['Token', 'Lemma', 'POS']))\n",
        "        print ()\n",
        "        i += 1\n",
        "        if i==5: break\n",
        "    print ('------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQz5q0C9_PD"
      },
      "source": [
        "\n",
        "## VOTRE RÉPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2NDYD5Es0Ic"
      },
      "source": [
        "# Biais des modèles construits sur des données\n",
        "\n",
        "Soit le texte suivant\n",
        "> *Elle est médecin. Il est infirmier.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZ-kRIWui7t"
      },
      "source": [
        "\n",
        "### QUESTIONS\n",
        "* Ouvrir [Google Translate dans votre navigateur](https://translate.google.fr/?hl=fr&sl=fr&tl=en&text=Elle%20est%20m%C3%A9decin.%20Il%20est%20infirmier.&op=translate)\n",
        "* Traduire du français (langue source) vers l'anglais (langue cible). Cliquer deux fois sur \"Intervertir les langues\" (pour traduire une fois vers l'anglais puis pour retraduire l'anglais vers le français). Observez-vous quelque chose ?\n",
        "* Faire la même chose en prenant comme langue cible du Hongrois. Observez vous quelque chose ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scPvHf9wumB1"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXOzv3zo_qB"
      },
      "source": [
        "---\n",
        "# Multilinguisme\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Donnés les éléments suivants :\n",
        "* https://spacy.io/models\n",
        "* https://fr.wikipedia.org/wiki/Liste_de_langues_par_nombre_total_de_locuteurs\n",
        "* Le score F-score est une moyenne des scores de Précision et de Rappel. \n",
        "* un [tableau des performances de spaCy telles que présentées en oct 2021 pour le 1er modèle de chaque langue](https://github.com/nicolashernandez/teaching_nlp/raw/main/performances%20spacy%20oct%202021.ods)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRxwNo-7K5E"
      },
      "source": [
        "# Benchmark NLP libs\n",
        "\n",
        "Ci-dessous quelques comparatifs de performance (~qualité et temps de traitement) selon les auteurs de bibliothèques. \n",
        "* spacy vs: stanza, flair https://spacy.io/usage/facts-figures\n",
        "* trankit vs: stanza, spacy... https://trankit.readthedocs.io/en/latest/performance.html\n",
        "* stanford vs: spacy https://nlp.stanford.edu/software/tokenizer.html#Speed (https://twitter.com/chrmanning/status/1013563834655621120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Hehbr6_Pjt"
      },
      "source": [
        "## QUESTIONS\n",
        "* En vous appuyant le comparatif rapporté par spaCy, observer les performances (qualité et temps de traitement) de spaCy, Stanza et de Flair sur des tâches reconnaissance d'entités nommées et de parsing en anglais, diriez-vous que Spacy est 1) la meilleure solution, 2) une solution état de l'art, 3) de performance moindre que les solutions existantes ?\n",
        "* Selon le comparatif de trankit, comment celui-ci se positionne sur la tâche de reconnaissance d'entités nommées par rapport à ses concurrents Stanza et spaCy ? Sur quel corpus et quelle langue peut-on faire ce comparatif ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW8cF6u9_Q52"
      },
      "source": [
        "\n",
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KiEX6l7lNgN"
      },
      "source": [
        "# Références\n",
        "* https://github.com/clement-plancq/outils-corpus/blob/master/outils_corpus-5.ipynb\n"
      ]
    }
  ]
}